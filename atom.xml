<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Niansong Zhang</title>
  
  <subtitle>Blog</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.zzzdavid.tech/"/>
  <updated>2022-08-26T19:32:11.149Z</updated>
  <id>https://www.zzzdavid.tech/</id>
  
  <author>
    <name>Niansong Zhang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>CVPR 2022 at New Orleans</title>
    <link href="https://www.zzzdavid.tech/cvpr22/"/>
    <id>https://www.zzzdavid.tech/cvpr22/</id>
    <published>2022-06-30T04:00:00.000Z</published>
    <updated>2022-08-26T19:32:11.149Z</updated>
    
    <content type="html"><![CDATA[<!-- excerpt --><p>The IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR) is a premier international conference held every year in the US. The 2022 CVPR is held in New Orleans, Louisiana, and I am fortunate enough to attend and present at the event. </p><h2 id="Conference-Format"><a href="#Conference-Format" class="headerlink" title="Conference Format"></a>Conference Format</h2><p>This year’s CVPR is a hyprid event. It’s the first CVPR happening in-person since the pandemic. The in-person conference lasts 6 days from June 19 to 24: Sunday to Monday are workshops, and Tuesday to Friday is the main conference. It’s a huge conference, even though it lasts about a week, there are lots of things happening in parallel. There are about 30 to 40 all-day workshops going in parallel, and three oral sessions going in parallel in the main conference. It’s impossible to go to everything, so everyone has to choose what they are most interested in. There’s an additional virtual poster session happens one-week after the main conference, and the in-person poster presenters are encouraged to sign up as well. </p><div class="figure " style="width:;"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1656969032/cvpr22/convention_e1qj2j.jpg" alt="The Ernest N. Morial Convention Center"><span class="caption">The Ernest N. Morial Convention Center</span></div><p>The event takes place at the Ernest N. Morial Convention Center at downtown New Orleans by the Mississippi river bank. It’s an enormous conference center, even 5,600 people looks sparse.</p><p>For the main conference paper presentation, there are two formats this year: oral and poster. Oral presentations has 5 minutes, and posters last for one morning or afternoon. The oral presenters also attend the poster sessions, so people have more chances to interact with the authors. </p><h2 id="Sunday-Workshops"><a href="#Sunday-Workshops" class="headerlink" title="Sunday Workshops"></a>Sunday Workshops</h2><p>I jumped between a few workshops for keynotes and full paper presentations. Here I select a few papers and talks that are interesting to me and offer some thoughts and digestion.</p><h3 id="NAS-Workshop"><a href="#NAS-Workshop" class="headerlink" title="NAS Workshop"></a>NAS Workshop</h3><p>Workshop link: <a href="https://cvpr-nas.com" target="_blank" rel="noopener">link</a></p><h4 id="Evolving-Search-Space-for-Neural-Architecture-Search"><a href="#Evolving-Search-Space-for-Neural-Architecture-Search" class="headerlink" title="Evolving Search Space for Neural Architecture Search"></a>Evolving Search Space for Neural Architecture Search</h4><ul><li>Paper link: <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Ci_Evolving_Search_Space_for_Neural_Architecture_Search_ICCV_2021_paper.pdf" target="_blank" rel="noopener">cvf.com</a></li><li>Published in ICCV21, The University of Sydney</li></ul><p>Neural architecture search automates the process of handcrafting neural nets, but the quality of searched candidates are affected by the search space, which again is handcrafted. Plus, enlarging the search space does not produce better candidates, instead, it is unbeneficial or even detrimental to existing NAS methods. </p><p>This work proposes to evolve the search space by repeating two steps: 1) search an optimized space from the search space subset, 2) re-fill this subset from a larger pool of operations that haven’t been traversed. Their method yields better performance and lower FLOPs comparing against NAS methods without distillation and weight pruning.</p><h3 id="Dynamic-Neural-Network-Workshop"><a href="#Dynamic-Neural-Network-Workshop" class="headerlink" title="Dynamic Neural Network Workshop"></a>Dynamic Neural Network Workshop</h3><p>Workshop link: <a href="https://sites.google.com/view/cvpr2022-dnetcv" target="_blank" rel="noopener">link</a></p><h4 id="More-ConvNets-in-the-2020s-Scaling-up-Kernels-Beyond-51x51-using-Sparsity"><a href="#More-ConvNets-in-the-2020s-Scaling-up-Kernels-Beyond-51x51-using-Sparsity" class="headerlink" title="More ConvNets in the 2020s: Scaling up Kernels Beyond 51x51 using Sparsity"></a>More ConvNets in the 2020s: Scaling up Kernels Beyond 51x51 using Sparsity</h4><ul><li>By Prof. Atlas Wang, UT Autstin</li><li>Slides: <a href="https://drive.google.com/file/d/1eQE1bZUyHcmw-fD2MAA6HY4xVLNtlpMy/view" target="_blank" rel="noopener">google drive</a></li></ul><p>Why is transformer better than convolution on visual tasks? On the one hand, CNNs have inductive biases like local smoothness, hierarchical representations, translation invariance. Transformers don’t have these inductive bias, and that allows transformers to learn better representations. On the other, and perhaps more importantly, transformers have global reception field since day-one. ViT divides images into patches and trains global attention across patches.</p><p>Is it possible to scale up convolution’s kernel size to have a large reception field? RepLKNet (Ding et al, 2022) shows that large kernels are effective for boosting CNN’s performance. However, applying RepLKNet’s reparameterization trick on ConvNeXT makes accuracy drop because local details are lost. The solution to scale convolution kernels beyond 51x51 is using sparsity. </p><div class="figure " style="width:;"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657130326/cvpr22/scaling.png" alt="kernel decomposition"><span class="caption">kernel decomposition</span></div><p>The recipe is: </p><ol><li>Large kernel decomposition: decompose a <code>MxM</code> convolution to three parallel branches of convolution layers with kernel size <code>MxN</code>, <code>NxM</code>, and <code>5x5</code>.</li><li>Dynamic feature sparsity: dynamically select a subset of kernel parameters in each train iteration (dropout is a random dynamic sparsity) by turning off kernel elements with large gradients.</li><li>Use sparse groups: do step 2 by channel-wisely.</li></ol><p><strong>Q&amp;A</strong></p><ol><li>Since large convolution kernels give us large reception field, does that mean we don’t have to rely on stacking lots of layers to get global reception field?<br>The answer is yes, they observe that CNNs with larger kernels requires less layers to achieve the same level of performance.</li><li>How regular is the sparse pattern?<br>The sparse pattern is elementwise and not regular. But the sparse pattern eventually converges during training, and is fixed during inference.</li></ol><h4 id="Towards-Robust-and-Efficient-Visual-Systems-via-Dynamic-Representations"><a href="#Towards-Robust-and-Efficient-Visual-Systems-via-Dynamic-Representations" class="headerlink" title="Towards Robust and Efficient Visual Systems via Dynamic Representations"></a>Towards Robust and Efficient Visual Systems via Dynamic Representations</h4><ul><li>By Xin Wang from Microsoft Research</li><li>Slides: <a href="https://drive.google.com/file/d/1iP3TzMHSGqGqEUJBLOC3IYDBiUhBDdge/view" target="_blank" rel="noopener">google drive</a></li></ul><ol><li><p>We need a new type of visual attention model to improve robustness</p><ul><li>Transformer’s self attention is not robust: e.g. when noise is added to the image. </li><li>It also doesn’t follow the human eye’s fixation (attention).</li><li>Human attention has two important features: recurrency and sparsity</li><li>Formulating this idea in NN: feedforward nn with recurrency + a sparse reconstruction layer</li><li>In this formulation, self-attention is a special case.</li><li>The result NN is robust to noise, compression, weather. The attentiond visualization also aligns with human eye fixation (how hasn’t anyone thought this before?).</li></ul></li><li><p>SkipNet: dynamic routing – to change network architecture at runtime</p><ul><li>Different images have different complexity, they should not go through the same depth of feature extraction.</li><li>Each layer has a gate to decide if the next layer should execute.</li><li>Using RL agent for the decision, with pretrained feature extractor.</li></ul></li><li><p>Zero-shot knowledge generalization</p><ul><li>Imagine “red elephant”. You have never seen one but you know what’s “red” and what’s “elephant”, so you can imagine that.</li><li>Proposes “task-aware feature embedding” </li></ul></li></ol><div class="figure " style="width:;"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657131056/cvpr22/tafe_k1cgm0.png" alt="TAFE generates layer weights according to task description"><span class="caption">TAFE generates layer weights according to task description</span></div><h3 id="ScanNet-Indoor-Scene-Understanding-Challenge"><a href="#ScanNet-Indoor-Scene-Understanding-Challenge" class="headerlink" title="ScanNet Indoor Scene Understanding Challenge"></a>ScanNet Indoor Scene Understanding Challenge</h3><p>Workshop link: <a href="http://www.scan-net.org/cvpr2022workshop/" target="_blank" rel="noopener">website</a></p><h4 id="Towards-Data-Efficient-and-Continual-Learning-for-3D-Scene-Understanding"><a href="#Towards-Data-Efficient-and-Continual-Learning-for-3D-Scene-Understanding" class="headerlink" title="Towards Data-Efficient and Continual Learning for 3D Scene Understanding"></a>Towards Data-Efficient and Continual Learning for 3D Scene Understanding</h4><ul><li>Prof. Gim Hee Lee from NUS</li></ul><ol><li>Data efficiency: using 2D image labels to train 3D bounding box detection on point cloud data.</li><li>Continual learning: knowledge distillation and solving catastrophic forgetting issues.<ul><li>Three consistency losses for knowledge distillation: alignment-aware, class-aware, and size-aware losses.</li><li>Use knowledge distillation to solve forgetting old classes.</li><li>The relevant paper is: Static-Dynamic Co-teaching for Class-Incremental 3D Object Detection, AAAI 22. <a href="https://arxiv.org/abs/2112.07241" target="_blank" rel="noopener">link</a></li></ul></li></ol><h2 id="Monday-Workshops"><a href="#Monday-Workshops" class="headerlink" title="Monday Workshops"></a>Monday Workshops</h2><h3 id="Efficient-Deep-Learning-for-Computer-Vision-Workshop"><a href="#Efficient-Deep-Learning-for-Computer-Vision-Workshop" class="headerlink" title="Efficient Deep Learning for Computer Vision Workshop"></a>Efficient Deep Learning for Computer Vision Workshop</h3><p>Host: Bichen Wu, Research Scientist at Meta<br>Link: <a href="https://sites.google.com/view/ecv2022/home?authuser=0" target="_blank" rel="noopener">website</a></p><h4 id="Efficient-and-Robust-Fully-attentional-Networks"><a href="#Efficient-and-Robust-Fully-attentional-Networks" class="headerlink" title="Efficient and Robust Fully-attentional Networks"></a>Efficient and Robust Fully-attentional Networks</h4><div class="figure fig-50 right" style="width:;"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657132082/cvpr22/Teaser_qbaubg.png" alt="FAN has more robust attention against noises"><span class="caption">FAN has more robust attention against noises</span></div><ul><li>By Jiashi Feng, ByteDance</li><li>Code: <a href="https://github.com/NVlabs/FAN" target="_blank" rel="noopener">https://github.com/NVlabs/FAN</a></li></ul><p>They first find that ViT is more robust than CNN against common image distortions: motion blur, noise, snow.<br>Also, more Self-Attention (SA) blocks makes ViT more robust. Their work (FAN) is making ViT “fully attentional”, by adding a channel attention block in parallel with the last MLP in the self-attention block. </p><p>Result: ~10% more robust on segmentation/detection than ViT baseline, comparable param size, and attention visualization focus more on the contour of interesting objects. The FAN paper also provides an explanation of how ViT is more robust: it optimizes the <em>Information Bottleneck</em>, and implicitly filters out image noise.</p><h4 id="Towards-Compact-and-Tiny-AI-models-on-Edge"><a href="#Towards-Compact-and-Tiny-AI-models-on-Edge" class="headerlink" title="Towards Compact and Tiny AI models on Edge"></a>Towards Compact and Tiny AI models on Edge</h4><p>By Prof. Yiran Chen, Duke University</p><ul><li>NAS for efficient searching<ul><li>search space shrinking (Zhang et al. AAAI 20)</li><li>Topology-aware NAS</li><li>Graph embedding for NN candidates (Cheng et al. AAAI 21).</li></ul></li><li>They have a paper on predictor-based NAS for point cloud NN (Qi et al. 2017).</li><li>Efficient 3D point-interact space, first and second-order point interaction. Reducing MAC and #param, baseline is KPConv. (WIP, arxiv)</li><li>Structural sparsity: dynamic winners-take-all, a dropout technique to prune activations layerwise, low overhead on edge device. Mixed-precision with bit-level sparsity (H. Yang et. al, ICLR21), adding a LASSO on bitwidth during training.</li><li>Next thing in NAS: interpretability. </li></ul><h4 id="Researching-Efficient-Networks-for-Hardware-that-Does-not-Exist-Yet-physics-and-economics-of-ML-co-design"><a href="#Researching-Efficient-Networks-for-Hardware-that-Does-not-Exist-Yet-physics-and-economics-of-ML-co-design" class="headerlink" title="Researching Efficient Networks for Hardware that Does not Exist Yet: physics and economics of ML co-design"></a>Researching Efficient Networks for Hardware that Does not Exist Yet: physics and economics of ML co-design</h4><p><strong>Łukasz Lew, Google</strong></p><ul><li>His talk high-lights the importance of quantization, custom numeric formats and measuring the cost of a system.</li><li>How do data-centers choose ML accelerators? Performance/TCO (total cost ownership), basically  electric bill.<ul><li>Google’s metric: Joules/inference, Joules/training to a given accuracy</li></ul></li><li>Quantization saves energy, it’s economically inevitable.</li><li>Mixed Quantization, measuring the cost is difficult. <ul><li>ACE: arithmetic computation effort. (Zhang et al. CVPR22) It approximates the logical complexity of MAC operations, and is independent of today’s hardware technology (hence, future-facing). It generalizes to custom numeric formats. </li></ul></li><li>Interesting facts (very useful for motivation slides as well):<ul><li>ML hardware introduces more and more formats with their tradeoffs, like bfloat, NVIDIA’s multiple fp8 variants.</li><li>We are using 15% of the hardware because of the dark silicon (cooling issue). ML chips are limited by power efficiency.</li><li>Pay attention to bit-level sparsity, energy is only used when bits are flipped. </li></ul></li></ul><p><strong>Thoughts</strong></p><ul><li>Chicken and egg problem:<ul><li>ML hardware designers must use mostly exisiting ML models</li><li>ML model designers must use existing hardware<br>A promising area: <strong>custom numeric formats</strong>. </li></ul></li></ul><h4 id="Project-Aria-Wearable-Computing-for-Augmented-Reality"><a href="#Project-Aria-Wearable-Computing-for-Augmented-Reality" class="headerlink" title="Project Aria: Wearable Computing for Augmented Reality"></a>Project Aria: Wearable Computing for Augmented Reality</h4><p><strong>Meta Reality Lab</strong></p><div class="figure fig-100 left" style="width:;"><a class="fancybox" href="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657132811/cvpr22/IMG_0887_zeh5mf.jpg" target="_blank" rel="noopener" title="Project Aria Glasses" data-caption="Project Aria Glasses" data-fancybox="default"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657132811/cvpr22/IMG_0887_zeh5mf.jpg" alt="Project Aria Glasses"></a><span class="caption">Project Aria Glasses</span></div><p>There’s an Egocentric AI Gap for VR, AR &amp; Wearables. Lots of data available, but they are not egocentric. Project Aria is a glass for data collection, to fill this gap. Unique challenges working with egocentric data: scene diversity + limited compute resources, limited sensing rate, noise from motion, multiple reference frames (dynamic settings), personal and privacy requirements. </p><div class="figure center" style="width:100%"; ><video class="fig-video" controls alt="Project Aria First-Person Perspective Demo Video"><source src="https://res.cloudinary.com/dxzx2bxch/video/upload/v1657132802/cvpr22/aria_aciwly.mov" type="video/mp4"><p>Your browser doesn't support HTML5 Video :/</p></video><span class="caption">Project Aria First-Person Perspective Demo Video</span></div><h4 id="Tackling-Model-and-Data-Efficiency-Challenges-for-Computer-Vision"><a href="#Tackling-Model-and-Data-Efficiency-Challenges-for-Computer-Vision" class="headerlink" title="Tackling Model and Data Efficiency Challenges for Computer Vision"></a>Tackling Model and Data Efficiency Challenges for Computer Vision</h4><p><strong>BichenWu, Meta</strong></p><ol><li><p>Model Efficiency: the FBNet Family v1 to v5</p><ul><li>V1: differentiable NAS</li><li>V2: differentiable can’t handle very large search space. Memory efficiency: sharing the same feature map with a mask. </li><li>V3: joint architecture-train recipe search. Optimizer type, lr, decay, dropout, mix-up raito, ema, …. They switched to predictor-based NAS with a performance predictor.</li><li>V4: skipped, it’s an internal model.</li><li>V5: unified, scalable, efficient NAS for perception NAS. Previous NAS methods are designed for single tasks, introducing multitask search. Disentangle the search process from downstream tasks, with a proxy multi-task dataset.  Simultaneously search for many tasks. New SOTA. (arxiv now)</li></ul></li><li><p>Data Efficiency (less labeling)<br>Cross-domain adaptive teacher. The question is: can we train a model once and transfer to a new domain without annotating new data? E.g. can a model trained to localize real-life objects recognize their cartoon version? Solution: a teacher model using a pseudo label in the target domain, and a student model trained with unsupervised loss from the teacher model. </p></li></ol><h4 id="When-Industrial-Model-Toolchain-meets-Xilinx-FPGA"><a href="#When-Industrial-Model-Toolchain-meets-Xilinx-FPGA" class="headerlink" title="When Industrial Model Toolchain meets Xilinx FPGA"></a>When Industrial Model Toolchain meets Xilinx FPGA</h4><p><strong>Jiahao Hu, Ruigao Gong, toolchain team, SenseTime Research</strong><br><strong>1st award of LPCV challenge, FPGA track</strong></p><p>This is a presentation for their solution of the LPCV challenge.</p><ol><li>Setting<ul><li>Task: COCO object detection</li><li>Hardware: Ultra96v2 + DPU overlay with Pynq</li><li>Model: YOLOX-FPGA</li></ul></li><li>Toolchain<ul><li>Training framework: United Perception. United Perception is a training framework for multiple tasks. It also has plenty off-the-shelf training techniques.</li><li>Quantization: MQBench for INT8 quantization</li><li>Multi-platform deployment framework: NART (close source)</li></ul></li><li>Tips and tricks<ul><li>Training recipe: object365 dataset pretrain + distillation</li><li>They optimized the post processing process by havng a C implementation for NMS, signoid, and decoder, resulting in a 74% reduction in post processing time compared with Python implementation.</li><li>Use multithreading for pre- and post-process, so the inference is pipelined.</li></ul></li></ol><p>Their training framework and inference code is open source:<br><a href="https://github.com/ModelTC/United-Perception" target="_blank" rel="noopener">https://github.com/ModelTC/United-Perception</a><br><a href="https://github.com/ModelTC/LPCV2021_Winner_Solution" target="_blank" rel="noopener">https://github.com/ModelTC/LPCV2021_Winner_Solution</a></p><p><strong>Thoughts</strong><br>Their compilation flow is onnx -&gt; xmodel -&gt; DPU instruction. This process happens in Vitis, and it seems that Vitis has a much better operator support on DPU now. </p><p>There’s a live question about deploying vision transformers on edge devices. Sensetime thinks it’s still a long way to go.</p><h3 id="Embedded-Vision-Workshop"><a href="#Embedded-Vision-Workshop" class="headerlink" title="Embedded Vision Workshop"></a>Embedded Vision Workshop</h3><p>Workshop link: <a href="https://embeddedvisionworkshop.wordpress.com" target="_blank" rel="noopener">website</a></p><h4 id="MAPLE-EDGE-A-Runtime-Latency-Predictor-for-Embedded-Devices"><a href="#MAPLE-EDGE-A-Runtime-Latency-Predictor-for-Embedded-Devices" class="headerlink" title="MAPLE-EDGE: A Runtime Latency Predictor for Embedded Devices"></a>MAPLE-EDGE: A Runtime Latency Predictor for Embedded Devices</h4><p>Paper link: <a href="https://openaccess.thecvf.com/content/CVPR2022W/EVW/papers/Nair_MAPLE-Edge_A_Runtime_Latency_Predictor_for_Edge_Devices_CVPRW_2022_paper.pdf" target="_blank" rel="noopener">cvpr</a></p><p>MAPLE-Edge is an edge device-oriented extension to MAPLE, designed specifically to estimate the latency of neural network architectures on unseen embedded devices. MAPLE-Edge trains a LPM-based (Linear Probability Model) hardware-aware regression model that can effectively estimate architecture latency. To do this, it trains the LPM on a dense hardware descriptor made up of CPU performance counters, in conjunction with architecture-latency pairs. In the data collection stage, MAPLE-Edge uses an automated pipeline to convert models in NAS-Bench-201 to their optimized counterparts, deploy it to the corresponding target device, and profile inference.</p><h4 id="MAPLE-X-Latency-Prediction-with-Explicit-Microprocessor-Prior-Knowledge"><a href="#MAPLE-X-Latency-Prediction-with-Explicit-Microprocessor-Prior-Knowledge" class="headerlink" title="MAPLE-X:  Latency Prediction with Explicit Microprocessor Prior Knowledge"></a>MAPLE-X:  Latency Prediction with Explicit Microprocessor Prior Knowledge</h4><p>Baseline: meta-learning based latency prediction model (HELP). This seems interesting as well.<br>Hypothesis: NN arch latency rankings are highly similar across devices.<br>The setting is NAS. The prior knowledge is latencies measured on a source device. The goal is to predict latencies on a target device. With the hypothesis, we can only measure a few nn archs on the target device and assign the rest with a prior according to their ranking.</p><!-- Comment: I actually hoped that it was using some explicit knowledge about the processor hardware architecture… --><h2 id="Main-Conference"><a href="#Main-Conference" class="headerlink" title="Main Conference"></a>Main Conference</h2><h3 id="Opening-Remark"><a href="#Opening-Remark" class="headerlink" title="Opening Remark"></a>Opening Remark</h3><div class="figure fig-100" style="width:;"><a class="fancybox" href="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657157291/cvpr22/opening/IMG_0926_q5awdl.jpg" target="_blank" rel="noopener" title="" data-caption="" data-fancybox="default"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657157291/cvpr22/opening/IMG_0926_q5awdl.jpg" alt=""></a></div><div class="figure fig-50" style="width:;"><a class="fancybox" href="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657157291/cvpr22/opening/IMG_0927_acaebg.jpg" target="_blank" rel="noopener" title="" data-caption="" data-fancybox="default"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657157291/cvpr22/opening/IMG_0927_acaebg.jpg" alt=""></a></div><div class="figure fig-50" style="width:;"><a class="fancybox" href="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657157291/cvpr22/opening/IMG_0934_e8parp.jpg" target="_blank" rel="noopener" title="" data-caption="" data-fancybox="default"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657157291/cvpr22/opening/IMG_0934_e8parp.jpg" alt=""></a></div><div class="figure fig-50" style="width:;"><a class="fancybox" href="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657157291/cvpr22/opening/IMG_0932_nd3fza.jpg" target="_blank" rel="noopener" title="" data-caption="" data-fancybox="default"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657157291/cvpr22/opening/IMG_0932_nd3fza.jpg" alt=""></a></div><div class="figure fig-50" style="width:;"><a class="fancybox" href="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657157291/cvpr22/opening/IMG_0934_e8parp.jpg" target="_blank" rel="noopener" title="" data-caption="" data-fancybox="default"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657157291/cvpr22/opening/IMG_0934_e8parp.jpg" alt=""></a></div><p>Some interesting statistics:</p><ol><li>8,161 paper submission, 2,064 are accepted.</li><li>5,641 in-person attendees, 4340 virtual attendees.</li><li>Exponential growth of submissions.</li><li>Rebuttal effectively increases the chance of getting accepted.</li></ol><h3 id="Our-Presentation"><a href="#Our-Presentation" class="headerlink" title="Our Presentation"></a>Our Presentation</h3><div class="figure " style="width:;"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657157879/cvpr22/IMG_0968_fnxbyx.jpg" alt="Our Poster Presentation"><span class="caption">Our Poster Presentation</span></div><p>I presented at the first poster session on Tuesday. Our work is about how to improve transformer’s generalization ability on 3D voxel data with a codebook self-attention and explicit geometric guidance. About 20 to 30 people stopped by during the poster session, and a few after the session.<br>Here are some questions I got during the presentation: </p><ol><li>How do you design the sparse pattern and choose the dilation? (asked most frequently)</li><li>How is <code>w</code> (the weight of the prototypes) generated?</li><li>Have you considered learning the sparse pattern during training?</li><li>Can we use this codebook design for generative models like color/material manipulation?</li><li>Voxel size? Are you using MinkowskiEngine for implementation?</li><li>How long does it take to train a model?</li><li>Future plan for this project?</li></ol><h3 id="Paper-Highlights"><a href="#Paper-Highlights" class="headerlink" title="Paper Highlights"></a>Paper Highlights</h3><p>Here I highlight a few papers that I find interesting or worth noting, and I categorize them into these groups: explanable vision, efficiency, and 3D scene understanding.</p><h3 id="Explanable-Vision"><a href="#Explanable-Vision" class="headerlink" title="Explanable Vision"></a>Explanable Vision</h3><h4 id="NLX-GPT-A-Model-for-Natural-Language-Explanations-in-Vision-and-Vision-Language-Tasks"><a href="#NLX-GPT-A-Model-for-Natural-Language-Explanations-in-Vision-and-Vision-Language-Tasks" class="headerlink" title="NLX-GPT: A Model for Natural Language Explanations in Vision and Vision-Language Tasks"></a>NLX-GPT: A Model for Natural Language Explanations in Vision and Vision-Language Tasks</h4><ul><li>Presentation: Oral</li><li>Paper: <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Sammani_NLX-GPT_A_Model_for_Natural_Language_Explanations_in_Vision_and_CVPR_2022_paper.pdf" target="_blank" rel="noopener">cvf</a></li><li>Institution: Vrije Universiteit Brussel</li><li>PI: Nikos Deligiannis</li></ul><div class="figure " style="width:;"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657160992/cvpr22/Screen_Shot_2022-07-06_at_10.29.23_PM_vjg5sm.png" alt=""></div><p>Use Vision-Language models to explain the decision-making process of a black box system via generating a natural language sentence. </p><h4 id="Deep-Spectral-Methods-A-Surprisingly-Strong-Baseline-for-Unsupervised-Semantic-Segmentation-and-Localization"><a href="#Deep-Spectral-Methods-A-Surprisingly-Strong-Baseline-for-Unsupervised-Semantic-Segmentation-and-Localization" class="headerlink" title="Deep Spectral Methods: A Surprisingly Strong Baseline for Unsupervised Semantic Segmentation and Localization"></a>Deep Spectral Methods: A Surprisingly Strong Baseline for Unsupervised Semantic Segmentation and Localization</h4><ul><li>Presentation: Oral</li><li>Paper: <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Melas-Kyriazi_Deep_Spectral_Methods_A_Surprisingly_Strong_Baseline_for_Unsupervised_Semantic_CVPR_2022_paper.pdf" target="_blank" rel="noopener">cvf</a></li><li>Institution: University of Oxford</li><li>PI: Andrea Vedaldi</li></ul><p>This work combines self-supervised learning and traditional graph semantic theory on semantic segmentation and localization tasks. The result outperforms SOTA self-supervised models by a large margin. </p><h3 id="3D-Scene-Understanding"><a href="#3D-Scene-Understanding" class="headerlink" title="3D Scene Understanding"></a>3D Scene Understanding</h3><h4 id="Stratified-Transformer-for-3D-Point-Cloud-Segmentation"><a href="#Stratified-Transformer-for-3D-Point-Cloud-Segmentation" class="headerlink" title="Stratified Transformer for 3D Point Cloud Segmentation"></a>Stratified Transformer for 3D Point Cloud Segmentation</h4><ul><li>Presentation: Poster</li><li>Paper: <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Lai_Stratified_Transformer_for_3D_Point_Cloud_Segmentation_CVPR_2022_paper.pdf" target="_blank" rel="noopener">cvf</a></li><li>Institution: CUHK, HKU, SmartMore, MPI Informatics, MIT</li><li>PI: Jiaya Jia</li></ul><div class="figure center" style="width:;"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657161408/cvpr22/stratified_p6d1oz.png" alt=""></div><p>A stratified sampling strategy for point cloud transformers that densely samples local points and sparsely samples distant points. The results demonstrate better generalization ability. </p><h4 id="Point-Density-Aware-Voxels-for-LiDAR-3D-Object-Detection"><a href="#Point-Density-Aware-Voxels-for-LiDAR-3D-Object-Detection" class="headerlink" title="Point Density-Aware Voxels for LiDAR 3D Object Detection"></a>Point Density-Aware Voxels for LiDAR 3D Object Detection</h4><ul><li>Presentation: Poster</li><li>Paper: <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Hu_Point_Density-Aware_Voxels_for_LiDAR_3D_Object_Detection_CVPR_2022_paper.pdf" target="_blank" rel="noopener">cvf</a></li><li>Institution: University of Toronto</li></ul><div class="figure center" style="width:;"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657161715/cvpr22/Screen_Shot_2022-07-06_at_10.41.45_PM_jrzuam.png" alt=""></div><p>A LiDar 3D object detecture architecture that takes point density variation into account during ROI pooling. It achieves the new SOTA on the Waymo Open Dataset.</p><h4 id="Domain-Adaptation-on-Point-Clouds-via-Geometry-Aware-Implicits"><a href="#Domain-Adaptation-on-Point-Clouds-via-Geometry-Aware-Implicits" class="headerlink" title="Domain Adaptation on Point Clouds via Geometry-Aware Implicits"></a>Domain Adaptation on Point Clouds via Geometry-Aware Implicits</h4><ul><li>Presentation: Poster</li><li>Paper: <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Shen_Domain_Adaptation_on_Point_Clouds_via_Geometry-Aware_Implicits_CVPR_2022_paper.pdf" target="_blank" rel="noopener">cvf</a></li><li>Institution: Zhejiang University, Stanford and Peiking University</li><li>PIs: He Wang, Youyi Zheng, Leonidas Guibas</li></ul><div class="figure center" style="width:;"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657162223/cvpr22/Screen_Shot_2022-07-06_at_10.50.05_PM_eivgfc.png" alt=""></div><p>Point cloud data of the same object can have significant geometric variation when captured by different censors or using different procedures. This work proposes an unsupervised domain adaptation method leveraging the gemoetric implicits. </p><h3 id="Efficiency"><a href="#Efficiency" class="headerlink" title="Efficiency"></a>Efficiency</h3><h4 id="It’s-All-in-the-Teacher-Zero-Shot-Quantization-Brought-Closer-to-the-Teacher"><a href="#It’s-All-in-the-Teacher-Zero-Shot-Quantization-Brought-Closer-to-the-Teacher" class="headerlink" title="It’s All in the Teacher: Zero-Shot Quantization Brought Closer to the Teacher"></a>It’s All in the Teacher: Zero-Shot Quantization Brought Closer to the Teacher</h4><ul><li>Presentation: Oral</li><li>Paper: <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Choi_Its_All_in_the_Teacher_Zero-Shot_Quantization_Brought_Closer_to_CVPR_2022_paper.pdf" target="_blank" rel="noopener">cvf</a></li><li>Institution: Yonsei University</li><li>PI: Jinho Lee</li></ul><p>Zero-shot quantization (or data-free quantization) is quantizing a neural network without access to any of its training data. This is done by taking information from the weights of a full-precision teacher network to compensate the performance drop of the quantized network.</p><h4 id="MiniViT-Compressing-Vision-Transformers-with-Weight-Multiplexing"><a href="#MiniViT-Compressing-Vision-Transformers-with-Weight-Multiplexing" class="headerlink" title="MiniViT: Compressing Vision Transformers with Weight Multiplexing"></a>MiniViT: Compressing Vision Transformers with Weight Multiplexing</h4><ul><li>Presentation: Poster</li><li>Paper: <a href="https://arxiv.org/pdf/2204.07154.pdf" target="_blank" rel="noopener">arxiv</a></li><li>Institution: Microsoft</li></ul><div class="figure " style="width:;"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657162435/cvpr22/Screen_Shot_2022-07-06_at_10.53.16_PM_grcmhy.png" alt=""></div><p>ViT has good performance but is computationally expensive. This work compresses ViT by multiplexing (sharing) weights of consecutive transformer blocks. </p><h4 id="Scaling-Up-Your-Kernels-to-31x31-Revisiting-Large-Kernel-Design-in-CNNs"><a href="#Scaling-Up-Your-Kernels-to-31x31-Revisiting-Large-Kernel-Design-in-CNNs" class="headerlink" title="Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs"></a>Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs</h4><ul><li>Presentation: Poster</li><li>Paper: <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ding_Scaling_Up_Your_Kernels_to_31x31_Revisiting_Large_Kernel_Design_CVPR_2022_paper.pdf" target="_blank" rel="noopener">cvf</a></li><li>Institution: Tsinghua University, MEGVII</li><li>PI: Guiguang Ding</li></ul><div class="figure " style="width:;"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657162583/cvpr22/Screen_Shot_2022-07-06_at_10.56.10_PM_xzgaiq.png" alt=""></div><p>This paper proposes RepLKNet to scale ConvNet kernels to 31x31 with reparameterization.</p><h4 id="A-ViT-Adaptive-Tokens-for-Efficient-Vision-Transformer"><a href="#A-ViT-Adaptive-Tokens-for-Efficient-Vision-Transformer" class="headerlink" title="A-ViT: Adaptive Tokens for Efficient Vision Transformer"></a>A-ViT: Adaptive Tokens for Efficient Vision Transformer</h4><ul><li>Presentation: Poster</li><li>Paper: <a href="https://arxiv.org/pdf/2112.07658.pdf" target="_blank" rel="noopener">arxiv</a></li><li>Institution: NVIDIA</li></ul><div class="figure " style="width:;"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657162765/cvpr22/Screen_Shot_2022-07-06_at_10.59.11_PM_wrgahn.png" alt=""></div><p>Let ViT learns which image patches to preserve, discarding redundant spatial tokens to achieve higher efficiency. The loss is designed to balance the accuracy-efficiency trade-off. </p><h3 id="The-Ongoing-Debate-of-ConvNet-or-Transformer"><a href="#The-Ongoing-Debate-of-ConvNet-or-Transformer" class="headerlink" title="The Ongoing Debate of ConvNet or Transformer"></a>The Ongoing Debate of ConvNet or Transformer</h3><h4 id="A-ConvNet-for-the-2020s"><a href="#A-ConvNet-for-the-2020s" class="headerlink" title="A ConvNet for the 2020s"></a>A ConvNet for the 2020s</h4><ul><li>Presentation: Oral</li><li>Paper: <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_A_ConvNet_for_the_2020s_CVPR_2022_paper.pdf" target="_blank" rel="noopener">cvf</a></li><li>Institution: Berkeley and FAIR</li><li>PIs: Trevor Darrell, Saining Xie<div class="figure center" style="width:;"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657163017/cvpr22/Screen_Shot_2022-07-06_at_11.03.21_PM_kjpfjq.png" alt=""></div></li></ul><p>This work is aimed to test the limit of a pure ConvNet. They gradually “modernize” a standard ResNet towards the design of a vision Transformer, and discover several key components that contribute to the performance difference:</p><ul><li>Stage Compute Raio</li><li>Non-overlapping Conv, “to pachify”</li><li>Use depthwise convolution</li><li>Inverted bottleneck</li><li>Large kernel sizes</li><li>Replacing ReLU with GELU</li><li>Fewer activation and normalization</li><li>Substitue BN with LN</li><li>Separate downsampling layer from basic block</li></ul><h3 id="Demos"><a href="#Demos" class="headerlink" title="Demos"></a>Demos</h3><p>CVPR features a huge demo event. Companies and sponsors showcase their product or research work with actual demos and give out gifts. Here are some pictures of the demos:</p><div class="figure fig-50" style="width:;"><a class="fancybox" href="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657157997/cvpr22/companies/IMG_0960_c6deos.jpg" target="_blank" rel="noopener" title="" data-caption="" data-fancybox="default"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657157997/cvpr22/companies/IMG_0960_c6deos.jpg" alt=""></a></div><div class="figure fig-50" style="width:;"><a class="fancybox" href="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657157996/cvpr22/companies/IMG_0956_jj6a3z.jpg" target="_blank" rel="noopener" title="" data-caption="" data-fancybox="default"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657157996/cvpr22/companies/IMG_0956_jj6a3z.jpg" alt=""></a></div><div class="figure fig-50" style="width:;"><a class="fancybox" href="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657157996/cvpr22/companies/IMG_0963_t7xaox.jpg" target="_blank" rel="noopener" title="" data-caption="" data-fancybox="default"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657157996/cvpr22/companies/IMG_0963_t7xaox.jpg" alt=""></a></div><div class="figure fig-50" style="width:;"><a class="fancybox" href="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657157996/cvpr22/companies/IMG_0970_ogl7lg.jpg" target="_blank" rel="noopener" title="" data-caption="" data-fancybox="default"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657157996/cvpr22/companies/IMG_0970_ogl7lg.jpg" alt=""></a></div><div class="figure fig-100" style="width:;"><a class="fancybox" href="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657157996/cvpr22/companies/IMG_0955_kvmmd9.jpg" target="_blank" rel="noopener" title="" data-caption="" data-fancybox="default"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657157996/cvpr22/companies/IMG_0955_kvmmd9.jpg" alt=""></a></div><div class="figure fig-50" style="width:;"><a class="fancybox" href="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657157996/cvpr22/companies/IMG_0957_oji0vx.jpg" target="_blank" rel="noopener" title="" data-caption="" data-fancybox="default"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657157996/cvpr22/companies/IMG_0957_oji0vx.jpg" alt=""></a></div><div class="figure fig-50" style="width:;"><a class="fancybox" href="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657157996/cvpr22/companies/IMG_1011_pfk9j4.jpg" target="_blank" rel="noopener" title="" data-caption="" data-fancybox="default"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657157996/cvpr22/companies/IMG_1011_pfk9j4.jpg" alt=""></a></div><p>Tesla’s cyber truck is a lot larger than I imagined. </p><h3 id="Student-Activity-Speed-Mentoring"><a href="#Student-Activity-Speed-Mentoring" class="headerlink" title="Student Activity: Speed Mentoring"></a>Student Activity: Speed Mentoring</h3><p>Students attending CVPR has a chance to participate “speed mentoring”. Each table sits around students with one empty seat, and mentors will rotate between tables. Mentors are professors and senior researchers from the industry. The students are free to ask about any questions. </p><h3 id="Tesla-AI-Event"><a href="#Tesla-AI-Event" class="headerlink" title="Tesla AI Event"></a>Tesla AI Event</h3><p>Companies would invite authors of relevant field to their exclusive events during CVPR. I was fortunate enough to be invited to Tesla’s AI event.</p><div class="figure " style="width:;"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657158319/cvpr22/companies/IMG_0999_lfllkl.jpg" alt="Elon connecting from the Giga factory in Austin"><span class="caption">Elon connecting from the Giga factory in Austin</span></div><p>Elon Musk connected with the live audience and did an AMA. The head engineers at Tesla introduce their vision-first approach on self-driving. I also had a chance to test drive a prototype Tesla Modle 3 on New Orlean’s streets.</p><h3 id="Special-Event-at-Mardi-Gras-World"><a href="#Special-Event-at-Mardi-Gras-World" class="headerlink" title="Special Event at Mardi Gras World"></a>Special Event at Mardi Gras World</h3><div class="figure center" style="width:100%"; ><video class="fig-video" controls alt="Live Performance at Mardi Gras World"><source src="https://res.cloudinary.com/dxzx2bxch/video/upload/v1657158373/cvpr22/534_egugtu.mov" type="video/mp4"><p>Your browser doesn't support HTML5 Video :/</p></video><span class="caption">Live Performance at Mardi Gras World</span></div><p>This is CVPR’s reception event happening in parallel with Tesla’s event. I didn’t attend this event but here is a video from a friend who did. </p><h2 id="New-Orleans"><a href="#New-Orleans" class="headerlink" title="New Orleans"></a>New Orleans</h2><h3 id="The-Streets-and-the-Mississippi-River"><a href="#The-Streets-and-the-Mississippi-River" class="headerlink" title="The Streets and the Mississippi River"></a>The Streets and the Mississippi River</h3><div class="figure fig-50" style="width:;"><a class="fancybox" href="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657158636/cvpr22/new_orleans/IMG_0841_ihs8io.jpg" target="_blank" rel="noopener" title="" data-caption="" data-fancybox="default"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657158636/cvpr22/new_orleans/IMG_0841_ihs8io.jpg" alt=""></a></div><div class="figure fig-50" style="width:;"><a class="fancybox" href="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657158636/cvpr22/new_orleans/IMG_0832_ybyhs0.jpg" target="_blank" rel="noopener" title="" data-caption="" data-fancybox="default"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657158636/cvpr22/new_orleans/IMG_0832_ybyhs0.jpg" alt=""></a></div><div class="figure fig-100" style="width:;"><a class="fancybox" href="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657158829/cvpr22/new_orleans/IMG_0821_t5w6cm.jpg" target="_blank" rel="noopener" title="" data-caption="" data-fancybox="default"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657158829/cvpr22/new_orleans/IMG_0821_t5w6cm.jpg" alt=""></a></div><div class="figure fig-50" style="width:;"><a class="fancybox" href="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657158635/cvpr22/new_orleans/IMG_0828_sbsuas.jpg" target="_blank" rel="noopener" title="" data-caption="" data-fancybox="default"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657158635/cvpr22/new_orleans/IMG_0828_sbsuas.jpg" alt=""></a></div><div class="figure fig-50" style="width:;"><a class="fancybox" href="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657158635/cvpr22/new_orleans/IMG_0826_mvjqoa.jpg" target="_blank" rel="noopener" title="" data-caption="" data-fancybox="default"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657158635/cvpr22/new_orleans/IMG_0826_mvjqoa.jpg" alt=""></a></div><div class="figure fig-50" style="width:;"><a class="fancybox" href="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657158636/cvpr22/new_orleans/IMG_0837_g6bd6c.jpg" target="_blank" rel="noopener" title="" data-caption="" data-fancybox="default"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657158636/cvpr22/new_orleans/IMG_0837_g6bd6c.jpg" alt=""></a></div><div class="figure fig-50" style="width:;"><a class="fancybox" href="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657158636/cvpr22/new_orleans/IMG_1065_gypgo3.jpg" target="_blank" rel="noopener" title="" data-caption="" data-fancybox="default"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1657158636/cvpr22/new_orleans/IMG_1065_gypgo3.jpg" alt=""></a></div><p>New Orleans is a beatiful city, and it’s much hotter and humid than I expected. During the day it’s about 38 degree with 70% humidity. During the conference I visited the French Quarter and walked along the Mississippi River. </p><h3 id="Food"><a href="#Food" class="headerlink" title="Food"></a>Food</h3><div class="figure fig-50" style="width:;"><a class="fancybox" href="https://res.cloudinary.com/dxzx2bxch/image/upload/v1656969788/cvpr22/food/IMG_0846_rbjw4w.jpg" target="_blank" rel="noopener" title="" data-caption="" data-fancybox="default"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1656969788/cvpr22/food/IMG_0846_rbjw4w.jpg" alt=""></a></div><div class="figure fig-50" style="width:;"><a class="fancybox" href="https://res.cloudinary.com/dxzx2bxch/image/upload/v1656969789/cvpr22/food/IMG_0894_dbg5me.jpg" target="_blank" rel="noopener" title="" data-caption="" data-fancybox="default"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1656969789/cvpr22/food/IMG_0894_dbg5me.jpg" alt=""></a></div><div class="figure fig-100" style="width:;"><a class="fancybox" href="https://res.cloudinary.com/dxzx2bxch/image/upload/v1656969790/cvpr22/food/IMG_0773_oea7cl.jpg" target="_blank" rel="noopener" title="" data-caption="" data-fancybox="default"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1656969790/cvpr22/food/IMG_0773_oea7cl.jpg" alt=""></a></div><div class="figure fig-50" style="width:;"><a class="fancybox" href="https://res.cloudinary.com/dxzx2bxch/image/upload/v1656969789/cvpr22/food/IMG_0910_wppska.jpg" target="_blank" rel="noopener" title="" data-caption="" data-fancybox="default"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1656969789/cvpr22/food/IMG_0910_wppska.jpg" alt=""></a></div><div class="figure fig-50" style="width:;"><a class="fancybox" href="https://res.cloudinary.com/dxzx2bxch/image/upload/v1656969790/cvpr22/food/IMG_0914_xexnrv.jpg" target="_blank" rel="noopener" title="" data-caption="" data-fancybox="default"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1656969790/cvpr22/food/IMG_0914_xexnrv.jpg" alt=""></a></div><div class="figure fig-50" style="width:;"><a class="fancybox" href="https://res.cloudinary.com/dxzx2bxch/image/upload/v1656969790/cvpr22/food/IMG_1062_wh3rpp.jpg" target="_blank" rel="noopener" title="" data-caption="" data-fancybox="default"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1656969790/cvpr22/food/IMG_1062_wh3rpp.jpg" alt=""></a></div><div class="figure fig-50" style="width:;"><a class="fancybox" href="https://res.cloudinary.com/dxzx2bxch/image/upload/v1656969789/cvpr22/food/IMG_0848_hdo91n.jpg" target="_blank" rel="noopener" title="" data-caption="" data-fancybox="default"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1656969789/cvpr22/food/IMG_0848_hdo91n.jpg" alt=""></a></div><div class="figure fig-50" style="width:;"><a class="fancybox" href="https://res.cloudinary.com/dxzx2bxch/image/upload/v1656969814/cvpr22/food/IMG_0793_wtltxh.jpg" target="_blank" rel="noopener" title="" data-caption="" data-fancybox="default"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1656969814/cvpr22/food/IMG_0793_wtltxh.jpg" alt=""></a></div><div class="figure fig-50" style="width:;"><a class="fancybox" href="https://res.cloudinary.com/dxzx2bxch/image/upload/v1656969790/cvpr22/food/IMG_1061_acuxwn.jpg" target="_blank" rel="noopener" title="" data-caption="" data-fancybox="default"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1656969790/cvpr22/food/IMG_1061_acuxwn.jpg" alt=""></a></div><p>The seafood in New Orleans is supreme. I especially enjoyed the seafood gumbo and the crawfish pasta. The Gus world-famous fried chicken is also delicious, make sure to try it if you are in New Orleans.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;My experience attending and presenting at CVPR22&lt;/p&gt;
    
    </summary>
    
      <category term="Conference Journal" scheme="https://www.zzzdavid.tech/categories/Conference-Journal/"/>
    
    
  </entry>
  
  <entry>
    <title>Efficient Path Profiling</title>
    <link href="https://www.zzzdavid.tech/profiling/"/>
    <id>https://www.zzzdavid.tech/profiling/</id>
    <published>2022-03-02T05:00:00.000Z</published>
    <updated>2022-08-26T19:32:11.149Z</updated>
    
    <content type="html"><![CDATA[<!-- excerpt --><h1 id="Efficient-Path-Profiling"><a href="#Efficient-Path-Profiling" class="headerlink" title="Efficient Path Profiling"></a>Efficient Path Profiling</h1><h2 id="A-Blog-Digest"><a href="#A-Blog-Digest" class="headerlink" title="A Blog Digest"></a>A Blog Digest</h2><p><strong>By Jiajie Li and Niansong Zhang</strong></p><p align="center"><img src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1646243822/cs6120/2_ugrqoi.png" alt="alt_text" title="image_tooltip" style="zoom:25%;" /></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h3><p>This paper addresses how to count each path’s execution times in a CFG with as little cost as possible. The algorithm has three steps. First, transform the CFG into DAG with a single back edge by removing the loop and adding dummy edges. Second, assign each edge a number so that each path sums to a unique encoding. Third, select edges on the least frequently executed paths to place instrumentations, and push the numbers to instrumentations while ensuring each path still sums to a unique encoding.</p><h3 id="Background-amp-Motivation"><a href="#Background-amp-Motivation" class="headerlink" title="Background &amp; Motivation"></a>Background &amp; Motivation</h3><p>The “old” way to identify heavily executed paths in a program is to approximate path frequency with edge or vertex frequency. However, this estimation is not accurate.</p><p align="center"><img src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1646243822/cs6120/1_y0atni.png" alt="alt_text" title="image_tooltip" style="zoom:25%;" /></p><p align="center"><strong> Left: A Control-Flow Graph (CFG) labeled with edge frequency. Right: Two possible path profiling results with the same edge frequency as the left figure.  </strong></p><p>For example, the two path profiling results in the table correspond to the same edge frequency on the left. However, they identify two different most executed paths. Therefore, using edge frequency to estimate path frequency is unreliable. It is easy to find out that path profiling results could determine edge/block profiling results, while the opposite direction is not. </p><p>Also, given an edge profiling result, a commonly used heuristic to select a heavily executed path follows the most frequently executed edge out of a basic block, which identifies path ACDEF in the above example. However, this path is not the most frequently visited in both Prof1 and Prof2, showing the inaccuracy of edge/block profiling. </p><p>The naive way to count how many times each path executes is to exhaust every path in the CFG and label each one. However, this quickly becomes intractable as the CFG becomes large, or the CFG contains cycles with infinite paths. </p><p>In short, the motivation is to use an efficient path profiling method instead of traditional block/edge profiling to improve the accuracy without increasing too much effort. To have a preview, the overhead of path profiling is 30.9%, while the overhead of edge profiling is 16.1%. (You can decide whether this overhead is acceptable for you!)</p><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>The essential idea behind path profiling is to encode paths with states. When passing through edges, we increment a register and naturally arrive at a unique number for each path. Furthermore, we can reduce the times we increment the register by placing “instrumentations” on some edges. This process is also “reversible”: we can find out the exact path in the Control-Flow Graph (CFG) given the encoding of that path. </p><p>The paper starts from a CFG with a single back edge and generalizes to CFGs with multiple back edges. The experiment section evaluates the efficient path profiling algorithm across SPEC95 benchmarks and demonstrates a 30.9% runtime overhead on average.</p><h3 id="Generating-Path-Encodings"><a href="#Generating-Path-Encodings" class="headerlink" title="Generating Path Encodings"></a>Generating Path Encodings</h3><p>How do we generate unique encoding for each path? We use the following simplified example to illustrate intuition.</p><p align="center"><img src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1646251186/cs6120/Screen_Shot_2022-03-02_at_14.59.36_cpnltz.png" alt="alt_text" title="image_tooltip" style="zoom:20%;" /></p><p align="center"><strong>A simplified example of generating path encoding  </strong></p><p>There are four paths in the graph; we now assign a value to each edge to make the path sum unique. We label the first outgoing edge to zero at the root node. Since we know along that edge there are three paths, we label the second outgoing edge with three.  </p><p>The intuition of the path encoding algorithm is to count the number of paths along “the other” edges and increment by the number of paths. </p><p align="center"><img src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1646243822/cs6120/3_dmiy5c.png"><p><p align="center"><strong> The path encoding generation algorithm </strong></p><p>Therefore, we count the number of paths from leaf to node. In other words, we visit vertices in the reverse topological order. </p><p>[Discussion]: based on the discussion in class, we think the time complexity of this algorithm can be represented as both O(n*n) and O(e). Because every edge is only visited once. </p><h3 id="Instrumentation"><a href="#Instrumentation" class="headerlink" title="Instrumentation"></a>Instrumentation</h3><p>It turns out we don’t have to increment the register at every edge. Instead, we can select a subset of edges where we increment the register and still get unique path encodings, thus increasing the efficiency of the path profiling algorithm. We place instrumentations on the selected edges to denote register increments.</p><p>Instrumentation aims to minimize the profiling overhead, namely the latency and memory overhead of incrementing registers and counters. Concretely, we want to increment the register as few times as possible. Therefore, the paper develops an approach to select the least frequently executed edges based on a prior edge profiling result.</p><p align="center"><img src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1646244206/cs6120/instr_phdtwe.png"></p><p align="center"><strong>Three steps to place instrumentations </strong></p><h4 id="Grow-Maximum-Spanning-Tree"><a href="#Grow-Maximum-Spanning-Tree" class="headerlink" title="Grow Maximum Spanning Tree"></a>Grow Maximum Spanning Tree</h4><p>By definition, a spanning tree is a subgraph that contains all vertices of an undirected graph. When the edges in the graph are weighted, the maximum spanning tree is the spanning tree that maximizes the sum of edge weights. In this case, the weights on the edges are edge frequencies. </p><h4 id="Find-the-Chords"><a href="#Find-the-Chords" class="headerlink" title="Find the Chords"></a>Find the Chords</h4><p>The edges in the graph but not in the spanning tree are called chords. With a spanning tree maximizing the edge frequency sum, the left chords are the edges least frequently executed. Therefore, we place the instrumentations on these chords to minimize the times we increment the path encoding register. </p><h4 id="Push-the-Values"><a href="#Push-the-Values" class="headerlink" title="Push the Values"></a>Push the Values</h4><p>Correctness always comes first over efficiency. We need to ensure that each path still sums to a unique number after instrumentations are placed. The paper adopts the “event counting” algorithm to identify the unique loops formed by chords and spanning trees and “push” the value in the loop to the instrumentations. </p><p align="center"><img src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1646243822/cs6120/4_huqtyo.png" alt="alt_text" title="image_tooltip" style="zoom:40%;" /></p><p align="center"><strong>Use event counting algorithm to decide increment values on instrumentations.</strong></p><h3 id="Profiling-Arbitrary-Control-Flow"><a href="#Profiling-Arbitrary-Control-Flow" class="headerlink" title="Profiling Arbitrary Control-Flow"></a>Profiling Arbitrary Control-Flow</h3><p>General CFGs have nested loops, which results in multiple back edges such as the edges I-&gt;A, E-&gt;B shown in the following figure. We only consider CFGs with one back edge above, here we will see how to deal with CFGs with multiple back edges and how to transform this problem to the previous one. </p><p>Figure (a) has two back edges I-&gt;A and inner E-&gt;B. We add dummy edges A-&gt;B and E-&gt;I, and remove the back edge E-&gt;B. Note that A is the entry block, and I is the exit block in this CFG. So the dummy edges we add are the edge from the entry block to the destination node of the back edge, and the edge from the source node of the back edge to the exit block. Through this transformation, general CFGs are downgraded to the previous CFG problem we already solved. </p><p align="center"><img src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1646243822/cs6120/5_ajv4lx.png" alt="alt_text" title="image_tooltip" style="zoom:50%;" /></p><p align="center"><strong>Removing back edge and creating dummy edges for general CFGs</strong></p><p>[Discussion]<br>An intuitive way to understand why adding dummy edges works: the original back edge E-&gt;B is denoted as E-&gt;I-&gt;A-&gt;B in the transformed graph with the help of dummy edges. </p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p><img src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1646243822/cs6120/6_xpyzbz.png" alt="alt_text" title="image_tooltip"></p><p>Table 1 shows the experiment results on runtime overhead of path profiling (PP) and edge profiling (QPT). The runtime overhead of path profiling is 30.9%, while the overhead of edge profiling is 16.1%. </p><h4 id="Is-the-overhead-acceptable"><a href="#Is-the-overhead-acceptable" class="headerlink" title="Is the overhead acceptable?"></a>Is the overhead acceptable?</h4><p>We discussed it with the conclusion that it depends (oh, the ultimate answer!). </p><p>It depends on the application and how frequently you need to use path profiling. For example, if we use it in JIT compilation and only profile it once, then 30% overhead is acceptable for its improving performance. </p><p>It’s also worth noting that the result presented in this paper might be the worst-case scenario of runtime overhead because the compiled SPEC benchmarks execute very fast. But in real cases, we may profile at the bytecode level. Then the overhead ratio might be lower because bytecode execution is slower. </p><h3 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h3><h4 id="Is-it-efficient-enough"><a href="#Is-it-efficient-enough" class="headerlink" title="Is it efficient enough?"></a>Is it efficient enough?</h4><p>We need to run edge profiling first and determine where we will put our instrumentation. Only after that, can we run path profiling. This additional overhead makes readers think that the Ball-Larus method would not be as efficient as it claims in the paper. </p><p>Some other works also raise questions on it: Kumar et al. [1] claim that methods like Ball-Larus algorithm take significant time to identify profile points in the program, cannot be used in dynamic (JIT) compilation. </p><p>We think the phase of identifying instrumentation could be accelerated by only analyzing a tiny and representative sample of the entire program because we only need a rough idea for the edge profiling result. There is also an interesting idea to adopt static program analysis methods like “constant propagation” to know some deterministic value range of certain variables to know additional information about edge profiling without any runtime analysis. </p><h3 id="Work-built-upon-this-method"><a href="#Work-built-upon-this-method" class="headerlink" title="Work built upon this method"></a>Work built upon this method</h3><h4 id="What-applications-will-benefit-from-accurate-Path-Profiling"><a href="#What-applications-will-benefit-from-accurate-Path-Profiling" class="headerlink" title="What applications will benefit from accurate Path Profiling?"></a>What applications will benefit from accurate Path Profiling?</h4><p> Profile-driven compilers and software test coverage are two applications that would benefit from accurate path profiling mentioned in the paper. We further discussed how to perform the optimizations given the path profiling information. </p><ol><li><p>Profile-driven compilers</p><p> JIT (Just-In-Time) compilation: understand the hot path and do optimization like superblock (straight-line code with side exits).</p></li><li><p>Software test coverage</p><p> Quantify the adequacy of a test dataset by profiling a program and reporting unexecuted statements or control-flow. </p></li></ol><h4 id="Research-work"><a href="#Research-work" class="headerlink" title="Research work"></a>Research work</h4><p>For functions with many paths, allocating an array entry for all program paths is prohibitively expensive, if not feasible [2]. Vaswani et al. [1] propose preferential path profiling (PPP): separating interesting paths from other paths and assigning a set of unique and compact numbers to these interesting paths. This enables the PPP implementation to record path information in an array instead of a hash table. </p><h4 id="Real-use-cases"><a href="#Real-use-cases" class="headerlink" title="Real use cases"></a>Real use cases</h4><p>Ball-Laraus used to have an <a href="https://opensource.apple.com/source/llvmCore/llvmCore-3418.0.80/lib/Transforms/Instrumentation/PathProfiling.cpp.auto.html" target="_blank" rel="noopener">implementation</a> in-tree in LLVM. But it was unmaintained and was removed in <a href="https://github.com/llvm/llvm-project/commit/ea564946251eb425fafde97e6398ec52a9ff6bf8" target="_blank" rel="noopener">2013</a>. </p><h4 id="Impact"><a href="#Impact" class="headerlink" title="Impact"></a>Impact</h4><p>This idea of instrumentation has also inspired applications outside the compiler world. Sanghi et al. [3] uses similar packet execution path profiling method to detect anomaly data packets in data plane systems. Break Dancing [4] uses path tracing in software to mimic Intel CPU’s hardware branch tracing capabilities for ARM and AMD CPUs. Magklis et al. [5] uses dynamic path profiling to decide when to let CPU scale voltage and frequency to save power and boost efficiency. </p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] Kumar, R. Vinodh, B. Lakshmi Narayanan, and R. Govindarajan. “Dynamic path profile aided recompilation in a Java just-in-time compiler.” International Conference on High-Performance Computing. Springer, Berlin, Heidelberg, 2002.</p><p>[2] Vaswani, Kapil, Aditya V. Nori, and Trishul M. Chilimbi. “Preferential path profiling: compactly numbering interesting paths.” ACM Sigplan Notices 42.1 (2007): 351-362.</p><p>[3] Sanghi, A., Kadiyala, K. P., Tammana, P., &amp; Joshi, S. (2021, August). Anomaly Detection in Data Plane Systems using Packet Execution Paths. In Proceedings of the ACM SIGCOMM 2021 Workshop on Secure Programmable network INfrastructure (pp. 9-15).</p><p>[4] Marin, G., Alexandrov, A., &amp; Moseley, T. (2021, June). Break dancing: low overhead, architecture neutral software branch tracing. In Proceedings of the 22nd ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems (pp. 122-133).</p><p>[5] Magklis, G., Scott, M. L., Semeraro, G., Albonesi, D. H., &amp; Dropsho, S. (2003, May). Profile-based dynamic voltage and frequency scaling for a multiple clock domain microprocessor. In Proceedings of the 30th annual international symposium on Computer architecture (pp. 14-27).</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;A blog digest of the paper &lt;em&gt;Efficient path profiling&lt;/em&gt; by Ball, Thomas, and James R. Larus. Proceedings of the 29th Annual IEEE/ACM International Symposium on Microarchitecture. MICRO 29. IEEE, 1996.&lt;/p&gt;
    
    </summary>
    
      <category term="Paper Reading" scheme="https://www.zzzdavid.tech/categories/Paper-Reading/"/>
    
    
      <category term="Compiler" scheme="https://www.zzzdavid.tech/tags/Compiler/"/>
    
  </entry>
  
  <entry>
    <title>How to Set Up VNC Server on a Linux Machine</title>
    <link href="https://www.zzzdavid.tech/vnc/"/>
    <id>https://www.zzzdavid.tech/vnc/</id>
    <published>2021-02-24T18:45:23.000Z</published>
    <updated>2022-08-26T19:32:11.150Z</updated>
    
    <content type="html"><![CDATA[<!-- excerpt --><!-- toc --><p>First of all, what is VNC?</p><blockquote><p>In computing, Virtual Network Computing is a graphical desktop-sharing system that uses the Remote Frame Buffer protocol to remotely control another computer. It transmits the keyboard and mouse events from one computer to another, relaying the graphical-screen updates back in the other direction, over a network. Wikipedia</p></blockquote><p>Basically it’s a remote desktop controller for all platforms. </p><h2 id="Settings"><a href="#Settings" class="headerlink" title="Settings"></a>Settings</h2><p>I was asked to start a VNC server on a CentOS remote machine, for people to access from Windows laptops. The CentOS server already has <code>vncserver</code> installed, also I am using conda on it.</p><h2 id="Install-VNC-server"><a href="#Install-VNC-server" class="headerlink" title="Install VNC server"></a>Install VNC server</h2><p>For completeness, I shall include a description of how to install VNC server.</p><ol><li>Install a desktop on the remote machine</li></ol><p>We often access the remote server without GUI desktop. If it doesn’t have desktop installed, we need to do it ourselves.</p><p>For Ubuntu:<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-<span class="builtin-name">get</span> install xfce xfce4-goodies -y</span><br></pre></td></tr></table></figure><br>or for CentOS:<br><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$ </span>sudo yum -y groups install <span class="string">"GNOME Desktop"</span></span><br><span class="line"><span class="variable">$ </span>startx</span><br></pre></td></tr></table></figure></p><ol><li>Install <code>vncserver</code></li></ol><p>On Ubuntu:<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install tightvncserver -y</span><br></pre></td></tr></table></figure><br>or on CentOS:<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum install tigervnc-server</span><br></pre></td></tr></table></figure></p><p>If the installation is successful, we should be able to run<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vncserver</span><br></pre></td></tr></table></figure></p><h2 id="Start-VNC-server"><a href="#Start-VNC-server" class="headerlink" title="Start VNC server"></a>Start VNC server</h2><p>Start VNC server by running:<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vncserver</span><br></pre></td></tr></table></figure></p><p>Check if it is running by checking the log:<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">(base) niansong@auros-gaming3e-2:~$ cat ~/.vnc/auros-gaming3e-2\:1.log</span><br><span class="line">Xvnc TigerVNC 1.8.0 - built Nov  2 2018 19:05:14</span><br><span class="line">Copyright (C) 1999-2017 TigerVNC Team and many others (see README.txt)</span><br><span class="line">See http://www.tigervnc.org <span class="keyword">for</span> information on TigerVNC.</span><br><span class="line">Underlying X server release 12001000, The X.Org Foundation</span><br><span class="line">Wed Feb 24 11:00:32 2021</span><br><span class="line"> vncext:      VNC extension running!</span><br><span class="line"> vncext:      Listening <span class="keyword">for</span> VNC connections on all interface(s), port 5901</span><br><span class="line"> vncext:      created VNC server <span class="keyword">for</span> screen 0</span><br><span class="line">GLib-GIO-Message: 11:00:36.261: Using the <span class="string">'memory'</span> GSettings backend.  Your settings will not be saved or shared with other applications.</span><br><span class="line">Wed Feb 24 11:00:36 2021</span><br><span class="line"> ComparingUpdateTracker: 0 pixels <span class="keyword">in</span> / 0 pixels out</span><br><span class="line"> ComparingUpdateTracker: (1:-nan ratio)</span><br></pre></td></tr></table></figure></p><p>If the log looks like this, no error or exiting, we are good.</p><h3 id="idb-conflict-with-conda"><a href="#idb-conflict-with-conda" class="headerlink" title="idb conflict with conda"></a>idb conflict with conda</h3><p>I got following complain in the log:</p><p><code>gnome-session[4099]: WARNING: Could not make bus activated clients aware ofDISPLAY=:7 environment variable: Failed to connect to socket/tmp/dbus-TAq3EF94t4: Connection refused</code>ß</p><p>After trying many methods, I found that it was a dbus conflict casued by conda (who could have thought). We need to uninstall the dbus module in conda:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ conda uninstall dbus</span><br><span class="line">$ conda uninstall --offline dbus <span class="comment"># uninstall without internet</span></span><br></pre></td></tr></table></figure><h2 id="Connect-to-VNC-server"><a href="#Connect-to-VNC-server" class="headerlink" title="Connect to VNC server"></a>Connect to VNC server</h2><p>We need to forward the display port to localhost, and then connect to it with VNC Viewer.</p><p>First we run this command:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh -L 5901:127.0.0.1:5901 -C -N -l &lt;username&gt; &lt;ip&gt;</span><br></pre></td></tr></table></figure><ul><li><code>5901</code>: the port at which <code>vncserver</code> is running, check it in the log. 5901 is the default number.</li><li><code>&lt;username&gt;</code> is the username you input when running <code>vncserver</code></li><li><code>ip</code> is the server’s ip</li></ul><p>In my case:</p><div class="figure center" style="width:80%;"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1614150673/posts/ssh_iyyxkv.png" style="width:80%;"alt=""></div><p>This command will hang in there, no output is normal. Then we connect to <code>localhost:5901</code> on VNC Viewer:</p><div class="figure center" style="width:90%;"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1614150673/posts/VNC_viewer_qjaqw9.png" style="width:90%;"alt=""></div><p>If the connection is successful, we will be able to get the desktop GUI like this:</p><div class="figure center" style="width:90%;"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1614150676/posts/success_z6vplv.png" style="width:90%;"alt=""></div><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p><a href="https://github.com/TigerVNC/tigervnc/issues/592" target="_blank" rel="noopener">https://github.com/TigerVNC/tigervnc/issues/592</a><br><a href="https://www.techrepublic.com/article/how-to-install-a-vnc-server-on-linux/" target="_blank" rel="noopener">https://www.techrepublic.com/article/how-to-install-a-vnc-server-on-linux/</a><br><a href="https://www.techrepublic.com/article/how-to-install-a-gui-on-top-of-centos-7/" target="_blank" rel="noopener">https://www.techrepublic.com/article/how-to-install-a-gui-on-top-of-centos-7/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;A tutorial to setup VNC server on linux machines for remote GUI desktop access.&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorial" scheme="https://www.zzzdavid.tech/categories/Tutorial/"/>
    
    
  </entry>
  
  <entry>
    <title>Introduction to SystemC</title>
    <link href="https://www.zzzdavid.tech/SystemC/"/>
    <id>https://www.zzzdavid.tech/SystemC/</id>
    <published>2021-02-08T18:45:23.000Z</published>
    <updated>2022-08-26T19:32:11.148Z</updated>
    
    <content type="html"><![CDATA[<!-- excerpt --><h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><blockquote><p>Loosely speaking, SystemC allows a user to write a set of C++ functions (processes) that are executed under control of a scheduler in an order that mimics the pasasge of simulated time and that are synchronized and communicate in a way that is useful for modeling electronic systems containing hardware and embedded software.</p></blockquote><p>The <em>module</em> is the basic buidling block of a system. A module can contain the following:</p><ul><li>Ports</li><li>Exports</li><li>Channels</li><li>Processes</li><li>Events</li><li>Instances of other modules</li><li>Other data members and functions</li></ul><p>They are all implemented as C++ classes. The base class of them is <code>sc_object</code>.</p><h2 id="Concepts"><a href="#Concepts" class="headerlink" title="Concepts"></a>Concepts</h2><h4 id="Scheduler"><a href="#Scheduler" class="headerlink" title="Scheduler"></a>Scheduler</h4><p>At the core of SystemC is a simulation engine containing a process scheduler. Processes are executed in response to the notification of events. Events are notified at specific points in simulated time. </p><h4 id="Elaboration-and-simulation"><a href="#Elaboration-and-simulation" class="headerlink" title="Elaboration and simulation"></a>Elaboration and simulation</h4><p>The execution of SystemC application consists of elaboration and simulation.<br>During elaboration, the module hierarchy is created, followed by simulation, during which the scheduler runs. </p><h4 id="Kernel"><a href="#Kernel" class="headerlink" title="Kernel"></a>Kernel</h4><p>The kernel is the part of SystemC class library implementation that provides the core functionality for elaboration and the scheduler.</p><h4 id="Processes-threads"><a href="#Processes-threads" class="headerlink" title="Processes (threads)"></a>Processes (threads)</h4><p>Processes are C++ functions registered with kernel. Processes are used to perform computations and hence to model the functionality of a system. They can be created statically during elaboration or dynamically during simulation.</p><h4 id="Sensitivity"><a href="#Sensitivity" class="headerlink" title="Sensitivity"></a>Sensitivity</h4><p>The sensitivity of a process identifies the set of events that would cause the scheduler to execute that process should those events be notified. Both <em>static</em> and <em>dynamic</em> sensitivity are provided. Static sensitivity is created at the time the process instance is created. Dynamic sensitivity is created during the execution of the process’s associated function. </p><h4 id="Channels-and-interfaces"><a href="#Channels-and-interfaces" class="headerlink" title="Channels and interfaces"></a>Channels and interfaces</h4><p>Interfaces are abstract classes that declare a set of pure virtual functions. These virtual functions are <em>implemented</em> by Channel. In other words, a channel is said to implement an interface if it defines all of the methods declared in that interface.<br>Channels serve to encapsulate the mechanism through which processes communicate and hence to model the communication aspects or protocols of a system. It can be used for inter-module communication or inter-process communication within a module.<br>Since the methods of a interface are virtual functions, they are typically called through an interface. A channel may implement more than one interface, and a single interface may be implemented by more than one channel.</p><h4 id="Ports-and-exports"><a href="#Ports-and-exports" class="headerlink" title="Ports and exports"></a>Ports and exports</h4><p>A port specifies that a particular interface is required by a module, whereas an export spefices that a particular interface is provided by a module.<br>Ports and exports forward method calls from the processes within a module to channels, to which those ports and exports are bound. Ports can only forward method calls <em>up</em> or <em>out</em> of a module, whereas exports can only forward method calls <em>down</em> or <em>into</em> a module.</p><h1 id="Tutorial"><a href="#Tutorial" class="headerlink" title="Tutorial"></a>Tutorial</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Combinational example:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># <span class="meta-keyword">include</span> <span class="meta-string">&lt;systemc.h&gt;</span></span></span><br><span class="line">SC_MODULE ( <span class="keyword">and</span> ) &#123;</span><br><span class="line">    sc_in&lt;DT&gt;   a;</span><br><span class="line">    sc_in&lt;DT&gt;   b;</span><br><span class="line">    sc_out&lt;DT&gt;  f;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">func</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        f.write(a.read() &amp; b.read())</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    SC_CTOR( <span class="keyword">and</span> ) &#123;</span><br><span class="line">       <span class="comment">// turn the function into a thread</span></span><br><span class="line">       SC_METHOD( func );</span><br><span class="line">       <span class="comment">// specify sensitivity</span></span><br><span class="line">       sensitive &lt;&lt; a &lt;&lt; b ;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>DT</code> is a placeholder, such as <code>sc_int&lt;4&gt;</code>, <code>sc_uint&lt;4&gt;</code></p><p>Sequential example:<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># <span class="meta-keyword">include</span> <span class="meta-string">&lt;systemc.h&gt;</span></span></span><br><span class="line">SC_MODULE ( <span class="keyword">and</span> ) &#123;</span><br><span class="line">    sc_in&lt;DT&gt;   a;</span><br><span class="line">    sc_in&lt;DT&gt;   b;</span><br><span class="line">    sc_in&lt;<span class="keyword">bool</span>&gt; clk;</span><br><span class="line">    sc_out&lt;DT&gt;  f;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">func</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        f.write(a.read() &amp; b.read())</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    SC_CTOR( <span class="keyword">and</span> ) &#123;</span><br><span class="line">       <span class="comment">// turn the function into a thread</span></span><br><span class="line">       SC_METHOD( func );</span><br><span class="line">       <span class="comment">// sensitive to positive edge of the clock</span></span><br><span class="line">       sensitive &lt;&lt; clk.pos();</span><br><span class="line">       <span class="comment">// sensitive &lt;&lt; clk.neg(); for negative edge</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p><h3 id="SC-CTOR"><a href="#SC-CTOR" class="headerlink" title="SC_CTOR"></a>SC_CTOR</h3><ul><li>The module constructor</li><li>Put the name of the module in the paranthesis</li><li>Inside <code>{}</code> we need to construct the processes and sensitivity list</li></ul><h3 id="Port-I-O"><a href="#Port-I-O" class="headerlink" title="Port I/O"></a>Port I/O</h3><ul><li>SystemC uses functions to read from <code>sc_in&lt;&gt;</code> or write to <code>sc_out&lt;&gt;</code><ul><li><code>.read()</code></li><li><code>.write()</code></li></ul></li><li>Examples<ul><li><code>x = inp.read();</code></li><li><code>outp.write(val);</code></li></ul></li></ul><h3 id="Processes-thread"><a href="#Processes-thread" class="headerlink" title="Processes (thread)"></a>Processes (thread)</h3><ul><li>A thread is a function made to act like a hardware process<ul><li>Runs concurrently</li><li>Sensitive to signals, clock edges, or fixed amounts of simulation time</li><li>Not called by the user, always active</li></ul></li><li>SystemC supports 3 kinds of threads<ul><li><code>SC_METHOD()</code><ul><li>Executes once every sensitivity event</li><li>Runs continuously</li><li>Analogous to a Verilog <code>@always</code> block</li><li>Synthesizable</li><li>Useful for combinational expressions or simple sequential logic that finishes in 1 clock cycle</li></ul></li><li><code>SC_THREAD()</code><ul><li>Runs only once at start of simulation them suspends itself when done</li><li>Can contain an infinite loop to execute code at fixed rate of time</li><li>NOT Synthesizable</li><li>Useful in testbenches to describe clocks or initial startup signal sequences</li></ul></li><li><code>SC_CTHREAD()</code><ul><li>C means “clocked”</li><li>Runs continuously</li><li>References a clock edge</li><li>Synthesizable</li><li>Can take <strong>one or more</strong> clock cycles to execute a single iteration</li><li>Used in 99% of all high-level behavioral designs</li></ul></li></ul></li></ul><h2 id="SC-CTHREAD-Clocked-Threads"><a href="#SC-CTHREAD-Clocked-Threads" class="headerlink" title="SC_CTHREAD Clocked Threads"></a>SC_CTHREAD Clocked Threads</h2><ul><li><code>SC_THREAD()</code> only support one-cycle operations, not much difference with RTL code</li><li><code>SC_CTHREAD()</code>is not limited to one cycle<ul><li>Can contain continuous loops</li><li>Can contain large blocks of code with operations of controls</li><li>Good for behavior description</li></ul></li></ul><p>Let’s look at a FIR example:<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;systemc.h&gt;</span></span></span><br><span class="line">SC_MODULE( fir ) &#123;</span><br><span class="line">    sc_in&lt;<span class="keyword">bool</span>&gt; clk;</span><br><span class="line">    sc_in&lt;<span class="keyword">bool</span>&gt; rst;</span><br><span class="line">    sc_in&lt; sc_int&lt;<span class="number">16</span>&gt; &gt; inp;</span><br><span class="line">    sc_out&lt; sc_int&lt;<span class="number">16</span>&gt; &gt; outp;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">fir_main</span><span class="params">()</span></span>; <span class="comment">// just the declaration</span></span><br><span class="line">    </span><br><span class="line">    SC_CTOR( fir ) &#123;</span><br><span class="line">        SC_CTHREAD( fir_main, clk.pos() ); <span class="comment">// second arg is sensitivity</span></span><br><span class="line">        reset_signal_is(rst, <span class="literal">true</span>); <span class="comment">// reset signal, true means reset is asserted high</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p>This is how to define compute with reset:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Coefficients for each FIR</span></span><br><span class="line"><span class="keyword">const</span> sc_uint&lt;<span class="number">8</span>&gt; coef[<span class="number">5</span>] = &#123;</span><br><span class="line">    <span class="number">18</span>, <span class="number">77</span>, <span class="number">108</span>, <span class="number">28</span>, <span class="number">21</span>, <span class="number">34</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// FIR Main thread</span></span><br><span class="line"><span class="keyword">void</span> fir::fir_main() &#123;</span><br><span class="line">    <span class="comment">// Reset code</span></span><br><span class="line">    <span class="comment">// Reset internal variables</span></span><br><span class="line">    <span class="comment">// Reset outputs</span></span><br><span class="line">    wait();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> ( <span class="literal">true</span> ) &#123;</span><br><span class="line">        <span class="comment">// Read inputs</span></span><br><span class="line">        <span class="comment">// Algorithm code</span></span><br><span class="line">        <span class="comment">// Write outputs</span></span><br><span class="line">        wait();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><code>wait()</code> means wait for one clock cycle</li><li>Everything from the beginning to the first <code>wait()</code> statement will become a reset state in the generated RTL</li><li>At each clock cycle the simulator checks if the reset is triggered. If it is, the process restarts from the beginning.</li></ul><p>Let’s write the FIR filter:<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span> fir::fir_main(<span class="keyword">void</span>) &#123;</span><br><span class="line">    <span class="comment">// Reset</span></span><br><span class="line">    outp.write(<span class="number">0</span>);</span><br><span class="line">    wait();</span><br><span class="line">    <span class="keyword">while</span>(<span class="literal">true</span>) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">5</span><span class="number">-1</span>; i &gt; <span class="number">0</span>; i--) &#123;</span><br><span class="line">            taps[i] = taps[i<span class="number">-1</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        taps[<span class="number">0</span>] = inp.read();</span><br><span class="line"></span><br><span class="line">        sc_int&lt;<span class="number">16</span>&gt; val;</span><br><span class="line">        <span class="keyword">for</span> ( <span class="keyword">int</span> i = <span class="number">0</span>; i &lt;<span class="number">5</span> ; i++) &#123;</span><br><span class="line">            val += coef[i] * taps[i];</span><br><span class="line">        &#125;</span><br><span class="line">        outp.write(val);</span><br><span class="line">        wait();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="Testbench"><a href="#Testbench" class="headerlink" title="Testbench"></a>Testbench</h2><p>Test environment:</p><pre><code>- SYSTEM  - testbench module  - design module</code></pre><p>Top level system module looks like this:<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">SC_MODULE( SYSTEM ) &#123;</span><br><span class="line">    <span class="comment">// Module declarations</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Local signal declarations</span></span><br><span class="line"></span><br><span class="line">    SC_CTOR(SYSTEM) &#123;</span><br><span class="line">        <span class="comment">// Module instance signal connections</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ~SYSTEM()</span><br><span class="line">        <span class="comment">// Destructor</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p>Now we code the actual top level program:</p><p><code>main.cc</code>:<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;systemc.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"fir.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"tb.h"</span></span></span><br><span class="line"></span><br><span class="line">SC_MODULE(SYSTEM) &#123;</span><br><span class="line">    <span class="comment">// note that we are declaring pointers</span></span><br><span class="line">    tb  *tb0;</span><br><span class="line">    fir *fir0;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// declare signals, note that clock is special</span></span><br><span class="line">    sc_signal&lt;<span class="keyword">bool</span>&gt; rst_sig;</span><br><span class="line">    sc_signal&lt; sc_int&lt;<span class="number">16</span>&gt; &gt; inp_sig;</span><br><span class="line">    sc_signal&lt; sc_int&lt;<span class="number">16</span>&gt; &gt; outp_sig;</span><br><span class="line">    sc_clock clk_sig;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// constructor to tie them together</span></span><br><span class="line">    SC_CTOR(SYSTEM)</span><br><span class="line">        <span class="comment">// first thing we do is parameterize clock signal</span></span><br><span class="line">        : clk_sig (<span class="string">"clk_sig"</span>, <span class="number">10</span>, SC_NS) <span class="comment">// this is a copy constructor</span></span><br><span class="line">        <span class="comment">// "clk_sig" is the name of the clock</span></span><br><span class="line">        <span class="comment">// 10 is the clock period, SC_NS is nano second, the unit of clock</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// to instantiate modules we use "new"</span></span><br><span class="line">        tb0 = <span class="keyword">new</span> tb(<span class="string">"tb0"</span>);</span><br><span class="line">        tb0-&gt;clk(clk_sig);</span><br><span class="line">        tb0-&gt;rst(rst_sig);</span><br><span class="line">        tb0-&gt;inp(inp_sig);</span><br><span class="line">        tb0-&gt;outp(outp_sig);</span><br><span class="line"></span><br><span class="line">        fir0 = <span class="keyword">new</span> fir(<span class="string">"fir0"</span>);</span><br><span class="line">        fir0-&gt;clk(clk_sig); <span class="comment">// thus the signals are connected</span></span><br><span class="line">        fir0-&gt;rst(rst_sig);</span><br><span class="line">        fir0-&gt;inp(inp_sig);</span><br><span class="line">        fir0-&gt;outp(outp_sig);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// destructor</span></span><br><span class="line">    ~SYSTEM() &#123;</span><br><span class="line">        <span class="comment">// memory housekeeping</span></span><br><span class="line">        <span class="keyword">delete</span> tb0;</span><br><span class="line">        <span class="keyword">delete</span> fir0;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">SYSTEM *top = <span class="literal">NULL</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">src_main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>* argv[])</span> <span class="comment">// our good'ol arg count and arg vector</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    top = <span class="keyword">new</span> SYSTEM(<span class="string">"top"</span>);</span><br><span class="line">    sc_start();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>Now we write testbench <code>tb.h</code>:<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;systemc.h&gt;</span></span></span><br><span class="line">SC_MODULE(tb) &#123;</span><br><span class="line">    <span class="comment">// note that the directions are inverted</span></span><br><span class="line">    sc_in&lt;<span class="keyword">bool</span>&gt; clk;</span><br><span class="line">    sc_out&lt;<span class="keyword">bool</span>&gt; rst;</span><br><span class="line">    sc_out&lt; sc_int&lt;<span class="number">16</span>&gt; &gt; inp;</span><br><span class="line">    sc_in&lt; sc_int&lt;<span class="number">16</span>&gt; &gt; outp;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// source produces the out signal</span></span><br><span class="line">    <span class="comment">// sink receives in signals</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">source</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">sink</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    SC_CTOR(tb) &#123;</span><br><span class="line">        SC_CTHREAD(source, clk.pos());</span><br><span class="line">        SC_CTHREAD(sink, clk.pos());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br><code>tb.cc</code><br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"tb.h"</span></span></span><br><span class="line"><span class="keyword">void</span> tb::source() &#123;</span><br><span class="line">    <span class="comment">// Reset</span></span><br><span class="line">    inp.write(<span class="number">0</span>);</span><br><span class="line">    rst.write(<span class="number">1</span>);</span><br><span class="line">    wait();</span><br><span class="line">    rst.write(<span class="number">0</span>);</span><br><span class="line">    wait(); </span><br><span class="line">    <span class="comment">// set rst to high, wait one cycle and set it to low, wait another cycle,</span></span><br><span class="line">    <span class="comment">// this way we generate a complete reset pulse</span></span><br><span class="line"></span><br><span class="line">    sc_int&lt;<span class="number">16</span>&gt; tmp;</span><br><span class="line">    <span class="comment">// send stimulus to FIR</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">64</span>; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (i &gt; <span class="number">23</span> &amp;&amp; i &lt; <span class="number">29</span>&gt;)</span><br><span class="line">            tmp = <span class="number">256</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            tmp = <span class="number">0</span>;</span><br><span class="line">        inp.write(tmp);</span><br><span class="line">        wait();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">```c++</span><br><span class="line"><span class="keyword">void</span> tb::sink() &#123;</span><br><span class="line">    sc_int&lt;<span class="number">16</span>&gt; indata;</span><br><span class="line">    <span class="comment">// read output coming from DUT</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">64</span>; i++) &#123;</span><br><span class="line">        indata = outp.read();</span><br><span class="line">        wait();</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; i &lt;&lt; <span class="string">" : \t"</span> &lt;&lt; indata.to_int() &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">        <span class="comment">// note the to_int()</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// finally, we stop the simulation</span></span><br><span class="line">    sc_stop();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://www.youtube.com/watch?v=-O2fkVbnqMo&amp;list=PLcvQHr8v8MQLj9tCYyOw44X1PLisEsX-J&amp;index=2" target="_blank" rel="noopener">FORTE’s Learn SystemC Youtube playlist</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;SystemC is a system-level modeling language, often applied to high-level synthesis.&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="https://www.zzzdavid.tech/categories/Research/"/>
    
      <category term="Tutorial" scheme="https://www.zzzdavid.tech/categories/Research/Tutorial/"/>
    
    
  </entry>
  
  <entry>
    <title>3D Scene Understanding</title>
    <link href="https://www.zzzdavid.tech/three-d/"/>
    <id>https://www.zzzdavid.tech/three-d/</id>
    <published>2020-12-21T18:45:23.000Z</published>
    <updated>2022-08-26T19:32:11.150Z</updated>
    
    <content type="html"><![CDATA[<!-- excerpt --><h1 id="3D-Datasets"><a href="#3D-Datasets" class="headerlink" title="3D Datasets"></a>3D Datasets</h1><h2 id="Data-Representations"><a href="#Data-Representations" class="headerlink" title="Data Representations"></a>Data Representations</h2><ol><li>Multiview images: multiple 2D images of the same object from different angles.</li><li>Depth map</li><li>Voxel: volumetric occupancy</li><li>Point cloud</li><li>Polygon mesh</li><li>Functions as implicit representation</li></ol><h2 id="ScanNet-Dataset"><a href="#ScanNet-Dataset" class="headerlink" title="ScanNet Dataset"></a>ScanNet Dataset</h2><ul><li>Scene: Indoor</li></ul><div class="figure center" style="width:100%;"><img class="fig-img" src="https://tva1.sinaimg.cn/large/0081Kckwgy1glxt6kwfzdj30me0hfqnt.jpg" style="width:100%;" alt="" div=""><p>ScanNet is a richly-annotated RGB-D video dataset. It has camera poses information, surface reconstruction, sementic segmentation labels.</p><h3 id="Data-Formats"><a href="#Data-Formats" class="headerlink" title="Data Formats"></a>Data Formats</h3><h4 id="ply-Reconstructed-surface-mesh-file"><a href="#ply-Reconstructed-surface-mesh-file" class="headerlink" title="*.ply Reconstructed surface mesh file"></a><code>*.ply</code> Reconstructed surface mesh file</h4><p>This is binary PLY format mesh with +Z axis in upright orientation.</p><h4 id="sens-RGB-D-sensor-stream"><a href="#sens-RGB-D-sensor-stream" class="headerlink" title="*.sens RGB-D sensor stream"></a><code>*.sens</code> RGB-D sensor stream</h4><p>This is binary format with per-frame color, depth, camera pose and other data.</p><h4 id="segs-json-Surface-mesh-segmentation-file"><a href="#segs-json-Surface-mesh-segmentation-file" class="headerlink" title="*.segs.json Surface mesh segmentation file"></a><code>*.segs.json</code> Surface mesh segmentation file</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">{</span><br><span class="line">  <span class="string">"params"</span>: {  <span class="comment">// segmentation parameters</span></span><br><span class="line">   <span class="string">"kThresh"</span>: <span class="string">"0.0001"</span>,</span><br><span class="line">   <span class="string">"segMinVerts"</span>: <span class="string">"20"</span>,</span><br><span class="line">   <span class="string">"minPoints"</span>: <span class="string">"750"</span>,</span><br><span class="line">   <span class="string">"maxPoints"</span>: <span class="string">"30000"</span>,</span><br><span class="line">   <span class="string">"thinThresh"</span>: <span class="string">"0.05"</span>,</span><br><span class="line">   <span class="string">"flatThresh"</span>: <span class="string">"0.001"</span>,</span><br><span class="line">   <span class="string">"minLength"</span>: <span class="string">"0.02"</span>,</span><br><span class="line">   <span class="string">"maxLength"</span>: <span class="string">"1"</span></span><br><span class="line">  },</span><br><span class="line">  <span class="string">"sceneId"</span>: <span class="string">"..."</span>,  <span class="comment">// id of segmented scene</span></span><br><span class="line">  <span class="string">"segIndices"</span>: [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">15</span>,<span class="number">15</span>,<span class="number">15</span>,<span class="number">15</span>],  <span class="comment">// per-vertex seg label</span></span><br><span class="line">}</span><br></pre></td></tr></table></figure><h3 id="Usage"><a href="#Usage" class="headerlink" title="Usage"></a>Usage</h3><p>Check <a href="https://github.com/chrischoy/SpatioTemporalSegmentation/blob/master/lib/pc_utils.py" target="_blank" rel="noopener">SpatioTemporalSegmentation/lib/pc_utils.py</a>.</p><p>To load a mesh file:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> plyfile <span class="keyword">import</span> PlyData</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_ply</span><span class="params">(self, index)</span>:</span></span><br><span class="line">    filepath = self.data_root / self.data_paths[index]</span><br><span class="line">    plydata = PlyData.read(filepath)</span><br><span class="line">    data = plydata.elements[<span class="number">0</span>].data</span><br><span class="line">    coords = np.array([data[<span class="string">'x'</span>], data[<span class="string">'y'</span>], data[<span class="string">'z'</span>]], dtype=np.float32).T</span><br><span class="line">    feats = np.array([data[<span class="string">'red'</span>], data[<span class="string">'green'</span>], data[<span class="string">'blue'</span>]], dtype=np.float32).T</span><br><span class="line">    labels = np.array(data[<span class="string">'label'</span>], dtype=np.int32)</span><br><span class="line">    <span class="keyword">return</span> coords, feats, labels, <span class="literal">None</span></span><br></pre></td></tr></table></figure></p><hr><h1 id="Tools"><a href="#Tools" class="headerlink" title="Tools"></a>Tools</h1><h2 id="Minkowski-Engine"><a href="#Minkowski-Engine" class="headerlink" title="Minkowski Engine"></a>Minkowski Engine</h2><blockquote><p>The Minkowski Engine is an auto-differentiation library for sparse tensors. It supports all standard network layers such as convolution, pooling, unpooling, and broadcasting operations for sparse tensors.</p></blockquote><p>3D voxel feature maps are often sparse. Minkowski Engine generalizes the definition of 3D sparse convolution and provide implementation support for it. The sparse convolution is defined as:</p><script type="math/tex; mode=display">\mathbf{x}_{\mathbf{u}}^{\text {out }}=\sum_{\mathbf{i} \in \mathcal{N}^{D}\left(\mathbf{u}, \mathcal{C}^{\text {in }}\right)} W_{\mathbf{i}} \mathbf{x}_{\mathbf{u}+\mathbf{i}}^{\text {in }} \text { for } \mathbf{u} \in \mathcal{C}^{\text {out }}</script><p>This definition does not limit the shape of the kernel.</p><ul><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.014ex" xmlns="http://www.w3.org/2000/svg" width="1.446ex" height="1.032ex" role="img" focusable="false" viewBox="0 -450 639 456" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-B-75"></use></g></g></g></g></svg></mjx-container> is the current convolution center.</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="10.867ex" height="2.826ex" role="img" focusable="false" viewBox="0 -999 4803.2 1249" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-C-4E"></use></g></g><g data-mml-node="mi" transform="translate(1060.8, 516.1) scale(0.707)"><use xlink:href="#MJX-TEX-I-44"></use></g></g><g data-mml-node="mo" transform="translate(1696.3, 0)"><use xlink:href="#MJX-TEX-N-28"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2085.3, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-B-75"></use></g></g><g data-mml-node="mo" transform="translate(2724.3, 0)"><use xlink:href="#MJX-TEX-N-2C"></use></g><g data-mml-node="msup" transform="translate(3169, 0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-C-43"></use></g></g><g data-mml-node="TeXAtom" transform="translate(527, 432.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-69"></use></g><g data-mml-node="mi" transform="translate(345, 0)"><use xlink:href="#MJX-TEX-I-6E"></use></g></g></g><g data-mml-node="mo" transform="translate(4414.2, 0)"><use xlink:href="#MJX-TEX-N-29"></use></g></g></g></svg></mjx-container> is a set of offsets that define the shape of a kernel, centered at <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.014ex" xmlns="http://www.w3.org/2000/svg" width="1.446ex" height="1.032ex" role="img" focusable="false" viewBox="0 -450 639 456" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-B-75"></use></g></g></g></g></svg></mjx-container>.</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.057ex" xmlns="http://www.w3.org/2000/svg" width="2.817ex" height="2.092ex" role="img" focusable="false" viewBox="0 -899.5 1245.2 924.5" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-C-43"></use></g></g><g data-mml-node="TeXAtom" transform="translate(527, 432.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-69"></use></g><g data-mml-node="mi" transform="translate(345, 0)"><use xlink:href="#MJX-TEX-I-6E"></use></g></g></g></g></g></svg></mjx-container>, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.057ex" xmlns="http://www.w3.org/2000/svg" width="3.574ex" height="2.036ex" role="img" focusable="false" viewBox="0 -874.7 1579.7 899.7" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-C-43"></use></g></g><g data-mml-node="TeXAtom" transform="translate(527, 432.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-6F"></use></g><g data-mml-node="mi" transform="translate(485, 0)"><use xlink:href="#MJX-TEX-I-75"></use></g><g data-mml-node="mi" transform="translate(1057, 0)"><use xlink:href="#MJX-TEX-I-74"></use></g></g></g></g></g></svg></mjx-container> are predefined input and output coordinates of sparse tensors.</li></ul><p>Minkowski Engine uses COO format (Coordinate list) to store sparse tensors, meaning that, it stores a list of non-zero coordinates and corresponding features:</p><script type="math/tex; mode=display">\mathbf{C}=\left[\begin{array}{cccc}x_{1}^{1} & x_{1}^{2} & \cdots & x_{1}^{D} \\\vdots & \vdots & \ddots & \vdots \\x_{N}^{1} & x_{N}^{2} & \cdots & x_{N}^{D}\end{array}\right], \mathbf{F}=\left[\begin{array}{c}\mathbf{f}_{1}^{T} \\\vdots \\\mathbf{f}_{N}^{T}\end{array}\right]</script><h3 id="Concepts"><a href="#Concepts" class="headerlink" title="Concepts"></a>Concepts</h3><p><strong>Coordinate Manager</strong><br>Because the feature map is sparse, we need to dynamically find neighbors between non-zero elements. Minkowski Engine has a mechanism called coordinate manager that caches and reuses calcualted neighbor coordinates. When a <code>MinkowskiEngine.SparseTensor</code> is initialized, a coordinate manager is also created. The coordinate manager can be accessed with <code>MinkowskiEngine.SparseTensor.coords_man</code>, and it can be shared with a new sparse tensor by providing it as an argument during initialization.</p><p><strong>Channel</strong><br>The channel number in 3D context is the length of the feature vector.</p><hr><h1 id="Paper-Digests"><a href="#Paper-Digests" class="headerlink" title="Paper Digests"></a>Paper Digests</h1><h2 id="PointContrast-Unsupervised-Pre-Training-for-3D-Point-Cloud-Understanding"><a href="#PointContrast-Unsupervised-Pre-Training-for-3D-Point-Cloud-Understanding" class="headerlink" title="PointContrast: Unsupervised Pre-Training for 3D Point Cloud Understanding"></a><a href="https://arxiv.org/abs/2007.10985" target="_blank" rel="noopener">PointContrast: Unsupervised Pre-Training for 3D Point Cloud Understanding</a></h2><ul><li>Authors: Saining Xie, Jiatao Gu, Demin Guo, Charles R. Qi, Leonidas Guibas, Or Litany</li><li>Venue: ECCV 2020</li><li>Institution: Facebook AI &amp; Stanford</li></ul><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>To design a full pipeline for 3D unsupervised pretraining: view generation methods, downstream task compatible backbone, contrastive loss design. Previous work (pretraining on ShapeNet) doesn’t work because of two reasons: source &amp; target data domain gap and lack of point-level representation.</p><h3 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h3><p><strong>PointContrast Pipeline: contrasting at point-level</strong></p><ol><li>Given point cloud <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.373ex" height="1.005ex" role="img" focusable="false" viewBox="0 -444 607 444" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-B-78"></use></g></g></g></g></svg></mjx-container>, generate two views <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="2.406ex" height="1.869ex" role="img" focusable="false" viewBox="0 -826.2 1063.6 826.2" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="msup"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-B-78"></use></g><g data-mml-node="mn" transform="translate(607, 363) scale(0.707)"><use xlink:href="#MJX-TEX-B-31"></use></g></g></g></g></g></svg></mjx-container> and <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="2.406ex" height="1.868ex" role="img" focusable="false" viewBox="0 -825.4 1063.6 825.4" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="msup"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-B-78"></use></g><g data-mml-node="mn" transform="translate(607, 363) scale(0.707)"><use xlink:href="#MJX-TEX-B-32"></use></g></g></g></g></g></svg></mjx-container> that are aligned in the same world coordinates. </li><li>Compute the correspndence mapping <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="2.378ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 1051 683" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-4D"></use></g></g></g></svg></mjx-container> between these two views. If <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="9.623ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4253.2 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><use xlink:href="#MJX-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(389, 0)"><use xlink:href="#MJX-TEX-I-69"></use></g><g data-mml-node="mo" transform="translate(734, 0)"><use xlink:href="#MJX-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(1178.7, 0)"><use xlink:href="#MJX-TEX-I-6A"></use></g><g data-mml-node="mo" transform="translate(1590.7, 0)"><use xlink:href="#MJX-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(2257.4, 0)"><use xlink:href="#MJX-TEX-N-2208"></use></g><g data-mml-node="mi" transform="translate(3202.2, 0)"><use xlink:href="#MJX-TEX-I-4D"></use></g></g></g></svg></mjx-container> then points <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.661ex" xmlns="http://www.w3.org/2000/svg" width="2.207ex" height="2.548ex" role="img" focusable="false" viewBox="0 -833.9 975.6 1126.1" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-78"></use></g><g data-mml-node="mn" transform="translate(572, 363) scale(0.707)"><use xlink:href="#MJX-TEX-N-31"></use></g><g data-mml-node="mi" transform="translate(572, -284.4) scale(0.707)"><use xlink:href="#MJX-TEX-I-69"></use></g></g></g></g></svg></mjx-container> and <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.97ex" xmlns="http://www.w3.org/2000/svg" width="2.207ex" height="2.857ex" role="img" focusable="false" viewBox="0 -833.9 975.6 1262.6" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-78"></use></g><g data-mml-node="mn" transform="translate(572, 363) scale(0.707)"><use xlink:href="#MJX-TEX-N-32"></use></g><g data-mml-node="mi" transform="translate(572, -284.4) scale(0.707)"><use xlink:href="#MJX-TEX-I-6A"></use></g></g></g></g></svg></mjx-container> are a matched pair.</li><li>Randomly sample two geometric transformations <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex" xmlns="http://www.w3.org/2000/svg" width="2.843ex" height="1.867ex" role="img" focusable="false" viewBox="0 -675 1256.6 825" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-B-54"></use></g><g data-mml-node="mn" transform="translate(800, -150) scale(0.707)"><use xlink:href="#MJX-TEX-B-31"></use></g></g></g></g></g></svg></mjx-container>, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex" xmlns="http://www.w3.org/2000/svg" width="2.843ex" height="1.867ex" role="img" focusable="false" viewBox="0 -675 1256.6 825" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-B-54"></use></g><g data-mml-node="mn" transform="translate(800, -150) scale(0.707)"><use xlink:href="#MJX-TEX-B-32"></use></g></g></g></g></g></svg></mjx-container> (translation, rotation, scaling), and apply each to the paired points.</li><li>Compute output point feature <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="18.019ex" height="2.435ex" role="img" focusable="false" viewBox="0 -826.2 7964.5 1076.2" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="msup"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-B-66"></use></g><g data-mml-node="mn" transform="translate(522.2, 363) scale(0.707)"><use xlink:href="#MJX-TEX-B-31"></use></g></g></g><g data-mml-node="mo" transform="translate(1256.6, 0)"><use xlink:href="#MJX-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(2312.3, 0)"><use xlink:href="#MJX-TEX-I-4E"></use></g><g data-mml-node="mi" transform="translate(3200.3, 0)"><use xlink:href="#MJX-TEX-I-4E"></use></g><g data-mml-node="mo" transform="translate(4088.3, 0)"><use xlink:href="#MJX-TEX-N-28"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(4477.3, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-B-54"></use></g><g data-mml-node="mn" transform="translate(800, -150) scale(0.707)"><use xlink:href="#MJX-TEX-B-31"></use></g></g></g><g data-mml-node="mo" transform="translate(5733.9, 0)"><use xlink:href="#MJX-TEX-N-28"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(6122.9, 0)"><g data-mml-node="msup"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-B-78"></use></g><g data-mml-node="mn" transform="translate(607, 363) scale(0.707)"><use xlink:href="#MJX-TEX-B-31"></use></g></g></g><g data-mml-node="mo" transform="translate(7186.5, 0)"><use xlink:href="#MJX-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(7575.5, 0)"><use xlink:href="#MJX-TEX-N-29"></use></g></g></g></svg></mjx-container> and <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="18.019ex" height="2.433ex" role="img" focusable="false" viewBox="0 -825.4 7964.5 1075.4" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="msup"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-B-66"></use></g><g data-mml-node="mn" transform="translate(522.2, 363) scale(0.707)"><use xlink:href="#MJX-TEX-B-32"></use></g></g></g><g data-mml-node="mo" transform="translate(1256.6, 0)"><use xlink:href="#MJX-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(2312.3, 0)"><use xlink:href="#MJX-TEX-I-4E"></use></g><g data-mml-node="mi" transform="translate(3200.3, 0)"><use xlink:href="#MJX-TEX-I-4E"></use></g><g data-mml-node="mo" transform="translate(4088.3, 0)"><use xlink:href="#MJX-TEX-N-28"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(4477.3, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-B-54"></use></g><g data-mml-node="mn" transform="translate(800, -150) scale(0.707)"><use xlink:href="#MJX-TEX-B-31"></use></g></g></g><g data-mml-node="mo" transform="translate(5733.9, 0)"><use xlink:href="#MJX-TEX-N-28"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(6122.9, 0)"><g data-mml-node="msup"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-B-78"></use></g><g data-mml-node="mn" transform="translate(607, 363) scale(0.707)"><use xlink:href="#MJX-TEX-B-32"></use></g></g></g><g data-mml-node="mo" transform="translate(7186.5, 0)"><use xlink:href="#MJX-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(7575.5, 0)"><use xlink:href="#MJX-TEX-N-29"></use></g></g></g></svg></mjx-container>.</li><li>Backprop to update NN with point-wise contrastive loss.</li></ol><p><strong>Contrastive Loss Design</strong><br>The intuition of contrastive loss is to seperate negative pairs and pull together positive pairs as much as possible.</p><span class="highlight-text green"> Hardest-Contrastive Loss </span><script type="math/tex; mode=display">\mathcal{L}_{c}=\sum_{(i, j) \in \mathcal{P}}\left\{\left[d\left(\mathbf{f}_{i}, \mathbf{f}_{j}\right)-m_{p}\right]_{+}^{2} /|\mathcal{P}|+0.5\left[m_{n}-\min _{k \in \mathcal{N}} d\left(\mathbf{f}_{i}, \mathbf{f}_{k}\right)\right]_{+}^{2} /\left|\mathcal{N}_{i}\right|+0.5\left[m_{n}-\min _{k \in \mathcal{N}} d\left(\mathbf{f}_{j}, \mathbf{f}_{k}\right)\right]_{+}^{2} /\left|\mathcal{N}_{j}\right|\right\}</script><ul><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex" xmlns="http://www.w3.org/2000/svg" width="1.418ex" height="1.923ex" role="img" focusable="false" viewBox="0 -700 626.6 850" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-B-66"></use></g><g data-mml-node="mi" transform="translate(351, -150) scale(0.707)"><use xlink:href="#MJX-TEX-B-69"></use></g></g></g></g></g></svg></mjx-container>, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.659ex" xmlns="http://www.w3.org/2000/svg" width="1.469ex" height="2.243ex" role="img" focusable="false" viewBox="0 -700 649.2 991.4" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-B-66"></use></g><g data-mml-node="mi" transform="translate(351, -150) scale(0.707)"><use xlink:href="#MJX-TEX-B-6A"></use></g></g></g></g></g></svg></mjx-container> are the features of matched points.</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="2.937ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1298 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-64"></use></g><g data-mml-node="mo" transform="translate(520, 0)"><use xlink:href="#MJX-TEX-N-28"></use></g><g data-mml-node="mo" transform="translate(909, 0)"><use xlink:href="#MJX-TEX-N-29"></use></g></g></g></svg></mjx-container>: Euclidean distance.</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.65ex" xmlns="http://www.w3.org/2000/svg" width="2.904ex" height="1.65ex" role="img" focusable="false" viewBox="0 -442 1283.7 729.2" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-6D"></use></g><g data-mml-node="mi" transform="translate(878, -150) scale(0.707)"><use xlink:href="#MJX-TEX-I-70"></use></g></g></g></g></svg></mjx-container> and <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="3.059ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 1352.3 599.8" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-6D"></use></g><g data-mml-node="mi" transform="translate(878, -150) scale(0.707)"><use xlink:href="#MJX-TEX-I-6E"></use></g></g></g></g></svg></mjx-container> are positive/negative margins, they are hyperparameters.</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.113ex" xmlns="http://www.w3.org/2000/svg" width="2.215ex" height="1.898ex" role="img" focusable="false" viewBox="0 -789 979 839" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-C-4E"></use></g></g></g></g></svg></mjx-container> is a randomly sampled set of non-matched (negative) points.<br><strong>hardest sample</strong> means the closest point in the <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="2.692ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 1190 727" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-C-4C"></use></g><g data-mml-node="mn" transform="translate(690, 0)"><use xlink:href="#MJX-TEX-C-32"></use></g></g></g></g></svg></mjx-container> normalized <em>feature space</em>. Because the closest negative point in the feature space is the hardest to separate. </li></ul><span class="highlight-text green"> PointInfoNCE Loss </span><script type="math/tex; mode=display">\mathcal{L}_{c}=-\sum_{(i, j) \in \mathcal{P}} \log \frac{\exp \left(\mathbf{f}_{i} \cdot \mathbf{f}_{j} / \tau\right)}{\sum_{(\cdot, k) \in \mathcal{P}} \exp \left(\mathbf{f}_{i} \cdot \mathbf{f}_{k} / \tau\right)}</script><p>PointInfoNCE is derived from InfoNCE. InfoNCE treat contrastive learning as classification problem, thus implemented with cross-entropy loss.</p><ul><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.029ex" xmlns="http://www.w3.org/2000/svg" width="1.17ex" height="1.005ex" role="img" focusable="false" viewBox="0 -431 517 444" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-3C4"></use></g></g></g></svg></mjx-container>, annealing temperature, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.029ex" xmlns="http://www.w3.org/2000/svg" width="6.951ex" height="1.186ex" role="img" focusable="false" viewBox="0 -511 3072.6 524" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-3C4"></use></g><g data-mml-node="mo" transform="translate(794.8, 0)"><use xlink:href="#MJX-TEX-N-2192"></use></g><g data-mml-node="mi" transform="translate(2072.6, 0)"><use xlink:href="#MJX-TEX-N-221E"></use></g></g></g></svg></mjx-container> the output distribution goes “flat”, i.e. positive pair become less distinguishable. <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="5.82ex" height="1.557ex" role="img" focusable="false" viewBox="0 -666 2572.6 688" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-3C4"></use></g><g data-mml-node="mo" transform="translate(794.8, 0)"><use xlink:href="#MJX-TEX-N-2192"></use></g><g data-mml-node="mn" transform="translate(2072.6, 0)"><use xlink:href="#MJX-TEX-N-30"></use></g></g></g></svg></mjx-container> the output distribution goes sharp, i.e. positive pair becomes more distinguishable.</li></ul><p><strong>Backbone Design</strong></p><div class="figure center" style="width:100%;"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1608727638/posts/Screen_Shot_2020-12-23_at_20.46.34_nwui76.png" style="width:100%;" alt="" div=""><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><h4 id="Why-Pretraining-with-ShapeNet-dataset-is-not-useful"><a href="#Why-Pretraining-with-ShapeNet-dataset-is-not-useful" class="headerlink" title="Why Pretraining with ShapeNet dataset is not useful?"></a>Why Pretraining with ShapeNet dataset is not useful?</h4><ul><li>Domain gap between source and target dataset: alignment, scene context …</li><li><strong>Point-level representation matters</strong>: local geometric feature is critical for 3D tasks<h4 id="PointInfoNCE-Loss-outperforms-Hardest-Contrastive-Loss"><a href="#PointInfoNCE-Loss-outperforms-Hardest-Contrastive-Loss" class="headerlink" title="PointInfoNCE Loss outperforms Hardest-Contrastive Loss"></a>PointInfoNCE Loss outperforms Hardest-Contrastive Loss</h4><h4 id="The-advantages-of-fully-convolutional-design"><a href="#The-advantages-of-fully-convolutional-design" class="headerlink" title="The advantages of fully-convolutional design"></a>The advantages of fully-convolutional design</h4></li><li>Don’t have to crop objects out from scene context as done in previous works.</li><li>Enables “point-level metric learning”.</li></ul><h3 id="Questions-and-Answers"><a href="#Questions-and-Answers" class="headerlink" title="Questions and Answers"></a>Questions and Answers</h3><p><strong>What is “view” and how is it generated?”</strong><br>View is partial point cloud seen from different viewpoints. We can generate two views by cropping them from a complete point cloud. “Aligning two views in the same world coordinates” means to register them, i.e., find the common matched points.</p><p><strong>How do we know the mapping between output features?</strong><br>The SRUNet’s output point cloud has the exact same coordinates with its input (just like segmentation). So we can use the input points’ mapping to map features in the output sparse tensors.</p><h1 id="Resources"><a href="#Resources" class="headerlink" title="Resources"></a>Resources</h1><h2 id="Paper-Lists"><a href="#Paper-Lists" class="headerlink" title="Paper Lists"></a>Paper Lists</h2><p><a href="https://github.com/zhulf0804/3D-PointCloud" target="_blank" rel="noopener">3D-PointCloud GitHub Repo</a></p><h2 id="Blog-Posts"><a href="#Blog-Posts" class="headerlink" title="Blog Posts"></a>Blog Posts</h2><p><a href="http://a-suozhang.xyz/2020/11/29/3d/" target="_blank" rel="noopener">3D related: View things from another dimension - Posted by Tianchen</a></p></div></div><svg style="display: none" id="MJX-SVG-global-cache"><defs><path id="MJX-TEX-B-75" d="M40 442L134 446Q228 450 229 450H235V273V165Q235 90 238 74T254 52Q268 46 304 46H319Q352 46 380 67T419 121L420 123Q424 135 425 199Q425 201 425 207Q425 233 425 249V316Q425 354 423 363T410 376Q396 380 369 380H356V442L554 450V267Q554 84 556 79Q561 62 610 62H623V31Q623 0 622 0Q603 0 527 -3T432 -6Q431 -6 431 25V56L420 45Q373 6 332 -1Q313 -6 281 -6Q208 -6 165 14T109 87L107 98L106 230Q106 358 104 366Q96 380 50 380H37V442H40Z"></path><path id="MJX-TEX-C-4E" d="M343 705Q358 705 358 698Q360 696 370 658T411 524T484 319Q536 174 590 82L595 73L615 152Q646 274 683 407Q729 571 752 637T799 727Q852 780 937 788Q939 788 947 788T958 789H962Q979 789 979 765Q979 722 951 692Q942 683 924 683Q888 681 859 672T818 654T803 639Q784 608 708 322T631 15Q631 14 630 15Q630 17 629 15Q628 14 628 12Q621 -4 601 -17T560 -31Q550 -31 546 -28T530 -7Q484 67 458 123T398 272Q352 392 314 514L306 535V534Q306 533 296 488T272 379T234 239T185 100T127 -7T61 -50Q34 -50 4 -34T-27 8Q-27 33 -12 61T18 90Q21 90 36 77T87 57H92Q109 57 123 78T162 173Q206 299 232 417T265 599T276 667Q284 681 304 693T343 705Z"></path><path id="MJX-TEX-I-44" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path><path id="MJX-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-TEX-C-43" d="M201 -25Q167 -25 136 -14T75 23T29 94T12 202Q12 290 50 394T161 574Q227 642 303 673T433 704Q435 705 457 705Q533 701 533 640Q533 606 507 548T464 474Q431 444 396 444Q381 444 381 453Q381 459 388 473T407 513T428 563Q433 580 433 594Q433 636 381 636Q314 636 260 594T175 489T128 363T112 247Q112 157 153 101T273 44Q347 44 398 121Q413 144 437 157T481 171Q496 171 496 160Q496 150 476 123Q426 56 350 16T201 -25Z"></path><path id="MJX-TEX-I-69" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-TEX-I-6E" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-TEX-I-6F" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path><path id="MJX-TEX-I-75" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-TEX-I-74" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path><path id="MJX-TEX-B-78" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z"></path><path id="MJX-TEX-B-31" d="M481 0L294 3Q136 3 109 0H96V62H227V304Q227 546 225 546Q169 529 97 529H80V591H97Q231 591 308 647L319 655H333Q355 655 359 644Q361 640 361 351V62H494V0H481Z"></path><path id="MJX-TEX-B-32" d="M175 580Q175 578 185 572T205 551T215 510Q215 467 191 449T137 430Q107 430 83 448T58 511Q58 558 91 592T168 640T259 654Q328 654 383 637Q451 610 484 563T517 459Q517 401 482 360T368 262Q340 243 265 184L210 140H274Q416 140 429 145Q439 148 447 186T455 237H517V233Q516 230 501 119Q489 9 486 4V0H57V25Q57 51 58 54Q60 57 109 106T215 214T288 291Q364 377 364 458Q364 515 328 553T231 592Q214 592 201 589T181 584T175 580Z"></path><path id="MJX-TEX-I-4D" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path><path id="MJX-TEX-I-6A" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path><path id="MJX-TEX-N-2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path><path id="MJX-TEX-I-78" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-TEX-B-54" d="M41 425Q41 426 51 545T62 669V675H737V669Q738 665 748 546T758 425V419H696V425Q687 517 669 555T595 607Q578 612 522 613H478V62H631V0H615Q585 3 399 3Q214 3 184 0H168V62H321V613H277H263Q164 613 134 561Q113 527 103 425V419H41V425Z"></path><path id="MJX-TEX-B-66" d="M308 0Q290 3 172 3Q58 3 49 0H40V62H109V382H42V444H109V503L110 562L112 572Q127 625 178 658T316 699Q318 699 330 699T348 700Q381 698 404 687T436 658T449 629T452 606Q452 576 432 557T383 537Q355 537 335 555T314 605Q314 635 328 649H325Q311 649 293 644T253 618T227 560Q226 555 226 498V444H340V382H232V62H318V0H308Z"></path><path id="MJX-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-TEX-I-4E" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path><path id="MJX-TEX-B-69" d="M72 610Q72 649 98 672T159 695Q193 693 217 670T241 610Q241 572 217 549T157 525Q120 525 96 548T72 610ZM46 442L136 446L226 450H232V62H294V0H286Q271 3 171 3Q67 3 49 0H40V62H109V209Q109 358 108 362Q103 380 55 380H43V442H46Z"></path><path id="MJX-TEX-B-6A" d="M104 610Q104 649 130 672T191 695Q225 693 249 670T273 610Q273 572 249 549T189 525Q152 525 128 548T104 610ZM78 442L173 446L268 450H274V196Q274 -5 274 -37T269 -83Q256 -132 201 -166T71 -200Q10 -200 -30 -173T-71 -102Q-71 -70 -51 -51T-1 -31Q27 -31 48 -49T69 -100Q69 -121 53 -147H56Q66 -149 77 -149H80Q90 -149 100 -146T127 -125T149 -73Q151 -55 151 149V362Q150 364 148 366T145 370T142 373T138 375T133 377T124 378T113 379T97 380H75V442H78Z"></path><path id="MJX-TEX-I-64" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path><path id="MJX-TEX-I-6D" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-TEX-I-70" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path><path id="MJX-TEX-C-4C" d="M62 -22T47 -22T32 -11Q32 -1 56 24T83 55Q113 96 138 172T180 320T234 473T323 609Q364 649 419 677T531 705Q559 705 578 696T604 671T615 645T618 623V611Q618 582 615 571T598 548Q581 531 558 520T518 509Q503 509 503 520Q503 523 505 536T507 560Q507 590 494 610T452 630Q423 630 410 617Q367 578 333 492T271 301T233 170Q211 123 204 112L198 103L224 102Q281 102 369 79T509 52H523Q535 64 544 87T579 128Q616 152 641 152Q656 152 656 142Q656 101 588 40T433 -22Q381 -22 289 1T156 28L141 29L131 20Q111 0 87 -11Z"></path><path id="MJX-TEX-C-32" d="M55 334Q55 386 105 419T236 453Q333 453 390 413T448 307Q448 278 437 256T406 218T365 193T318 172T277 151L248 134Q219 118 191 102T163 84T267 83L382 85H391Q399 99 406 126Q410 143 413 145T429 148Q440 148 442 147T449 139Q449 137 435 73T420 7Q420 6 414 0H233Q94 0 71 0T46 5Q46 5 46 6Q44 8 44 24Q44 39 46 41Q47 44 98 78T212 155T294 212Q347 257 347 304Q347 354 306 380T203 407Q150 407 120 377Q118 375 123 373Q146 362 146 332Q146 315 133 302T101 288Q85 288 70 298T55 334Z"></path><path id="MJX-TEX-I-3C4" d="M39 284Q18 284 18 294Q18 301 45 338T99 398Q134 425 164 429Q170 431 332 431Q492 431 497 429Q517 424 517 402Q517 388 508 376T485 360Q479 358 389 358T299 356Q298 355 283 274T251 109T233 20Q228 5 215 -4T186 -13Q153 -13 153 20V30L203 192Q214 228 227 272T248 336L254 357Q254 358 208 358Q206 358 197 358T183 359Q105 359 61 295Q56 287 53 286T39 284Z"></path><path id="MJX-TEX-N-2192" d="M56 237T56 250T70 270H835Q719 357 692 493Q692 494 692 496T691 499Q691 511 708 511H711Q720 511 723 510T729 506T732 497T735 481T743 456Q765 389 816 336T935 261Q944 258 944 250Q944 244 939 241T915 231T877 212Q836 186 806 152T761 85T740 35T732 4Q730 -6 727 -8T711 -11Q691 -11 691 0Q691 7 696 25Q728 151 835 230H70Q56 237 56 250Z"></path><path id="MJX-TEX-N-221E" d="M55 217Q55 305 111 373T254 442Q342 442 419 381Q457 350 493 303L507 284L514 294Q618 442 747 442Q833 442 888 374T944 214Q944 128 889 59T743 -11Q657 -11 580 50Q542 81 506 128L492 147L485 137Q381 -11 252 -11Q166 -11 111 57T55 217ZM907 217Q907 285 869 341T761 397Q740 397 720 392T682 378T648 359T619 335T594 310T574 285T559 263T548 246L543 238L574 198Q605 158 622 138T664 94T714 61T765 51Q827 51 867 100T907 217ZM92 214Q92 145 131 89T239 33Q357 33 456 193L425 233Q364 312 334 337Q285 380 233 380Q171 380 132 331T92 214Z"></path><path id="MJX-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></defs></svg>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Deep learning methods for 3D scene understanding, particularly focused unsupervised methods.&lt;/p&gt;
    
    </summary>
    
      <category term="Readings" scheme="https://www.zzzdavid.tech/categories/Readings/"/>
    
      <category term="Research" scheme="https://www.zzzdavid.tech/categories/Readings/Research/"/>
    
    
  </entry>
  
  <entry>
    <title>Gumbel-Softmax</title>
    <link href="https://www.zzzdavid.tech/GumbelSoftmax/"/>
    <id>https://www.zzzdavid.tech/GumbelSoftmax/</id>
    <published>2020-12-21T18:45:23.000Z</published>
    <updated>2022-08-26T19:32:11.146Z</updated>
    
    <content type="html"><![CDATA[<!-- excerpt --><h2 id="Why-are-we-interested-in-Gumbel-Softmax"><a href="#Why-are-we-interested-in-Gumbel-Softmax" class="headerlink" title="Why are we interested in Gumbel-Softmax?"></a>Why are we interested in Gumbel-Softmax?</h2><p>Gumbel-Softmax makes categorical sampling <em>differentiable</em>. Why is that important? Because sometimes we want to optimize discrete or categorical choices with gradient methods. One of the applications is that we can use it in differentiable neural architecture search when we want to decide the operation on one edge.</p><div class="figure center" style="width:100%;"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1610715531/posts/Screen_Shot_2021-01-15_at_20.58.17_mcenuy.png" style="width:100%;" alt="DARTS: Differentiable Architecture Search"><span class="caption">DARTS: Differentiable Architecture Search</span></div><p>The original DARTS paper did not introduce Gumbel-Softmax. To decide which operation should be on one edge, it does the following things:</p><ol><li>During search, the output of each operation is joined (weighted sum) with a group of learnable parameters (<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex" xmlns="http://www.w3.org/2000/svg" width="2.361ex" height="1.339ex" role="img" focusable="false" viewBox="0 -442 1043.6 592" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-3B1"></use></g><g data-mml-node="mn" transform="translate(640, -150) scale(0.707)"><use xlink:href="#MJX-TEX-N-31"></use></g></g></g></g></svg></mjx-container>, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex" xmlns="http://www.w3.org/2000/svg" width="2.361ex" height="1.339ex" role="img" focusable="false" viewBox="0 -442 1043.6 592" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-3B1"></use></g><g data-mml-node="mn" transform="translate(640, -150) scale(0.707)"><use xlink:href="#MJX-TEX-N-32"></use></g></g></g></g></svg></mjx-container>, …, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="2.521ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 1114.3 599.8" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-3B1"></use></g><g data-mml-node="mi" transform="translate(640, -150) scale(0.707)"><use xlink:href="#MJX-TEX-I-6E"></use></g></g></g></g></svg></mjx-container>), <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.357ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 600 453" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-6E"></use></g></g></g></svg></mjx-container> is the number of operation choices), like the color lines in the above figure.</li><li>When search is done, a final architecture is derived, meaning that we have to choose an operation for each edge. How do we do that? We use <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex" xmlns="http://www.w3.org/2000/svg" width="7.774ex" height="1.464ex" role="img" focusable="false" viewBox="0 -442 3436 647" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-61"></use></g><g data-mml-node="mi" transform="translate(529, 0)"><use xlink:href="#MJX-TEX-I-72"></use></g><g data-mml-node="mi" transform="translate(980, 0)"><use xlink:href="#MJX-TEX-I-67"></use></g><g data-mml-node="mi" transform="translate(1457, 0)"><use xlink:href="#MJX-TEX-I-6D"></use></g><g data-mml-node="mi" transform="translate(2335, 0)"><use xlink:href="#MJX-TEX-I-61"></use></g><g data-mml-node="mi" transform="translate(2864, 0)"><use xlink:href="#MJX-TEX-I-78"></use></g></g></g></svg></mjx-container> to choose the operation with the largest <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.448ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 640 453" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-3B1"></use></g></g></g></svg></mjx-container>.</li></ol><p>But, doing so causes a “gap”: the original network has a weighted sum of all operations during training, but the derived network loses all of the unchosen operation’s information, maybe the derived network has a different final performance than expected. </p><p>“If only we can train <em>one</em> network each time during search!” one might say. Indeed, selecting one operation for an edge during search instead of using a weighted sum is a good idea. But, sampling itself is not differentiable, we can’t update the parameters with gradient descent. </p><p>Don’t worry, Gumbel-Softmax is coming to rescue.</p><h2 id="Gumbel-distribution"><a href="#Gumbel-distribution" class="headerlink" title="Gumbel distribution"></a>Gumbel distribution</h2><p>First we need to understand what is Gumbel distribution. </p><p>Gumbel distribution stands for <strong>Generalized Extreme Value distribution Type-I</strong>. It is used to model the distirbution of the maximum of various distributions. Gumbel distribution is a particular case of the generalized extreme value distribution. </p><p>Gumbel distribution has two parameters:</p><ul><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.489ex" xmlns="http://www.w3.org/2000/svg" width="1.364ex" height="1.489ex" role="img" focusable="false" viewBox="0 -442 603 658" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-3BC"></use></g></g></g></svg></mjx-container>: location</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="1.281ex" height="2.034ex" role="img" focusable="false" viewBox="0 -705 566 899" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-3B2"></use></g></g></g></svg></mjx-container>: scale, larger <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="1.281ex" height="2.034ex" role="img" focusable="false" viewBox="0 -705 566 899" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-3B2"></use></g></g></g></svg></mjx-container> leads to fatter distribution.</li></ul><p>The PDF of Gumbel distribution is:<br><div class="figure center" style="width:70%;"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1610713072/600px-Gumbel-Density.svg_ydkffu.png" style="width:70%;" alt="" div="" p=""><script type="math/tex; mode=display">PDF(x) = \frac{1}{\beta} e^{-(a+e^{-z})}</script><p>where <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.091ex" xmlns="http://www.w3.org/2000/svg" width="8.189ex" height="3.138ex" role="img" focusable="false" viewBox="0 -905 3619.5 1387.2" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-7A"></use></g><g data-mml-node="mo" transform="translate(742.8, 0)"><use xlink:href="#MJX-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(1798.6, 0)"><g data-mml-node="mrow" transform="translate(220, 492.7) scale(0.707)"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-78"></use></g><g data-mml-node="mo" transform="translate(572, 0)"><use xlink:href="#MJX-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(1350, 0)"><use xlink:href="#MJX-TEX-I-3BC"></use></g></g><g data-mml-node="mi" transform="translate(710.4, -345) scale(0.707)"><use xlink:href="#MJX-TEX-I-3B2"></use></g><rect width="1581" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container>. We see it has two exponential functions, so it is also known as the double-exponential distribution.</p><h2 id="What-is-Gumbel-Softmax"><a href="#What-is-Gumbel-Softmax" class="headerlink" title="What is Gumbel-Softmax?"></a>What is Gumbel-Softmax?</h2><p>When we “choose an operation for one edge”, what we are doing is actually drawing a sample from a categorical distribution. A categorical distribution means a random variable can take the value of many discrete categories, with each case’s probability known. </p><p>Let’s say, we have a categorical variable <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.052ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 465 453" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-7A"></use></g></g></g></svg></mjx-container> with class probabilities <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex" xmlns="http://www.w3.org/2000/svg" width="2.203ex" height="1.314ex" role="img" focusable="false" viewBox="0 -431 973.6 581" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-3C0"></use></g><g data-mml-node="mn" transform="translate(570, -150) scale(0.707)"><use xlink:href="#MJX-TEX-N-31"></use></g></g></g></g></svg></mjx-container>, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex" xmlns="http://www.w3.org/2000/svg" width="2.203ex" height="1.314ex" role="img" focusable="false" viewBox="0 -431 973.6 581" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-3C0"></use></g><g data-mml-node="mn" transform="translate(570, -150) scale(0.707)"><use xlink:href="#MJX-TEX-N-32"></use></g></g></g></g></svg></mjx-container>, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="2.652ex" height="0.271ex" role="img" focusable="false" viewBox="0 -120 1172 120" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><use xlink:href="#MJX-TEX-N-2026"></use></g></g></g></svg></mjx-container>, <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="2.236ex" height="1.332ex" role="img" focusable="false" viewBox="0 -431 988.4 588.8" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-3C0"></use></g><g data-mml-node="mi" transform="translate(570, -150) scale(0.707)"><use xlink:href="#MJX-TEX-I-6B"></use></g></g></g></g></svg></mjx-container>. To draw samples <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.052ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 465 453" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-7A"></use></g></g></g></svg></mjx-container> from the categorical distribution, we use the Gumbel-Max trick:</p><script type="math/tex; mode=display">z=\text { one_hot }\left(\underset{i}{\arg \max }\left[g_{i}+\log \pi_{i}\right]\right)</script><p>where <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex" xmlns="http://www.w3.org/2000/svg" width="7.424ex" height="1.464ex" role="img" focusable="false" viewBox="0 -442 3281.3 647" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-67"></use></g><g data-mml-node="mn" transform="translate(477, -150) scale(0.707)"><use xlink:href="#MJX-TEX-N-31"></use></g></g><g data-mml-node="mo" transform="translate(1047.2, 0)"><use xlink:href="#MJX-TEX-N-2026"></use></g><g data-mml-node="msub" transform="translate(2385.9, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-67"></use></g><g data-mml-node="mi" transform="translate(477, -150) scale(0.707)"><use xlink:href="#MJX-TEX-I-6B"></use></g></g></g></g></svg></mjx-container> are i.i.d samples drawn from <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="12.787ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 5651.7 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-47"></use></g><g data-mml-node="mi" transform="translate(786, 0)"><use xlink:href="#MJX-TEX-I-75"></use></g><g data-mml-node="mi" transform="translate(1358, 0)"><use xlink:href="#MJX-TEX-I-6D"></use></g><g data-mml-node="mi" transform="translate(2236, 0)"><use xlink:href="#MJX-TEX-I-62"></use></g><g data-mml-node="mi" transform="translate(2665, 0)"><use xlink:href="#MJX-TEX-I-65"></use></g><g data-mml-node="mi" transform="translate(3131, 0)"><use xlink:href="#MJX-TEX-I-6C"></use></g><g data-mml-node="mo" transform="translate(3429, 0)"><use xlink:href="#MJX-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(3818, 0)"><use xlink:href="#MJX-TEX-N-30"></use></g><g data-mml-node="mo" transform="translate(4318, 0)"><use xlink:href="#MJX-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(4762.7, 0)"><use xlink:href="#MJX-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(5262.7, 0)"><use xlink:href="#MJX-TEX-N-29"></use></g></g></g></svg></mjx-container> distribution. <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.466ex" xmlns="http://www.w3.org/2000/svg" width="7.737ex" height="1.491ex" role="img" focusable="false" viewBox="0 -453 3419.7 659" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-N-61"></use><use xlink:href="#MJX-TEX-N-72" transform="translate(500, 0)"></use><use xlink:href="#MJX-TEX-N-67" transform="translate(892, 0)"></use></g><g data-mml-node="mo" transform="translate(1392, 0)"><use xlink:href="#MJX-TEX-N-2061"></use></g><g data-mml-node="mo" transform="translate(1558.7, 0)"><use xlink:href="#MJX-TEX-N-6D"></use><use xlink:href="#MJX-TEX-N-61" transform="translate(833, 0)"></use><use xlink:href="#MJX-TEX-N-78" transform="translate(1333, 0)"></use></g></g></g></svg></mjx-container> is defined as:  </p><script type="math/tex; mode=display">\underset{x}{\arg \max } f(x)=\{x \mid f(x)=M\}=: f^{-1}(M)</script><p>as <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="2.378ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 1051 683" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-4D"></use></g></g></g></svg></mjx-container> is the maximum of <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="4.299ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1900 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-66"></use></g><g data-mml-node="mo" transform="translate(550, 0)"><use xlink:href="#MJX-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(939, 0)"><use xlink:href="#MJX-TEX-I-78"></use></g><g data-mml-node="mo" transform="translate(1511, 0)"><use xlink:href="#MJX-TEX-N-29"></use></g></g></g></svg></mjx-container>. The problem here is <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.466ex" xmlns="http://www.w3.org/2000/svg" width="7.737ex" height="1.491ex" role="img" focusable="false" viewBox="0 -453 3419.7 659" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-N-61"></use><use xlink:href="#MJX-TEX-N-72" transform="translate(500, 0)"></use><use xlink:href="#MJX-TEX-N-67" transform="translate(892, 0)"></use></g><g data-mml-node="mo" transform="translate(1392, 0)"><use xlink:href="#MJX-TEX-N-2061"></use></g><g data-mml-node="mo" transform="translate(1558.7, 0)"><use xlink:href="#MJX-TEX-N-6D"></use><use xlink:href="#MJX-TEX-N-61" transform="translate(833, 0)"></use><use xlink:href="#MJX-TEX-N-78" transform="translate(1333, 0)"></use></g></g></g></svg></mjx-container> is no way differentiable, so we use softmax instead, which is a <em>softened</em> argmax function, making it differentiable. So we get a softened <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.052ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 465 453" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-7A"></use></g></g></g></svg></mjx-container>, denoted as <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex" xmlns="http://www.w3.org/2000/svg" width="1.109ex" height="1.464ex" role="img" focusable="false" viewBox="0 -442 490 647" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-79"></use></g></g></g></svg></mjx-container>, the elements of <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex" xmlns="http://www.w3.org/2000/svg" width="1.109ex" height="1.464ex" role="img" focusable="false" viewBox="0 -442 490 647" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-79"></use></g></g></g></svg></mjx-container> are:</p><script type="math/tex; mode=display">y_{i}=\frac{\exp \left(\left(\log \left(\pi_{i}\right)+g_{i}\right) / \tau\right)}{\sum_{j=1}^{k} \exp \left(\left(\log \left(\pi_{j}\right)+g_{j}\right) / \tau\right)} \quad \text { for } i=1, \ldots, k</script><p>This is the Gumbel-Softmax trick.</p><p>Here we added a temperature variable <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.029ex" xmlns="http://www.w3.org/2000/svg" width="1.17ex" height="1.005ex" role="img" focusable="false" viewBox="0 -431 517 444" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-3C4"></use></g></g></g></svg></mjx-container> to control the <em>softness</em> of softmax. </p><div class="figure center" style="width:100%;"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1610815031/posts/Screen_Shot_2021-01-17_at_00.36.43_bwpf0r.png" style="width:100%;" alt="" div=""><p>Let me explain. Softmax is just a normalized exponential function. At high temperature, every element <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="12.836ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 5673.3 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><use xlink:href="#MJX-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(389, 0)"><use xlink:href="#MJX-TEX-I-6C"></use></g><g data-mml-node="mi" transform="translate(687, 0)"><use xlink:href="#MJX-TEX-I-6F"></use></g><g data-mml-node="mi" transform="translate(1172, 0)"><use xlink:href="#MJX-TEX-I-67"></use></g><g data-mml-node="mo" transform="translate(1649, 0)"><use xlink:href="#MJX-TEX-N-28"></use></g><g data-mml-node="msub" transform="translate(2038, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-3C0"></use></g><g data-mml-node="mi" transform="translate(570, -150) scale(0.707)"><use xlink:href="#MJX-TEX-I-69"></use></g></g><g data-mml-node="mo" transform="translate(2902, 0)"><use xlink:href="#MJX-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(3513.2, 0)"><use xlink:href="#MJX-TEX-N-2B"></use></g><g data-mml-node="msub" transform="translate(4513.4, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-67"></use></g><g data-mml-node="mi" transform="translate(477, -150) scale(0.707)"><use xlink:href="#MJX-TEX-I-69"></use></g></g><g data-mml-node="mo" transform="translate(5284.3, 0)"><use xlink:href="#MJX-TEX-N-29"></use></g></g></g></svg></mjx-container> is divided by a big number, making them all much smaller, so the absolute difference between every element is also smaller, so the distribution is closer to uniform. In contast, at low temperature (smaller than 1), dividing <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.029ex" xmlns="http://www.w3.org/2000/svg" width="1.17ex" height="1.005ex" role="img" focusable="false" viewBox="0 -431 517 444" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-3C4"></use></g></g></g></svg></mjx-container> makes the elements bigger, so the difference between elements are also bigger, making the distribtion “sharper”.  </p><p>We can think of as heating a crystal. Higher temperature melts the crystal, it becomes more soft (closer to uniform distribution). When it cools down, it becomes hardened and sharp. </p><p>We often use an annealing schedule with softmax, starting from a high temperature and gradually cooling it down. This is because we want every choice of operator sufficiently trained at early stage, and gradually forms a preference at later stage.   </p><h2 id="The-Gumbel-Softmax-gradient-estimator"><a href="#The-Gumbel-Softmax-gradient-estimator" class="headerlink" title="The Gumbel-Softmax gradient estimator"></a>The Gumbel-Softmax gradient estimator</h2><p>In some literature we may see the jargon “Gumble-Softmax gradient estimator”. It is well explained in the original <a href="https://arxiv.org/pdf/1611.01144.pdf" target="_blank" rel="noopener">paper</a>:</p><p>“The Gumbel-Softmax distribution is smooth for <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.09ex" xmlns="http://www.w3.org/2000/svg" width="5.318ex" height="1.597ex" role="img" focusable="false" viewBox="0 -666 2350.6 706" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-3C4"></use></g><g data-mml-node="mo" transform="translate(794.8, 0)"><use xlink:href="#MJX-TEX-N-3E"></use></g><g data-mml-node="mn" transform="translate(1850.6, 0)"><use xlink:href="#MJX-TEX-N-30"></use></g></g></g></svg></mjx-container>, and therefore has a well-defined gradient <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.817ex" xmlns="http://www.w3.org/2000/svg" width="2.813ex" height="3.058ex" role="img" focusable="false" viewBox="0 -990.5 1243.3 1351.7" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mrow" transform="translate(248.3, 485) scale(0.707)"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-N-2202"></use></g><g data-mml-node="mi" transform="translate(566, 0)"><use xlink:href="#MJX-TEX-I-79"></use></g></g><g data-mml-node="mrow" transform="translate(220, -345.6) scale(0.707)"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-N-2202"></use></g><g data-mml-node="mi" transform="translate(566, 0)"><use xlink:href="#MJX-TEX-I-3C0"></use></g></g><rect width="1003.3" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container> with respect to the parameter <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.29ex" height="1ex" role="img" focusable="false" viewBox="0 -431 570 442" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-TEX-I-3C0"></use></g></g></g></svg></mjx-container>. Thus, by replacing categorical samples with Gumbel-Softmax samples we can use backpropagation to compute gradients. We denote this procedure of replacing non-differentiable categorical smaples with differetiable approximation during training as the Gumbel-Softmax estimator.””</p><h3 id="Straight-through-Gumbel-Softmax-gradient-estimator"><a href="#Straight-through-Gumbel-Softmax-gradient-estimator" class="headerlink" title="Straight-through Gumbel-Softmax gradient estimator"></a>Straight-through Gumbel-Softmax gradient estimator</h3><p>“Straight-through” means that only backward gradient propagation uses the differentiable variable, the forward pass still uses categorical variable.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><a href="https://arxiv.org/pdf/1611.01144.pdf" target="_blank" rel="noopener">Categorical Reparameterization With Gumbel-Softmax</a></li><li><a href="https://towardsdatascience.com/what-is-gumbel-softmax-7f6d9cdcb90e" target="_blank" rel="noopener">What is Gumbel-Softmax?</a></li><li><a href="https://en.wikipedia.org/wiki/Gumbel_distribution" target="_blank" rel="noopener">Gumbel distribution</a></li></ol></div></div></p><svg style="display: none" id="MJX-SVG-global-cache"><defs><path id="MJX-TEX-I-3B1" d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z"></path><path id="MJX-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-TEX-I-6E" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-TEX-I-61" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path><path id="MJX-TEX-I-72" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-TEX-I-67" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path><path id="MJX-TEX-I-6D" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-TEX-I-78" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-TEX-I-3BC" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path><path id="MJX-TEX-I-3B2" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path><path id="MJX-TEX-I-7A" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path><path id="MJX-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-TEX-I-3C0" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path><path id="MJX-TEX-N-2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path><path id="MJX-TEX-I-6B" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path><path id="MJX-TEX-I-47" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path><path id="MJX-TEX-I-75" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-TEX-I-62" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path><path id="MJX-TEX-I-65" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path><path id="MJX-TEX-I-6C" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path><path id="MJX-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path id="MJX-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-TEX-N-61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z"></path><path id="MJX-TEX-N-72" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z"></path><path id="MJX-TEX-N-67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z"></path><path id="MJX-TEX-N-2061" d=""></path><path id="MJX-TEX-N-6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z"></path><path id="MJX-TEX-N-78" d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z"></path><path id="MJX-TEX-I-4D" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path><path id="MJX-TEX-I-66" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path><path id="MJX-TEX-I-79" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-TEX-I-3C4" d="M39 284Q18 284 18 294Q18 301 45 338T99 398Q134 425 164 429Q170 431 332 431Q492 431 497 429Q517 424 517 402Q517 388 508 376T485 360Q479 358 389 358T299 356Q298 355 283 274T251 109T233 20Q228 5 215 -4T186 -13Q153 -13 153 20V30L203 192Q214 228 227 272T248 336L254 357Q254 358 208 358Q206 358 197 358T183 359Q105 359 61 295Q56 287 53 286T39 284Z"></path><path id="MJX-TEX-I-6F" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path><path id="MJX-TEX-I-69" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-TEX-N-3E" d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z"></path><path id="MJX-TEX-N-2202" d="M202 508Q179 508 169 520T158 547Q158 557 164 577T185 624T230 675T301 710L333 715H345Q378 715 384 714Q447 703 489 661T549 568T566 457Q566 362 519 240T402 53Q321 -22 223 -22Q123 -22 73 56Q42 102 42 148V159Q42 276 129 370T322 465Q383 465 414 434T455 367L458 378Q478 461 478 515Q478 603 437 639T344 676Q266 676 223 612Q264 606 264 572Q264 547 246 528T202 508ZM430 306Q430 372 401 400T333 428Q270 428 222 382Q197 354 183 323T150 221Q132 149 132 116Q132 21 232 21Q244 21 250 22Q327 35 374 112Q389 137 409 196T430 306Z"></path></defs></svg>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Gumbel-Softmax is a reparameterization trick to make the sampling process from categorical distribution differentiable.&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="https://www.zzzdavid.tech/categories/Research/"/>
    
      <category term="Tutorial" scheme="https://www.zzzdavid.tech/categories/Research/Tutorial/"/>
    
    
  </entry>
  
  <entry>
    <title>Paper Readings</title>
    <link href="https://www.zzzdavid.tech/Readings/"/>
    <id>https://www.zzzdavid.tech/Readings/</id>
    <published>2020-12-06T18:45:23.000Z</published>
    <updated>2022-08-26T19:32:11.147Z</updated>
    
    <content type="html"><![CDATA[<!-- excerpt --><!-- This is a blog post to remind myself of the papers that I read. I intend to keep the explanation short, simple, and straightforward. Just like explaining what it is to my 5-year-old self. --><h2 id="Virtualizing-FPGAs-in-the-Cloud"><a href="#Virtualizing-FPGAs-in-the-Cloud" class="headerlink" title="Virtualizing FPGAs in the Cloud"></a>Virtualizing FPGAs in the Cloud</h2><p>Authors: Yue Zha, Jing Li<br>Venue: ASPLOS 20<br>Institution: University of Pennsylvania</p><h2 id="Point-Voxel-CNN-for-Efficient-3D-Deep-Learning"><a href="#Point-Voxel-CNN-for-Efficient-3D-Deep-Learning" class="headerlink" title="Point-Voxel CNN for Efficient 3D Deep Learning"></a>Point-Voxel CNN for Efficient 3D Deep Learning</h2><p>Authors: Zhijian Liu*, Haotian Tang*, Yujun Lin, Song Han<br>Venue: NIPS 2019<br>Institution: MIT &amp; SJTU</p><h4 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h4><ol><li>Voxel-based methods cannot scale to high resolution. Point-based methods have poor data locality (sparse data access).</li></ol><h4 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h4><h5 id="Point-Voxel-CNN-combines-the-best-from-both-worlds"><a href="#Point-Voxel-CNN-combines-the-best-from-both-worlds" class="headerlink" title="Point-Voxel CNN combines the best from both worlds"></a>Point-Voxel CNN combines the best from both worlds</h5><p>Point-Voxel Convolution: coarse voxelization<br><img src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1603798938/posts/PVConv_mlop52.png" /></p><ol><li><em>Normalize</em>: spatial locations (x,y,z) are normalized to [0,1] first.</li><li><em>Voxelization</em>: points that fall into the same voxel grid are averaged.</li><li><em>Convolve</em>: 3D convolution.</li><li><em>Devoxelize</em>: trilinear interpolation.</li><li><em>Fuse</em>: add interpolated point to MLP output points.</li></ol><ul><li>PVCNN represents the input data as point cloud to reduce memory consumption. Voxel branch can be coarse as detail information is preserved with point branch.</li></ul><ul><li>PVCNN leverages voxel-based convolution to obtain contiguous memory access pattern. No convolution with point-cloud, no dynamic-kernel and KNN computation, so random memory access is avoided.</li></ul><h4 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h4><h5 id="1-First-of-all-what-is-Voxel"><a href="#1-First-of-all-what-is-Voxel" class="headerlink" title="1. First of all, what is Voxel?"></a>1. First of all, what is Voxel?</h5><p>Voxel is like the 3D equivalence of pixel. A voxel represents a single sample, or data point, on a regularly spaced, three-dimensional grid.   </p><h5 id="2-What-are-voxelization-and-devoxelization"><a href="#2-What-are-voxelization-and-devoxelization" class="headerlink" title="2. What are voxelization and devoxelization?"></a>2. What are voxelization and devoxelization?</h5><p><img src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1603799433/posts/voxelization_pt0a7d.png" /></p><h5 id="3-What-is-Volumetric-Convolution"><a href="#3-What-is-Volumetric-Convolution" class="headerlink" title="3. What is Volumetric Convolution?"></a>3. What is Volumetric Convolution?</h5><p>[27] <a href="http://www.ri.cmu.edu/pub_files/2015/9/voxnet_maturana_scherer_iros15.pdf" target="_blank" rel="noopener">Daniel Maturana and Sebastian Scherer. VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition. in IROS, 2015</a></p><p>Section 3 gives a formal introduction of the volumetric convolution. We see that volumetric convolution is actually 3D convolution:</p><p align="center"><img src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1603890830/posts/jriyCTU_mpc5ey.png"/></p><h5 id="4-Why-point-based-methods-have-sparse-data-access"><a href="#4-Why-point-based-methods-have-sparse-data-access" class="headerlink" title="4. Why point-based methods have sparse data access?"></a>4. Why point-based methods have sparse data access?</h5><ol><li>Neighbor points are not stored contiguously in the point representation so indexing them requrest a nearest neighbor search.</li><li>Becasue relative positions of neighbors are not fixed, these point-based models have to generate the convolution kernels dynamically based on different offsets.<br>(Dynamic kernel is a special kind of method)</li></ol><h5 id="5-Why-random-memory-access-causes-bank-conflict"><a href="#5-Why-random-memory-access-causes-bank-conflict" class="headerlink" title="5. Why random memory access causes bank conflict?"></a>5. Why random memory access causes bank conflict?</h5><p>[28] <a href="https://course.ece.cmu.edu/~ece740/f11/lib/exe/fetch.php?media=wiki:lectures:onur-740-fall11-lecture25-mainmemory.pdf" target="_blank" rel="noopener">Onur Mutlu. DDR Access Illustration.</a></p><h5 id="6-Why-voxel-based-method-consumes-more-memory-than-point-cloud-based-method-Doesn’t-it-have-less-information-because-some-of-the-points-are-merged-into-one-voxel"><a href="#6-Why-voxel-based-method-consumes-more-memory-than-point-cloud-based-method-Doesn’t-it-have-less-information-because-some-of-the-points-are-merged-into-one-voxel" class="headerlink" title="6. Why voxel-based method consumes more memory than point-cloud based method? Doesn’t it have less information because some of the points are merged into one voxel?"></a>6. Why voxel-based method consumes more memory than point-cloud based method? Doesn’t it have less information because some of the points are merged into one voxel?</h5><p>I’m not exactly sure about the answer right now. One guess is that during voxelization, the space where it doesn’t have any point is also voxelized. Maybe that’s why we could use sparse voxel.</p><h4 id="Interesting-Facts"><a href="#Interesting-Facts" class="headerlink" title="Interesting Facts"></a>Interesting Facts</h4><ol><li>The computation cost and memory footprint of voxel-based models grow <strong>cubically</strong> with input resolution. </li></ol><hr><h2 id="Searching-Efficient-3D-Architectures-with-Sparse-Point-Voxel-Convolution"><a href="#Searching-Efficient-3D-Architectures-with-Sparse-Point-Voxel-Convolution" class="headerlink" title="Searching Efficient 3D Architectures with Sparse Point-Voxel Convolution"></a>Searching Efficient 3D Architectures with Sparse Point-Voxel Convolution</h2><p><a href="https://hanlab.mit.edu/projects/spvnas/" target="_blank" rel="noopener">project page</a> | <a href="https://arxiv.org/abs/2007.16100" target="_blank" rel="noopener">arxiv</a></p><p>Authors: Haotian Tang*, Zhijian Liu*, Shengyu Zhao, Yujun Lin, Ji Lin, Hanrui Wang, Song Han<br>Venue: ECCV 2020<br>Institution: MIT &amp; Tsinghua University</p><h4 id="Background-1"><a href="#Background-1" class="headerlink" title="Background"></a>Background</h4><ol><li><p>Previous models for 3D segmentation is still memory-intensive, so they can’t affort high resolution.</p></li><li><p>Small objects often become indistinguishable because of aggressive downsampling or coarse voxelization.</p></li></ol><h4 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h4><h5 id="A-new-3D-module-SPConv-Sparse-Point-Voxel-Convolution"><a href="#A-new-3D-module-SPConv-Sparse-Point-Voxel-Convolution" class="headerlink" title="A new 3D module: SPConv (Sparse Point-Voxel Convolution)"></a>A new 3D module: SPConv (Sparse Point-Voxel Convolution)</h5><p><img src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1603916779/posts/SPVNAS_wouksz.png" /></p><ol><li><p>Input point-cloud is the same as before, but voxelization is sparse, i.e., it produces sparse voxel grid.<br>p.s. The author also talked about implementing sparse voxelization with GPU Hash table.</p></li><li><p>Sparse convolution<br>From: <a href="https://arxiv.org/pdf/1904.08755.pdf" target="_blank" rel="noopener">Minkowski CNN: 4D Spatio-Temporal ConvNets</a>.<br>“Sparse convolution” is actually a stack of sparse residual blocks.</p></li><li><p>Devoxelization is the same: trilinear interpolation.</p></li></ol><hr><h5 id="3D-NAS-with-SPConv-SPVNAS"><a href="#3D-NAS-with-SPConv-SPVNAS" class="headerlink" title="3D NAS with SPConv - SPVNAS"></a>3D NAS with SPConv - SPVNAS</h5><ol><li><p>Design Space: <strong>fine-grained channel numbers</strong>, <strong>fixed kernel size</strong>.<br>The design space is very much like OFA - elastic channel numbers, elastic depth. It also uses progressive shrinking (depth) to train supernet. Differences are:</p><ul><li>OFA has fixed input/output channel number for blocks, only the middle layer’s channel can change (by a expansion ratio). <strong>SPVNAS allows all channels to change</strong>.</li><li><strong>SPVNAS has a fixed 3x3x3 kernel size</strong>. This is because larger kernels are computationally expensive (cubic). Also sparse convolution brings significant overhead to build larger kernels.</li></ul></li><li><p>Search method: single-objective Genetic Algorithm with MAC constraint. Candidates that don’t meet the constraint are discarded.</p></li></ol><h4 id="Q-amp-A-1"><a href="#Q-amp-A-1" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h4><h5 id="1-Won’t-sparse-PVConv-cause-random-memory-access"><a href="#1-Won’t-sparse-PVConv-cause-random-memory-access" class="headerlink" title="1. Won’t sparse PVConv cause random memory access?"></a>1. Won’t sparse PVConv cause random memory access?</h5><p>I suppose so. There must be random memory access while building the sparse kernel.</p><h5 id="2-Elaborate-on-sparse-conv"><a href="#2-Elaborate-on-sparse-conv" class="headerlink" title="2. Elaborate on sparse conv?"></a>2. Elaborate on sparse conv?</h5><p>From: <a href="https://arxiv.org/pdf/1904.08755.pdf" target="_blank" rel="noopener">Minkowski CNN: 4D Spatio-Temporal ConvNets</a>.<br>Also: <a href="https://arxiv.org/abs/1505.02890" target="_blank" rel="noopener">Sparse convolutional neural networks</a></p><h4 id="Interesting-Facts-1"><a href="#Interesting-Facts-1" class="headerlink" title="Interesting Facts"></a>Interesting Facts</h4><ol><li>Point-based methods (point cloud and rasterized voxel grids) waste up to 90% of their time on structuring the irregular data.</li></ol><hr><h2 id="Overwrite-Quantization-Opportunistic-Outlier-Handling-for-Neural-Network-Accelerators"><a href="#Overwrite-Quantization-Opportunistic-Outlier-Handling-for-Neural-Network-Accelerators" class="headerlink" title="Overwrite Quantization: Opportunistic Outlier Handling for Neural Network Accelerators"></a>Overwrite Quantization: Opportunistic Outlier Handling for Neural Network Accelerators</h2><ul><li>Unpublished preprint</li><li>Authors: Ritchie Zhao, Christopher De Sa, Zhiru Zhang</li><li><a href="https://arxiv.org/abs/1910.06909" target="_blank" rel="noopener">ArXiv</a></li></ul><p><strong>TL;DR</strong>: A quantization scheme where the outlier could overwrite its neighbor value, and a low-overhead hardware implementation of the idea.</p><h4 id="Basic-Idea"><a href="#Basic-Idea" class="headerlink" title="Basic Idea"></a>Basic Idea</h4><div class="figure fig-50" style="width:;"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1604648821/posts/Screen_Shot_2020-11-06_at_15.46.53_rzp1ye.png" alt=""></div><p>The basic idea of OverQ is to let outlier number take the bits of its neighbor to store its value. </p><p>OverQ is an <em>opportunistic</em> quantization scheme, meaning that $x_2$ will only overwrite $x_3$ only if $x_3$ is small. If $x_3$ is overwritten, its value is treated as zero. There are two types of OverQ: OverQ-Split and OverQ-Shift.</p><h4 id="Two-types-of-OverQ"><a href="#Two-types-of-OverQ" class="headerlink" title="Two types of OverQ"></a>Two types of OverQ</h4><div class="figure " style="width:100%;"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1604648835/posts/Screen_Shot_2020-11-06_at_15.47.10_alhk0m.png" style="width:100%;"alt=""></div><p>We take dot product of two vectors as an example. Both methods use a flag bit to indicated whether it is overwritten. </p><h5 id="OverQ-Split"><a href="#OverQ-Split" class="headerlink" title="OverQ-Split"></a>OverQ-Split</h5><p>The outlier is divided by two to save in both positions. Then the weight is copied. The dynamic range is extended from $[0,2^n-1]$ to $[0, 2^{n+1}-1]$ for UInt.</p><h5 id="OverQ-Shift"><a href="#OverQ-Shift" class="headerlink" title="OverQ-Shift"></a>OverQ-Shift</h5><p>OverQ-Shift reserves one bit to indicate shift direction. Then the other bits can be used to store the extra LSB or MSB. This way it has $2 \times n - 1$ bits. </p><p>There is an extra mode called zero-reuse: when the neighbor is zero, it uses the extra bits to store LSB for higher precision. For example:</p><div class="figure " style="width:100%;"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1604648856/posts/Screen_Shot_2020-11-06_at_15.47.33_qgg5s1.png" style="width:100%;"alt=""></div><h4 id="Q-amp-A-2"><a href="#Q-amp-A-2" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h4><h5 id="1-Once-you-re-arrange-the-weight-kernels-the-output-channels’-order-is-also-going-to-change-Do-we-change-them-back-Or-doesn’t-it-affect-the-convolution-result-in-the-downstream-layer"><a href="#1-Once-you-re-arrange-the-weight-kernels-the-output-channels’-order-is-also-going-to-change-Do-we-change-them-back-Or-doesn’t-it-affect-the-convolution-result-in-the-downstream-layer" class="headerlink" title="1. Once you re-arrange the weight kernels, the output channels’ order is also going to change. Do we change them back? Or doesn’t it affect the convolution result in the downstream layer?"></a>1. Once you re-arrange the weight kernels, the output channels’ order is also going to change. Do we change them back? Or doesn’t it affect the convolution result in the downstream layer?</h5><p>I was wondering if channel reordering would change the hardware design, and now I suppose it wouldn’t. My idea of implementing this: suppose we have two conv layers, first we permute the conv kernels in the first layer to get desired output channel order. Then we must permute the channel order of the second conv layer’s kernels accordingly.  Then the hardware just runs the neural network as if it was never channel-reordered.</p><hr><h2 id="An-overview-of-proxy-label-approches-for-semi-supervised-learning"><a href="#An-overview-of-proxy-label-approches-for-semi-supervised-learning" class="headerlink" title="An overview of proxy-label approches for semi-supervised learning"></a>An overview of proxy-label approches for semi-supervised learning</h2><ul><li>A blog post by Sebastian Ruder. <a href="https://ruder.io/semi-supervised/" target="_blank" rel="noopener">link</a></li></ul><blockquote><p>While unsupervised learning is still elusive, researchers have made a lot of progress in semi-supervised learning. This post focuses on a particular promising category of semi-supervised learning methods that assign proxy labels to unlabelled data, which are used as targets for learning.</p></blockquote><p>Proxy labels are generated by the model itself or variants of it without any additional supervision. These labels can be considered as <em>noisy</em> or <em>weak</em>.</p><p>There are three categories of proxy-label learning:</p><ul><li><strong>Self-training</strong>: using a model’s own predictions as proxy labels.</li><li><strong>Multi-view learning</strong>: train models with different <em>views</em> of the data, then use its predictions as proxy label.</li><li><strong>Self-ensembling</strong>: ensembles variations of a model’s own predictions and uses these as feedback for learning.</li></ul><h3 id="Self-training"><a href="#Self-training" class="headerlink" title="Self-training"></a>Self-training</h3><p>Self-training uses the confident predictions as labels. There’s a pre-set confidence threshold. In every iteration, the model is trained on the labelled dataset, then make predictions on unlabelled data. Those predictions with confidence score over the threshold will be added to the labelled dataset.</p><p>The downside of self-training is that the model cannot correct its own mistakes. This effect is exacerbated if the domain of the unlabelled data is different from the labelled data.</p><h3 id="Multi-view-training"><a href="#Multi-view-training" class="headerlink" title="Multi-view training"></a>Multi-view training</h3><p>Multi-view training train multiple models with different <em>views</em> of the data. These <em>views</em> differs in many ways, such as the feature they use, the model architectures, or different parts of the dataset.</p><ul><li><strong>Co-training</strong>: trian two identical models on two parts of the dataset, confident predictions of one model is added to the other’s training set.</li><li><strong>Democratic Co-learning</strong>: train many different models on complete dataset, then use them to predict unlabelled data. If many models all make confident predictions on some data, it will be added to the training set.  <ul><li><strong>Tri-training with disagreement</strong>: three models selected with diversity are trained first in each iteration, and the unlabelled data which two models agree but one model disagrees is added to the training set.</li><li><strong>Asymmetric tri-training</strong>: test and unlabelled data is from different domain than labelled training data. To accommodate this change, one of the models is trained only on proxy labels and not on labelled data, and uses only this model to classify target domain examples at test time. Also three models share the same feature extractor.</li><li><strong>Multi-task tri-training</strong>: aims to relieve the training expense of tri-training. All models share parameters and trained with multi-task learning. To prevent mult-task tri-training reduced to self-training, we add an orthogonality constraint to the loss term to ensure the diversity of softmax layer’s input.</li></ul></li></ul><h3 id="Self-ensembling"><a href="#Self-ensembling" class="headerlink" title="Self-ensembling"></a>Self-ensembling</h3><p>Like tri-training, self-ensembling methods also uses many variants of the model. But it does not require the diversity of the variants. Self-ensembling approaches mostly use a single model under different configurations. </p><ul><li><strong>Ladder networks</strong>: this method aims to make the model robust to noise. For unlabelled data, it first makes a prediction as the label, then add noise to the same data, and train the model with the prediction on clean data.</li><li><strong>Virtual Adversarial Training</strong>: If perturbing the original sample is not possible or desired, we can instead add the <em>worst possible</em> perturbation to the example in the feature space. This make the input an adversarial example. But this method does not require label, unlike adversarial training. To do so, first, an image is taken and an adversarial variant of it is created such that the KL-divergence between the model output for the original image and the adversarial image is maximized.</li><li><strong>Pi model</strong>: ensembles the predicitons of one model under two different perturbation of the input data and two different dropout conditions. </li><li><strong>Temporal Ensembling</strong>: still one model architecture, but the model ensembles its prediction that is accumulated over timesteps (past predictions).</li><li><strong>Mean teacher</strong>: still one model, the alternative edition has averaged-over-training parameters.  </li></ul><h2 id="Semi-supervised-Learning-in-Computer-Vision"><a href="#Semi-supervised-Learning-in-Computer-Vision" class="headerlink" title="Semi-supervised Learning in Computer Vision"></a>Semi-supervised Learning in Computer Vision</h2><ul><li>blog post by Amit Chaudhary: <a href="https://amitness.com/2020/07/semi-supervised-learning/" target="_blank" rel="noopener">link</a></li></ul><p>This post is a nice visualization of the previous post. </p><hr><h2 id="4D-Spatio-Temporal-ConvNets-Minkowski-Convolutional-Neural-Network"><a href="#4D-Spatio-Temporal-ConvNets-Minkowski-Convolutional-Neural-Network" class="headerlink" title="4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Network"></a>4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Network</h2><ul><li>Venue: CVPR 2019 </li><li>Institution: Stanford University</li><li>Authors: Christopher Choy, JunYoung Gwak, Silvio Savarese</li></ul><h3 id="COO-format-Sparse-Tensor"><a href="#COO-format-Sparse-Tensor" class="headerlink" title="COO-format Sparse Tensor"></a>COO-format Sparse Tensor</h3><p>COO = Coordinate Format, also known as ‘ijv’ or ‘triplet’ format. Basically it uses three lists to store the sparse matrix: row, col, data. So <code>data[i]</code> is value at <code>(row[i], col[i])</code>.</p><p>Minkowski engine extends the coordinates with two entries: time step $t_i$, and batch index $b_i$. So the sparse tensor is stored like this:</p><script type="math/tex; mode=display">C=\left[\begin{array}{ccccc}x_{1} & y_{1} & z_{1} & t_{1} & b_{1} \\& & \vdots & & \\x_{N} & y_{N} & z_{N} & t_{N} & b_{N}\end{array}\right], F=\left[\begin{array}{c}\mathbf{f}_{1}^{T} \\\vdots \\\mathbf{f}_{N}^{T}\end{array}\right]</script><p>$\mathbf{f_i}$ is the feature vector at the associated coordinate. </p><h3 id="Generalized-Sparse-Convolution"><a href="#Generalized-Sparse-Convolution" class="headerlink" title="Generalized Sparse Convolution"></a>Generalized Sparse Convolution</h3><p>For 3D space point-cloud data, the coordinates are no longer integers, and for 3D convolution, the kernel tensor’s shape may not be cubic. So we need a sparse convolution algorithm that is generalized for any kernel shape, any coordinates:</p><script type="math/tex; mode=display">x_u^{out}  = \sum_{i \in \mathcal{N}^D(u, \mathcal{C}^{in})}  W_i x_{u+i}^{in} ~ for ~ u \in \mathcal{C}^{out}</script><ul><li>$u$: D-dimensional coordinate</li><li>$\mathcal{N}^D$ is a set of offsets that define the shape of a kernel</li><li>$\mathcal{N}^D (u, \mathcal{C}^{in})$ is the set of offsets from the center $u$, that exists in $\mathcal{C}^{in}$</li><li>$\mathcal{C}^{in}$ (input coordinate) is not necessarily identical to $\mathcal{C}^{out}$ (output coordinate).</li></ul><hr><h2 id="AXI-HyperConnect-A-Predictable-Hypervisor-levelk-Interconnect-for-Hardware-Accelerators-in-FPGA-SoC"><a href="#AXI-HyperConnect-A-Predictable-Hypervisor-levelk-Interconnect-for-Hardware-Accelerators-in-FPGA-SoC" class="headerlink" title="AXI HyperConnect: A Predictable, Hypervisor-levelk Interconnect for Hardware Accelerators in FPGA SoC"></a>AXI HyperConnect: A Predictable, Hypervisor-levelk Interconnect for Hardware Accelerators in FPGA SoC</h2><ul><li>Venue: DAC 2020</li><li>Institution: Department of Excellence in Robotics &amp; AI, Acuola Superiore Sant’Anna, Pisa, Italy</li><li>Authors: Francesco Restuccia, <em>et al.</em><br><a href="https://github.com/ellerbrock/open-source-badges/" target="_blank" rel="noopener"><img src="https://badges.frapsoft.com/os/v3/open-source.svg?v=103" alt="Open Source Love"></a></li></ul><p>In this work, a new hypervisor-level hardware component named AXI HyperConnect is proposed. It allows interconnecting hardware accelerators to the same bus while ensuring <em>isolation</em> and <em>predictability</em>. </p><h3 id="Background-2"><a href="#Background-2" class="headerlink" title="Background"></a>Background</h3><div class="figure center" style="width:90%;"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1608259979/posts/Screen_Shot_2020-12-18_at_10.52.52_nd4ooc.png" style="width:90%;"alt=""></div><ul><li>PS (Processing System) includes one or more processors.</li><li>PS and FPGA access a DRAM contoller to reach a shared DRAM memory.</li><li>The AXI standard is simultaneous, bidirectional data exchange standard.</li></ul><h3 id="The-integration-phase-of-Hyperconnect"><a href="#The-integration-phase-of-Hyperconnect" class="headerlink" title="The integration phase of Hyperconnect"></a>The integration phase of Hyperconnect</h3><p>We assume that the IP description is provided in an XML format such as the popular IP-XACT. Each hardware accelerator should implement an interface composed of an AXI control slave interface and an AXI master interface.</p><ol><li>Each AXI master port of hardware accelerator is connected to an input slave port of an AXI HyperConnect. </li><li>The master port of the AXI HyperConnect is connected to the FPGA-PS interface, while the hardware accelerator’s AXI slave ports are connected to the PS-FPGA interface.</li><li>Once all accelerators have been connected, the system integrator uses a synthesis tool (Vivado, Quartus) to synthesize the overall design.</li><li>Finally the bitstream is generated. </li></ol><h3 id="AXI-HyperConnect-Details"><a href="#AXI-HyperConnect-Details" class="headerlink" title="AXI HyperConnect Details"></a>AXI HyperConnect Details</h3><div class="figure center" style="width:90%;"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1608261344/posts/Screen_Shot_2020-12-18_at_11.15.37_tnkcde.png" style="width:90%;"alt=""></div><h4 id="eFIFO-efficient-FIFO"><a href="#eFIFO-efficient-FIFO" class="headerlink" title="eFIFO (efficient FIFO)"></a>eFIFO (efficient FIFO)</h4><p>The eFIFO module is a buffered AXI interface. Each eFIFO module defines five independent FIFO queues, one for each AXI channel. Each of the queues is implemented as a <strong>proactive circular buffer</strong> (proactive means always ready to receive).</p><p><strong>decoupling mechanism</strong>: when a hardware accelerator is decoupled, the AXI handshake signals on all the AXI channels are kept low, not allowing data exchange. </p><h4 id="TS-Transaction-Supervisor"><a href="#TS-Transaction-Supervisor" class="headerlink" title="TS (Transaction Supervisor)"></a>TS (Transaction Supervisor)</h4><p>TS is the core module to control bandwidth and memory access management. TS implementation detials: <a href="https://dl.acm.org/doi/fullHtml/10.1145/3358183" target="_blank" rel="noopener">“Is your bus arbiter really fair?”</a>. TS does the following things:</p><ul><li>splits read/write requests into sub-requests with nominal burst size and merges incoming data/write responses.</li><li>A reservation mechanism: reserve a configurable budget of transaction for each input port that is periodically recharged (Time Division).</li></ul><h4 id="EXBAR-Efficient-Cross-bar"><a href="#EXBAR-Efficient-Cross-bar" class="headerlink" title="EXBAR (Efficient Cross-bar)"></a>EXBAR (Efficient Cross-bar)</h4><p>The EXBAR is a low-latency crossbar in charge of solving the conflicts of read/write requests. It implements round-robin artbitration with a <em>fixed</em> granularity of one trasaction per TS module in each round-cycle (thus the latency is predictable).</p><hr><h2 id="Cross-Modal-Generalization-Learning-in-Low-Resource-Modalities-via-Meta-Alignment"><a href="#Cross-Modal-Generalization-Learning-in-Low-Resource-Modalities-via-Meta-Alignment" class="headerlink" title="Cross-Modal Generalization: Learning in Low Resource Modalities via Meta-Alignment"></a>Cross-Modal Generalization: Learning in Low Resource Modalities via Meta-Alignment</h2><ul><li>Venue: <em>preprint for now</em> </li><li>Institution: CMU, University of Tokyo</li><li>Authors: Paul Pu Liang, Peter Wu, Liu Ziyin, Louis-Philippe Morency, Ruslan Salakhutdinov</li></ul><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><div class="figure center" style="width:80%;"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1608267116/posts/Screen_Shot_2020-12-18_at_12.51.22_lkjifj.png" style="width:80%;"alt=""></div><p><strong>Modality</strong><br>Input spaces, the means that a concept is expressed, or a type of representation. e.g.: visual, acoustic, tactile, ligustic are different modalities.<br><strong>Key Research Question</strong></p><ol><li>How to generalize across modalities when using seperate encoders for source and target modalities?<br>This problem statement differs from conventional meta-learning and domain adaptation in that <em>the source and target modality do not share the same encoder</em>.</li><li>What is the minimal extra supervision required to learn new output concepts expressed in new input modalities?<br><strong>Goal/Motivation</strong><br>To train models in high-resource source modality, and generalize models in low-resource (few labeled samples) target modality.</li></ol><h3 id="Key-Concepts"><a href="#Key-Concepts" class="headerlink" title="Key Concepts"></a>Key Concepts</h3><h4 id="Cross-modal-generalization"><a href="#Cross-modal-generalization" class="headerlink" title="Cross-modal generalization"></a>Cross-modal generalization</h4><p>A learning paradim to train a model that can<br>(1) quickly perform new tasks in target modality<br>(2) doing so while being trained on a different source modality</p><script type="math/tex; mode=display">\underset{w}{\arg \max } \mathcal{L}\left[f_{w}\right]:=\underset{w}{\arg \max } \underset{m, n \sim p(m, n) \atop x, y \sim p_{m, n}(x, y)}{\mathbb{E}} \log \left[\frac{f_{w}(x, y, m, n)}{p(x, y \mid m, n)}\right]</script><hr><h2 id="DADA-Differentiable-Automatic-Data-Augmentation"><a href="#DADA-Differentiable-Automatic-Data-Augmentation" class="headerlink" title="DADA: Differentiable Automatic Data Augmentation"></a><a href="https://arxiv.org/pdf/2003.03780.pdf" target="_blank" rel="noopener">DADA: Differentiable Automatic Data Augmentation</a></h2><ul><li>Venue: ECCV 2020</li><li>Intitution: Peking University, Anyvision, Queesn University of Belfast, The University of Edinburgh</li><li>Authors: Yonggang Li, Guosheng Hu, Yongtao Wang, Timothy Hospedales, Neil M. Robertson, Yongxin Yang</li></ul><h3 id="Related-works"><a href="#Related-works" class="headerlink" title="Related works"></a>Related works</h3><ol><li><p>AutoAugment<br>AutoAugment is the pioneering work for automatic data argumentation. It uses Reinforcement Learning to solve an optimization problem: to maximze accuracy on validation dataset with selected augmentation scheme. It models the policy search problem as a sequence prediction problem, and uses an RNN controller to predict the policy. RL optimizes the controller parameters. Two parameters are optimized: the type of data augmentation, and the intensity of selected augmentation. AutoAugment is effective but very computationally expensive, searching augmentation scheme for single task on single dataset costs up to 5k GPU hours.</p></li><li><p>Population Based Augmentation (PBA)<br>PBA introduces efficient population based optimization, which was originally used in HPO (Hyperparameter Opimization). Note: probabily evolutionary strategy.</p></li><li><p>Fast AutoAugment (Fast AA)<br>Fast AA and PBA are both proposed to solve the efficiency problem of AutoAugment. Fast AA models the augmentation selection problem as a <em>density matching</em> problem, and solve it through Bayesian Optimization.</p></li></ol><hr>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;A reading diary to keep track of papers that I read.&lt;/p&gt;
    
    </summary>
    
      <category term="Readings" scheme="https://www.zzzdavid.tech/categories/Readings/"/>
    
    
  </entry>
  
  <entry>
    <title>Loop Optimization in HLS</title>
    <link href="https://www.zzzdavid.tech/loop_opt/"/>
    <id>https://www.zzzdavid.tech/loop_opt/</id>
    <published>2020-10-31T04:00:00.000Z</published>
    <updated>2022-08-26T19:32:11.149Z</updated>
    
    <content type="html"><![CDATA[<!-- excerpt --><h2 id="What-is-II"><a href="#What-is-II" class="headerlink" title="What is II?"></a>What is II?</h2><p>II means initiation interval. </p><p>For a function, II is the number of clock cycles before it could accept new inputs and is generally the <em>most critical performance metric in any system.</em> </p><p>For a loop, II is the number of clock cycles before the next iteration of a loop starts to process data.</p><h2 id="Loop-Pipelining"><a href="#Loop-Pipelining" class="headerlink" title="Loop Pipelining"></a>Loop Pipelining</h2><p><p align=center><br><img src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1604459708/posts/X14770-loop-pipelining_v42eza.svg" alt="https://res.cloudinary.com/dxzx2bxch/image/upload/v1604459708/posts/X14770-loop-pipelining_v42eza.svg"></p><p><p/><br>Although we are already familiar with “pipelining”, this figure still gives a great example for “II” and loop latency.</p><p>There are some more details about loop pipelining. One thing worth noticing is that when a loop is called multiple times, there could be a thing called io “bubble” happening:</p><p><img src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1604467903/posts/Screen_Shot_2020-11-04_at_13.30.51_pvobq4.png" alt="https://res.cloudinary.com/dxzx2bxch/image/upload/v1604467903/posts/Screen_Shot_2020-11-04_at_13.30.51_pvobq4.png"></p><p>This means that when the loop finishes executing, the next read and write won’t start until the loop is called again, even though II=1. So, if there are loops that are called multiple times, we can identify them and allow each call to overlap to remove the bubble. This is called <strong>rewind</strong>.</p><p><img src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1604467905/posts/Screen_Shot_2020-11-04_at_13.30.58_reproh.png" alt="https://res.cloudinary.com/dxzx2bxch/image/upload/v1604467905/posts/Screen_Shot_2020-11-04_at_13.30.58_reproh.png"></p><h2 id="Loop-Unrolling"><a href="#Loop-Unrolling" class="headerlink" title="Loop Unrolling"></a>Loop Unrolling</h2><p>Loop unrolling is to trade performance (latency, throughput) with resources. </p><p>By default, loops are kept rolled in Vivado HLS. These rolled loops generate a hardware resource which is used by each iteration of the loop.</p><p><img src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1604459839/posts/loop_unroll_icpavw.png" alt="https://res.cloudinary.com/dxzx2bxch/image/upload/v1604459839/posts/loop_unroll_icpavw.png"></p><p><strong>Rolled Loop</strong></p><p>When the loop is rolled, each iteration is peformed in seperate clock cycles. This implementation takes four clock cycles, only requires one multiplier and each block RAM can be a single-port block RAM.</p><p><strong>Partially Unrolled Loop</strong></p><p>We can partially unroll a loop by specifying a unroll factor. In this example, we are setting unrolling factor = 2. This implementation requires two multipliers and a dual-port RAMs to support two reads or writes to each RAM in the same clock cycle. This implementation does however only take 2 clock cycles to complete.</p><p><strong>Unrolled Loop</strong></p><p>If the loop is fully unrolled, it uses 4 multipliers and requires the ability to perform 4 reads and 4 writes at the same time (we can achieve this with array partitioning). This uses more computation and storage resources but only take 1 clock cycle to complete.</p><h2 id="Array-Partitioning"><a href="#Array-Partitioning" class="headerlink" title="Array Partitioning"></a>Array Partitioning</h2><p><p align=center><br><img src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1604461901/posts/X14768-resource-contention_de0gk5.svg" alt="https://res.cloudinary.com/dxzx2bxch/image/upload/v1604461901/posts/X14768-resource-contention_de0gk5.svg"></p><p/><p>A common issue in pipelined loop is memory conflict. Arrays in HLS are implemented as Block RAMs. If there are two accesses to the same array in the loop body, it will need two read operation through the same memory port. So II becomes 2.</p><p>How do we solve this? The idea is to break one array into multiple parts, each part implemented as a BRAMs, so that we can access them at them same time. </p><p><img src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1604461378/posts/array_partition_ifubyx.png" alt="https://res.cloudinary.com/dxzx2bxch/image/upload/v1604461378/posts/array_partition_ifubyx.png"></p><p>There are three fashions of array partitioning:</p><ol><li><strong>Block</strong>: the original array is split into equally sized blocks of consecutive elements of the original </li><li><strong>Cyclic</strong>: the original array is split into equally sized blocks interleaving the elemetns of the original array.</li><li><strong>Complete</strong>: the default operation is to split the array into its individual elements. This corresponds to resolving a memory into registers.</li></ol><p><strong>References</strong></p><p><a href="https://www.xilinx.com/support/documentation/sw_manuals/xilinx2015_2/sdsoc_doc/topics/calling-coding-guidelines/concept_pipelining_loop_unrolling.html#:~:text=Both%20loop%20pipelining%20and%20loop%20unrolling%20exploit%20the%20parallelism%20between%20loop%20iterations.&amp;text=It%20implies%20that%20the%20operation,the%20operation%20in%20subsequent%20iteration." target="_blank" rel="noopener">Loop pipelining and loop unrolling</a></p><p>ug902-vivado-high-level-synthesis.pdf</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Loop optimization in Vivado HLS.&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorial" scheme="https://www.zzzdavid.tech/categories/Tutorial/"/>
    
    
  </entry>
  
  <entry>
    <title>On HeteroCL</title>
    <link href="https://www.zzzdavid.tech/HeteroCL/"/>
    <id>https://www.zzzdavid.tech/HeteroCL/</id>
    <published>2020-06-05T11:41:18.000Z</published>
    <updated>2022-08-26T19:32:11.146Z</updated>
    
    <content type="html"><![CDATA[<!-- excerpt --><p>Welcome to my notes on HeteroCL. </p><h2 id="How-do-we-install-HeteroCL"><a href="#How-do-we-install-HeteroCL" class="headerlink" title="How do we install HeteroCL?"></a>How do we install HeteroCL?</h2><p>To install HeteroCL, simple clone it from GitHub.<br><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="keyword">clone</span> <span class="title">--recursive</span> https://github.com/cornell-zhang/heterocl.git</span><br></pre></td></tr></table></figure></p><p>After that, go to the downloaded directory and make it.<br><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$ </span>make -j8</span><br></pre></td></tr></table></figure></p><h3 id="Options"><a href="#Options" class="headerlink" title="Options"></a>Options</h3><ol><li>You can set your own CMake or LLVM version by setting the PATH in <code>Makefile.config</code>.</li><li>You can turn off the VHLS C simulation feature by setting <code>USE_VIVADO_HLS</code> to 0 in <code>Makefile.config</code>.</li></ol><h2 id="Components-of-HeteroCL"><a href="#Components-of-HeteroCL" class="headerlink" title="Components of HeteroCL"></a>Components of HeteroCL</h2><div class="figure center" style="width:;"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1591322050/Screen_Shot_2020-06-05_at_09.53.57_v6rkbe.png" alt="HeteroCL framework overview"><span class="caption">HeteroCL framework overview</span></div><p>HeteroCL is High-Level-Synthesis with style. How is that so? Previous HLS tools, for example, Vivado HLS, need us users to specify hardware customizations while describing the function, so that it could generate correct circuits from that description. HeteroCL does exactly the same job, but it decouples the hardware customization from algorithm specification, allowing a cleaner programming process. </p><h3 id="Relation-with-TVM"><a href="#Relation-with-TVM" class="headerlink" title="Relation with TVM"></a>Relation with TVM</h3><p>HeteroCL is like an extension from TVM in the sense of compiler. TVM focuses on Tensor computations for CPU and GPU backends. Though TVM supports FPGA backend by using VTA, its tensor operations are pre-defined. On the other hand, HeteroCL offers additional features for FPGA programming that are more flexible and general. Such features include:</p><ul><li>Decoupled Algorithm/Hardware specification</li><li>Imperative programming for general applications</li><li>Various hardware customization primitives</li><li>Bit-accurate data types</li></ul><h2 id="Workflow-of-HeteroCL"><a href="#Workflow-of-HeteroCL" class="headerlink" title="Workflow of HeteroCL"></a>Workflow of HeteroCL</h2><p>Basic steps to use HeteroCL include:</p><ol><li>Import HeteroCL <code>import heterocl as hcl</code></li><li>Initialize Environment <code>hcl.init()</code></li><li>Define Algorithm</li><li>Define Inputs/Outputs <code>hcl.placeholder()</code></li><li>Apply Hardware Customization <code>s= hcl.create_schedule</code>  </li><li>View lower IR <code>hcl.lower(s)</code></li><li>Create Executable <code>f = hcl.build()</code></li><li>Run Executable <code>f()</code></li></ol><p>For FPGA backend HeteroCL generates HLS code. For CPU backend it generates a executable.</p><h2 id="Tutorials"><a href="#Tutorials" class="headerlink" title="Tutorials"></a>Tutorials</h2><p><a href="http://heterocl.csl.cornell.edu/doc/tutorials/tutorial_03_api.html" target="_blank" rel="noopener">Quick Link</a></p><h3 id="Using-Vivado-HLS-Backend-for-csynth-csim"><a href="#Using-Vivado-HLS-Backend-for-csynth-csim" class="headerlink" title="Using Vivado HLS Backend for csynth/csim"></a>Using Vivado HLS Backend for csynth/csim</h3><p>A simple HLS backend example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> heterocl <span class="keyword">as</span> hcl</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gemm</span><span class="params">(m=<span class="number">16</span>, n=<span class="number">16</span>, k=<span class="number">16</span>, dtype=hcl.Int<span class="params">()</span>, target=None)</span>:</span></span><br><span class="line">    matrix_1 = hcl.placeholder((m, k), dtype=dtype)</span><br><span class="line">    matrix_2 = hcl.placeholder((k, n), dtype=dtype)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">kernel</span><span class="params">(matrix_1, matrix_2)</span>:</span></span><br><span class="line">        r = hcl.reduce_axis(<span class="number">0</span>, k, <span class="string">'k'</span>)</span><br><span class="line">        <span class="keyword">return</span> hcl.compute((m, n),</span><br><span class="line">                <span class="keyword">lambda</span> x, y: hcl.sum(matrix_1[x, r] * matrix_2[r, y],</span><br><span class="line">                                     axis=r, dtype=dtype),</span><br><span class="line">                dtype=dtype,</span><br><span class="line">                name=<span class="string">"out_matrix"</span>)</span><br><span class="line"></span><br><span class="line">    s = hcl.create_schedule([matrix_1, matrix_2], kernel)</span><br><span class="line">    s.to([matrix_1, matrix_2], target.xcel)</span><br><span class="line">    s.to(kernel.out_matrix, target.host)</span><br><span class="line">    f = hcl.build(s, target=target)</span><br><span class="line">    <span class="keyword">return</span> f</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">target = hcl.platform.zc706</span><br><span class="line">target.config(compile=<span class="string">"vivado_hls"</span>, mode=<span class="string">"csim|csyn"</span>)</span><br><span class="line"></span><br><span class="line">m = k = n = <span class="number">16</span></span><br><span class="line">dtype = hcl.Int()</span><br><span class="line">hcl_m1 = hcl.asarray(np.random.randint(<span class="number">10</span>, size=(m, k)), dtype=hcl.Int())</span><br><span class="line">hcl_m2 = hcl.asarray(np.random.randint(<span class="number">10</span>, size=(m, k)), dtype=hcl.Int())</span><br><span class="line">hcl_m3 = hcl.asarray(np.zeros((m, n)), dtype=dtype)</span><br><span class="line"></span><br><span class="line">fg = gemm(m, n, k, dtype=hcl.Int(), target=target)</span><br><span class="line">fg(hcl_m1, hcl_m2, hcl_m3)</span><br><span class="line"></span><br><span class="line">report = fg.report(target)</span><br></pre></td></tr></table></figure><p>With above installation steps and Vivado Design Suite 2018.3 installed, I could not get HLS backend working.<br>See the below error message:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[15:27:16] Generating harness files ...</span><br><span class="line">[15:27:16] Compiling the program ...</span><br><span class="line">Traceback (most recent <span class="keyword">call</span> <span class="keyword">last</span>):</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"hls_backend_example.py"</span>, line <span class="number">36</span>, <span class="keyword">in</span> &lt;<span class="keyword">module</span>&gt;</span><br><span class="line">    fg(hcl_m1, hcl_m2, hcl_m3)</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/home/zhangniansong/.local/lib/python3.7/site-packages/heterocl-0.1-py3.7.egg/heterocl/tvm/_ffi/function.py"</span>, line <span class="number">128</span>, <span class="keyword">in</span> __call__</span><br><span class="line">    <span class="keyword">return</span> f(*args)</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/home/zhangniansong/.local/lib/python3.7/site-packages/heterocl-0.1-py3.7.egg/heterocl/tvm/_ffi/_ctypes/function.py"</span>, line <span class="number">183</span>, <span class="keyword">in</span> __call__</span><br><span class="line">    ctypes.byref(ret_val), ctypes.byref(ret_tcode)))</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/home/zhangniansong/.local/lib/python3.7/site-packages/heterocl-0.1-py3.7.egg/heterocl/tvm/_ffi/base.py"</span>, line <span class="number">66</span>, <span class="keyword">in</span> check_call</span><br><span class="line">    <span class="keyword">raise</span> TVMError(py_str(_LIB.TVMGetLastError()))</span><br><span class="line">heterocl.tvm._ffi.base.TVMError: TVMCall CFunc <span class="keyword">Error</span>:</span><br><span class="line">Traceback (most recent <span class="keyword">call</span> <span class="keyword">last</span>):</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/home/zhangniansong/.local/lib/python3.7/site-packages/heterocl-0.1-py3.7.egg/heterocl/tvm/_ffi/_ctypes/function.py"</span>, line <span class="number">54</span>, <span class="keyword">in</span> cfun</span><br><span class="line">    rv = local_pyfunc(*pyargs)</span><br><span class="line">  <span class="keyword">File</span> <span class="string">"/home/zhangniansong/.local/lib/python3.7/site-packages/heterocl-0.1-py3.7.egg/heterocl/tvm/runtime.py"</span>, line <span class="number">153</span>, <span class="keyword">in</span> copy_and_compile</span><br><span class="line">    <span class="keyword">with</span> <span class="keyword">open</span>(<span class="string">"project/run.tcl"</span>,<span class="string">"r"</span>) <span class="keyword">as</span> tcl_file:</span><br><span class="line">FileNotFoundError: [Errno <span class="number">2</span>] <span class="keyword">No</span> such <span class="keyword">file</span> <span class="keyword">or</span> <span class="keyword">directory</span>: <span class="string">'project/run.tcl'</span></span><br></pre></td></tr></table></figure><p>HeteroCL can’t find my Vivado HLS include files. So what we gotta do is to specify where the library files are located.</p><p>I added following two lines in the <code>~/.bashrc</code> file</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> VIVADO_HLS_HOME=/home/zhangniansong/Vivado/2018.3</span><br><span class="line"><span class="built_in">export</span> CPATH=<span class="variable">$CPATH</span>:<span class="variable">$VIVADO_HLS_HOME</span>/include</span><br></pre></td></tr></table></figure><p><code>CPATH</code> is for g++ include directory. It is the same as <code>-I</code>.</p><p>Then I reinstall HeteroCL, and Voila!</p><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">INFO: [Common 17-206] Exiting vivado_hls at Sat Jul  4 16:07:47 2020...</span><br><span class="line">+-------------------+-----------------------------------+</span><br><span class="line">|<span class="string"> HLS Version       </span>|<span class="string"> Vivado HLS 2018.3                 </span>|</span><br><span class="line">|<span class="string"> Product family    </span>|<span class="string"> zynq                              </span>|</span><br><span class="line">|<span class="string"> Target device     </span>|<span class="string"> xc7z020clg484-1                   </span>|</span><br><span class="line">|<span class="string"> Top Model Name    </span>|<span class="string"> test                              </span>|</span><br><span class="line">+-------------------+-----------------------------------+</span><br><span class="line">|<span class="string"> Target CP         </span>|<span class="string"> 10.00 ns                          </span>|</span><br><span class="line">|<span class="string"> Estimated CP      </span>|<span class="string"> 8.510 ns                          </span>|</span><br><span class="line">|<span class="string"> Latency (cycles)  </span>|<span class="string"> Min 18308 ; Max 18308             </span>|</span><br><span class="line">|<span class="string"> Interval (cycles) </span>|<span class="string"> Min 18309 ; Max 18309             </span>|</span><br><span class="line">|<span class="string"> Resources         </span>|<span class="string"> Type        Used    Total    Util </span>|</span><br><span class="line">|<span class="string">                   </span>|<span class="string"> --------  ------  -------  ------ </span>|</span><br><span class="line">|<span class="string">                   </span>|<span class="string"> BRAM_18K       3      280      1% </span>|</span><br><span class="line">|<span class="string">                   </span>|<span class="string"> DSP48E         3      220      1% </span>|</span><br><span class="line">|<span class="string">                   </span>|<span class="string"> FF           496   106400      0% </span>|</span><br><span class="line">|<span class="string">                   </span>|<span class="string"> LUT          756    53200      1% </span>|</span><br><span class="line">+-------------------+-----------------------------------+</span><br><span class="line">[16:07:47] Execution complete</span><br></pre></td></tr></table></figure><p>Vivado HLS backend now works.</p><hr><h1 id="Understanding-How-HeteroCL-works"><a href="#Understanding-How-HeteroCL-works" class="headerlink" title="Understanding How HeteroCL works"></a>Understanding How HeteroCL works</h1><p>To understand how HeteroCL works, we try to answer following questions:</p><ol><li>How is the IR defined?</li><li>How is the dataflow graph built?</li></ol><h2 id="The-extended-Halide-IR-system"><a href="#The-extended-Halide-IR-system" class="headerlink" title="The extended Halide IR system"></a>The extended Halide IR system</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;HeteroCL is a software-defined reconfigurable computing framework. It is composed of a Python-based DSL (Domain-Specific Language) and an FPGA compilation flow. &lt;/p&gt;
    
    </summary>
    
      <category term="Tutorial" scheme="https://www.zzzdavid.tech/categories/Tutorial/"/>
    
    
      <category term="FPGA" scheme="https://www.zzzdavid.tech/tags/FPGA/"/>
    
  </entry>
  
  <entry>
    <title>Neural Architecture Search, Basic Concepts, Paper Summary and Implementations</title>
    <link href="https://www.zzzdavid.tech/NAS/"/>
    <id>https://www.zzzdavid.tech/NAS/</id>
    <published>2020-05-14T21:27:00.000Z</published>
    <updated>2022-08-26T19:32:11.147Z</updated>
    
    <content type="html"><![CDATA[<!-- excerpt --><!-- toc --><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Neural Architecture Search, or NAS, can be seen as a subtask of AutoML. It tries to automate the designing process of neural network architectures. An architecture search space can be seen as a Directed Asyclic Graph (DAG), and each possible architecture is a subgraph. We denote the search space as $A$, the subgraph $a$, the searched neural network $N(a,w)$. The goals of NAS are often two-fold: </p><ul><li>Weight Optimization</li></ul><script type="math/tex; mode=display">w_a = \underset{w}{\operatorname{argmin}}(Loss_{train}(N(a,w)))</script><ul><li>Architecture Optimization</li></ul><script type="math/tex; mode=display">a^* = \underset{a\in A}{\operatorname{argmax}}(Accuracy_{val}(N(a,w)))</script><p>The constraints are diverse: such as computation (FLOPS), speed (latency), or even energy consumption and memory usage.</p><h2 id="Surveys"><a href="#Surveys" class="headerlink" title="Surveys"></a>Surveys</h2><h3 id="Neural-Architecture-Search-A-Survey"><a href="#Neural-Architecture-Search-A-Survey" class="headerlink" title="Neural Architecture Search: A Survey"></a>Neural Architecture Search: A Survey</h3><ul><li>Authors: Thomas Elsken, Jan Hendrik Metzen, Frank Hutter</li><li>Institution: Bosch Center for AI, University of Freiburg</li><li>arXiv: <a href="https://arxiv.org/abs/1808.05377" target="_blank" rel="noopener">https://arxiv.org/abs/1808.05377</a></li></ul><div class="figure center" style="width:100%;"><img class="fig-img" src="https://res.cloudinary.com/dx7aiyb0q/image/upload/v1589450934/posts/Screen_Shot_2020-05-14_at_18.08.16_amj1pu.png" style="width:100%;"alt=""></div><p>NAS methods encompasses three sub-tasks: designing search space, select search strategy, and use proper estimation strategy.</p><h4 id="Search-Space"><a href="#Search-Space" class="headerlink" title="Search Space"></a>Search Space</h4><p>The search space defines what architectures are discoverable during search. Search space designs can be largely classified into global search space and cell-based search space.</p><ul><li>Global: chain-structured, chain-structured with skip, segment-based</li><li>Cell-based</li></ul><div class="figure center" style="width:;"><img class="fig-img" src="https://res.cloudinary.com/dx7aiyb0q/image/upload/v1589455315/posts/searchspace_v83k9c.png" alt="Global Search Space"><span class="caption">Global Search Space</span></div><h5 id="Chain-Structured-Search-Space"><a href="#Chain-Structured-Search-Space" class="headerlink" title="Chain-Structured Search Space"></a>Chain-Structured Search Space</h5><p>The simplist form of search space. Parameters include: the number of layers, the type of layers, hyperparameters of the layers, e.g., number of filters, kernel sizes and strides for convolutional layer.</p><h5 id="Chain-Structured-with-Skips"><a href="#Chain-Structured-with-Skips" class="headerlink" title="Chain-Structured with Skips"></a>Chain-Structured with Skips</h5><p>Obviously the idea comes from ResNet, DenseNet or Xception. The branching can be described with layer’s inputs as a function </p><script type="math/tex; mode=display">g_i (L_{i-1}^{out}, ..., L_0^{out})</script><p>Taking existing architectures for example, for chain-structured net, <script type="math/tex">g_i (L_{i-1}^{out}, ..., L_0^{out}) = L_{i-1}^{out}</script>. For ResNet, <script type="math/tex">g_i (L_{i-1}^{out}, ..., L_0^{out}) = L_{i-1}^{out} + L_j^{out}, (j < i-1)</script>. For DenseNet, <script type="math/tex">g_i (L_{i-1}^{out}, ..., L_0^{out}) = concat(L_{i-1}^{out},...,L_0^{out})</script></p><h5 id="Segment-based"><a href="#Segment-based" class="headerlink" title="Segment-based"></a>Segment-based</h5><p>Segment-based method is considered as a global search space design, but with another level of granularity: segment, or block. </p><div class="figure center" style="width:;"><img class="fig-img" src="https://res.cloudinary.com/dx7aiyb0q/image/upload/v1589509162/posts/Screen_Shot_2020-05-15_at_10.17.01_neskhw.png" alt="Mnas Search Space"><span class="caption">Mnas Search Space</span></div><p>Taking Mnas as an example, the network layers are grouped into a many blocks. The skeleton constructed with blocks is predefined. Inside each block, there are a variable number of repeated layers. Blocks are not necessarily the the same, the layer amount and operations can be different.</p><h4 id="Cell-based-Structure"><a href="#Cell-based-Structure" class="headerlink" title="Cell-based Structure"></a>Cell-based Structure</h4><p>Cell-based architecture is a lot like the segment-based one. However, instead of searching the topology/hyperparameters of each block, cell-based approach defines several types of cells, then connects repeated cells in a pre-defined manner. In such ways, the search space is further downsized.</p><div class="figure center" style="width:80%;"><img class="fig-img" src="https://res.cloudinary.com/dx7aiyb0q/image/upload/v1589510150/posts/Screen_Shot_2020-05-15_at_10.35.39_dl207b.png" style="width:80%;"alt="Illustration of the cell search space"><span class="caption">Illustration of the cell search space</span></div><p>(Zoph, 2018) defined two types of cell: <em>normal cell</em> or <em>reduction cell</em>. Normal cell preserves dimensionality while reduction cell reduces spatial dimensions. </p><p>The problem of cell-based structure is that the macro-architecture, i.e. the structure based on cells, are still manually designed. (Liu, 2018b) tried to overcome macro-architecture engineering by introducing hierarchical search space. </p><h4 id="Search-Strategy"><a href="#Search-Strategy" class="headerlink" title="Search Strategy"></a>Search Strategy</h4><h5 id="Reinforcement-Learning-Strategy"><a href="#Reinforcement-Learning-Strategy" class="headerlink" title="Reinforcement Learning Strategy"></a>Reinforcement Learning Strategy</h5><p>Different RL approaches differ in how they represent the agent’s policy and how they optimize it. </p><ul><li>Architecture generation: agent’s action</li><li>Search space: action space</li><li>Evaluation: agent’s reward is based on an estimate of the performance.</li></ul><p>Agent’s Policy:</p><ul><li>RNN policy: sequentially sample a string that encodes the architecture</li><li>…</li></ul><p>Training method:</p><ul><li>REINFORCE policy gradient algorithm</li><li>Proximal Policy Optimization</li><li>Q-learning</li></ul><h5 id="Neron-Evolutionary-Strategy"><a href="#Neron-Evolutionary-Strategy" class="headerlink" title="Neron-Evolutionary Strategy"></a>Neron-Evolutionary Strategy</h5><p>Neuro-evolutionary methods differ in how they sample parents, udpate populations, and generate offsprings.</p><div class="figure center" style="width:;"><img class="fig-img" src="https://res.cloudinary.com/dx7aiyb0q/image/upload/v1589528317/posts/evo_u31wss.png" alt=""></div><p>In the context of NAS, mutations are local operations, such as adding or removing a layer, altering the hyperparameters of a layer, adding skip connections.</p><h5 id="Gradient-based-Optimization"><a href="#Gradient-based-Optimization" class="headerlink" title="Gradient-based Optimization"></a>Gradient-based Optimization</h5><p>(Liu, 2019b) proposed a <em>continuous relaxation</em> to enable direct gradient-based optimization: instead of fixing a single operation <script type="math/tex">o_i</script> to be executed at a specific layer, the authors compute a convex combination from a set of operations <script type="math/tex">{o_1, ..., o_m}</script>. Given a layer input $x$, the layer output $y$ is computed as <script type="math/tex">y = \sum_{i=1}^m \alpha_i o_i(x)</script>, <script type="math/tex">(\alpha_i \geq 0, \sum_{i=1}^m \alpha_i = 1)</script></p><h4 id="Performance-Estimation-Strategy"><a href="#Performance-Estimation-Strategy" class="headerlink" title="Performance Estimation Strategy"></a>Performance Estimation Strategy</h4><div class="figure center" style="width:;"><img class="fig-img" src="https://res.cloudinary.com/dx7aiyb0q/image/upload/v1589529265/posts/Screen_Shot_2020-05-15_at_15.54.14_wmhqux.png" alt="Overview of different methods for speeding up performance estimation in NAS"><span class="caption">Overview of different methods for speeding up performance estimation in NAS</span></div><ul><li><p><strong>Lower Fidelity</strong><br> The bias in the estimation is the gap between the full evaluation. This may not be problematic as only relative ranking is considered. But when the gap is large, the relative ranking may not be stable.</p></li><li><p><strong>Learning Curve Extrapolation</strong> or <strong>Surrogate Model</strong><br> Using early learning curve of architecture information to extrapolate performance. </p></li><li><p><strong>Network Morphism</strong><br>No inherent search space upper bound. However, strict netwrok morphisms can only make architectures larger, therefore leading to redundancy. </p></li><li><p><strong>One-Shot</strong><br>Training only one supernet, so only validation is needed at evaluation stage. The supernet restricts the search space, and may be limited by GPU memory. </p><ul><li><strong>ENAS</strong> learns a RNN controller that samples architectures from the search space and trains the one-shot model based on approximate gradients obtained through REINFORCE.</li><li><strong>DARTS</strong> jointly optimize all weights with a continuous relaxation of the search space. </li><li><strong>SNAS</strong> optimizes a distribution over the candidate operations. The authors employ the concrete distribution and reparameterization to relax the discrete distribution and make it differentiable. </li><li><strong>Proxyless NAS</strong> “binarizes” the weights, maksing out all but one edge per operation to overcome the necessity of keeping the entire supernet model in GPU memory.</li></ul></li></ul><blockquote><p>It is currently not well understood which biases they introduce into the search if the sampling of distribution of architectures is optimized along with the one-shot model instead of fixing it. For example, an initial bias in exploring certain parts of the search space more than others might lead to the weights of the one-shot model being better adapted for these architectures, which in turn would reinforce the bias of the search to these parts in the search space. <strong>This might result in premature convergence of NAS or little correlation between the one-shot and true performance of an architecture.</strong></p></blockquote><h4 id="Future-Directions"><a href="#Future-Directions" class="headerlink" title="Future Directions"></a>Future Directions</h4><ol><li>Applying NAS to less explored domains, such as image restoration, semantic segmentation, transfer learning, machine translation, GAN, or sensor fusion. </li><li>Develop MAS methos for multi-task problems, and for multi-objective problems, such as latency and power optimization.</li><li>Universal NAS on different tasks. More general and flexible search space design. </li><li>Developing benchmarks.</li></ol><p>While NAS has achieved impressive performance, so far it provides little insights into why specific architectures work well and how similar the architectures derived in indepen- dent runs would be. Identifying common motifs, providing an understanding why those motifs are important for high performance, and investigating if these motifs generalize over different problems would be desirable.</p><h2 id="Paper-Summary"><a href="#Paper-Summary" class="headerlink" title="Paper Summary"></a>Paper Summary</h2><h3 id="MnasNet-Platform-Aware-Neural-Architecture-Search-for-Mobile"><a href="#MnasNet-Platform-Aware-Neural-Architecture-Search-for-Mobile" class="headerlink" title="MnasNet: Platform-Aware Neural Architecture Search for Mobile"></a>MnasNet: Platform-Aware Neural Architecture Search for Mobile</h3><ul><li>Authors: Mingxing Tan, Bo Chen <em>et al.</em></li><li>Year: </li><li>Intitution: Google</li><li>arXiv: <a href="https://arxiv.org/pdf/1807.11626.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1807.11626.pdf</a></li></ul><h3 id="Once-For-All-Train-One-Network-and-Specialize-It-For-Efficient-Deployment"><a href="#Once-For-All-Train-One-Network-and-Specialize-It-For-Efficient-Deployment" class="headerlink" title="Once-For-All: Train One Network and Specialize It For Efficient Deployment"></a>Once-For-All: Train One Network and Specialize It For Efficient Deployment</h3><ul><li>Authors: Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, Song Han</li><li>Venue: ICLR 2020</li><li>Institution: MIT</li><li>arXiv: <a href="https://arxiv.org/abs/1908.09791" target="_blank" rel="noopener">https://arxiv.org/abs/1908.09791</a></li><li>Code: <a href="https://github.com/mit-han-lab/once-for-all" target="_blank" rel="noopener">https://github.com/mit-han-lab/once-for-all</a></li></ul><h4 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h4><p>Designing a <em>once-for-all</em> network that flexibly supports different depths, widths, kernel sizes, and resolutions without retraining. </p><h4 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h4><ul><li>The model training stage and architecture search stage is decoupled.</li><li>A <strong>progressive shrinking</strong> method to trian the supernet</li></ul><h3 id="Black-Box-Search-Space-Profiling-for-Accelerator-Aware-Neural-Architecture-Search"><a href="#Black-Box-Search-Space-Profiling-for-Accelerator-Aware-Neural-Architecture-Search" class="headerlink" title="Black Box Search Space Profiling for Accelerator-Aware Neural Architecture Search"></a>Black Box Search Space Profiling for Accelerator-Aware Neural Architecture Search</h3><ul><li>Authors: Shulin Zeng, Hanbo Sun, Yu Xing, Xuefei Ning, Yi Shan, Xiaoming Chen, Yu Wang, Huazhong Yang</li><li>Venue: 2020 ASP-DAC</li><li>Institution: Tsinghua University, and Institute of Computing Technology, Chinese Academy of Sciences</li><li>Xplore: <a href="https://ieeexplore.ieee.org/document/9045179" target="_blank" rel="noopener">https://ieeexplore.ieee.org/document/9045179</a></li></ul><h4 id="Motivation-1"><a href="#Motivation-1" class="headerlink" title="Motivation"></a>Motivation</h4><ol><li>Designing a suitable search space for DNN accelerators is still challenging.</li><li>Evaluating latency across platforms with various architectures and compilers is hard because we don’t have prior knowlege of the hardware &amp; compiler.</li></ol><h4 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h4><ol><li>A black-box search space tuning method for Instruction Set Architecture (ISA) accelerators.</li><li>A layer-adaptive latency correction method.</li></ol><h5 id="The-Complete-NAS-Workflow"><a href="#The-Complete-NAS-Workflow" class="headerlink" title="The Complete NAS Workflow"></a>The Complete NAS Workflow</h5><div class="figure center" style="width:;"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1591263877/Screen_Shot_2020-06-04_at_17.41.54_fqqxq2.png" alt="Entire NAS workflow"><span class="caption">Entire NAS workflow</span></div><p>The magic happens in “Basenet Pool” and “Policy-aware Latency”, where the authors constructs a automatic search space design method. “Policy-aware Latency LUT” stands for an improved latency profiling method. Previous works use standalone block latency to build a LUT, ignoring inter-layer latency influences. Such influences are caused by compiler policies, such as “layer fusion” that combines layers to shorten latency. This new latency profiling method aims to take inter-layer latency dependencies into account without knowing the underlying compiler or hardware.</p><h5 id="How-to-Optimize-Search-Space"><a href="#How-to-Optimize-Search-Space" class="headerlink" title="How to Optimize Search Space?"></a>How to Optimize Search Space?</h5><div class="figure center" style="width:;"><img class="fig-img" src="https://res.cloudinary.com/dxzx2bxch/image/upload/v1591263876/Screen_Shot_2020-06-04_at_17.42.16_uij13m.png" alt="Search Space Optimization"><span class="caption">Search Space Optimization</span></div><p><strong>SSBN</strong>: stands for <strong>Search Space Base Network</strong>. Based on a specific net from the Search Space, change at most one block at a time to get a SSBN. Because the original Search Space can be regarded as linear combination of SSBNs, this reduces complexity without harming degree of freedom.</p><p><strong>Generating Basenet Pool</strong>: there are currently 3 types of mainstream hand-designed blocks: VGG, Res, and MobileNet-based. For each type the authors construct a Specified Network (SN), and obtain its SSBNs. All SSBNs together are the Basenet Pool.</p><p><strong>Selecting Search Space</strong>: for each group of SSBNs, the authors design a cost function combining <strong>accuracy</strong> and <strong>latency</strong>. For each group of SSBNs, their costs are first calculated. After that, they are sorted in ascending w.r.t. the cost. A threshold defined by the user selects a range of<br>SSBNs that goes into the new search space. The new search space is much smaller.</p><h5 id="How-to-Correct-Block-Latency"><a href="#How-to-Correct-Block-Latency" class="headerlink" title="How to Correct Block Latency?"></a>How to Correct Block Latency?</h5><p>The authors design two methods for latency correction. The first one uses overall network latency measurements to calculate the difference between standalone and actual block latency. The second one is a refinement of the first. It takes other block’s latency into account, thus more fine-grained.</p><script type="math/tex; mode=display">L_{BNblock}(n,l) = L_{BN}(n,l) - L_{SN} + L_{SNblock}(l)</script><ul><li>$L_{BNblock}(n,l)$ is the block latency we want to know. </li><li>$n$ is the type of block. $l$ is the index of layer.</li><li>$L_{SNblock}(l)$ is the standalone layer latency.</li><li>$L_{SN}$ is the overal latency of the network.</li><li>$L_{BN}(n,l)$ is the overall latency where the target block is substituted.</li></ul><script type="math/tex; mode=display">L_{CNblock}(n,l) = \frac{1}{L} \sum_{(n',l' \in CN)} L_{BNblock}(n, l, n')</script><p>This one is difficult to understand.</p><p>$L$ is the number of layers (or blocks) in the network. The authors first create $L$ networks preserving the target block but the rest are all one particular type. Then within each of these “Specified Network”, our target block’s latency is measured with method 1. At last, the block latency measurements are averaged to get the estimated “candidate network (CN)” block latency.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Knowledge and methods for Neural Architecture Search and Automated Deep Learning.&lt;/p&gt;
    
    </summary>
    
      <category term="Tutorial" scheme="https://www.zzzdavid.tech/categories/Tutorial/"/>
    
    
      <category term="AI" scheme="https://www.zzzdavid.tech/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Goodbye to All That</title>
    <link href="https://www.zzzdavid.tech/Goodbye-to-All-That/"/>
    <id>https://www.zzzdavid.tech/Goodbye-to-All-That/</id>
    <published>2016-01-10T04:41:18.000Z</published>
    <updated>2022-08-26T19:32:11.146Z</updated>
    
    <content type="html"><![CDATA[<p>  “All I mean is that I was very young in New York, and that at some point the golden rhythm was broken, and I am not that young anymore.”<br>  — Joan Didion</p><hr><a id="more"></a><blockquote><p>How many miles to Babylong?<br><br></p><p>Three score miles and and ten—</p><p>Can I get there by candlelight?</p><p>Yes, and back again—</p><p>If your feet are nimble and light</p><p>You can get there by candlelight.</p></blockquote><hr><p>It is easy to see the beginnings of things, and harder to see the ends. I can remember now, with a clarity that makes the nerves in the back of my neck constrict, when New York began for me, but I cannot lay my  finger upon the moment it ended, can never cut through the ambiguities and second starts and broken resolves to the exact place on the page where the heroine is no longer as optimistic as she once was. When I first saw New York I was twenty, and it was summertime, and I got off a DC-7 at the old Idlewild temporary terminal in a new dress which had seemed very smart in Sacramento but seemed less smart already, even in  the old Idlewild temporary terminal, and the warm air smelled of mildew and some instinct, programmed by all the movies I had ever seen and all the songs I had ever read about New York, informed me that it would never be quite the same again. In fact it never was. Some time later there was a song in the jukeboxes on the Upper East Side that went “but where is the schoolgirl who used to be me,” and if it was late enough at night I used to wonder that. I know now that almost everyone wonders something like that, sooner or later and no matter what he or she is doing, but one of the mixed blessings of being twenty and twenty-one and even twenty-three is the conviction that nothing like this, all evidence to the contrary notwithstanding, has ever happened to anyone before.</p><p>Of course it might have been some other city, had circumstances been different and the time been different and had I been different, might have been Paris or Chicago or even San Francisco, but because I am talking about myself I am talking here about New York. That first night I opened my window on the bus into town and watched for the skyline, but all I could see were the wastes of Queens and big signs that said MIDTOWN TUNNEL THIS LANE and then a flood of summer rain (even that seemed remarkable and exotic, for I had come out of the West where there was no summer rain), and for the next three days I sat wrapped in blankets in a hotel room air conditioned to 35 degrees and tried to get over a cold and a high fever. It did not occur to me to call a doctor, because I knew none, and although it did occur to me to call the desk and ask that the air conditioner be turned off, I never called, because I did not know how much to tip whoever might come—was anyone ever so young? I am here to tell you that someone was. All I could do during those years was talk long-distance to the boy I already knew I would never marry in the spring. I would stay in New York, I told him, just six months, and I could see the Brooklyn Bridge from my window. As it turned out the bridge was the Triborough, and I stayed eight years.</p><hr><div class="figure " style="width:;"><img class="fig-img" src="https://res.cloudinary.com/dx7aiyb0q/image/upload/v1582836789/posts/didion2_gzerr9.jpg" alt=""></div><p>In retrospect it seems to me that those days before I knew the names of all the bridges were happier than the ones that came later, but perhaps you will see that as we go along. Part of what I want to tell you is what it is like to be young in New York, how six months can become eight years with the deceptive ease of a film dissolve, for that is how those years appear to me now, in a long sequence of sentimental dissolves and old-fashioned trick shots—the Seagram Building fountains dissolve into snowflakes, I enter a revolving door at twenty and come out a good deal older, and on a different street. But most particularly I want to explain to you, and in the process perhaps to myself, why I no longer live in New York. It is often said that New York is a city for only the very rich and the very poor. It is less often said that New York is also, at least for those of us who came there from somewhere else, a city only for the very young.</p><p>I remember once, one cold bright December evening in New York, suggesting a friend who complained of having been around too long that he come with me to a party where there would be, I assured him with the bright resourcefulness of twenty-three, “new faces.” He laughed literally until he choked, and I had to roll down the taxi window and hit him on the back. “New faces,” he said finally, “don’t tell me about new faces.” It seemed that the last time he had gone to a party where he had been promised “new faces,” there had been fifteen people in the room, and he had already spelt with five of the women and owed money to all but two of the men. I laughed with him, but the first snow had just begun to fall and the big Christmas trees glittered yellow and white as far as I could see up Park Avenue and I had a new dress and it would be a long while before I would come to understand the particular moral of the story.</p><p>It would be a long while because, quite simply, I was in love with New York. I do not mean “love” in any colloquial way, I mean that I was in love with the city, the way you love the first person who ever touches you and you never love anyone quite that way again. I remember walking across Sixty-second Street one twilight that first spring, or the second spring, they were all alike for a while. I was late to meet someone but I stopped at Lexington Avenue and bought a peach and stood on the corner eating it and knew that I had come out out of the West and reached the mirage. I could taste the peach and feel the soft air blowing from a subway grating on my legs and I could smell lilac and garbage and expensive perfume and I knew that it would cost something sooner or later—because I did not belong there, did not come from there—but when you are twenty-two or twenty-three, you figure that later you will have a high emotional balance, and be able to pay whatever it costs. I still believed in possibilities then, still had the sense, so peculiar to New York, that something extraordinary would happen any minute, any day, any month. I was making only 65 or 70 dollars then a week then (“Put yourself in Hattie Carnegie’s hands,” I was advised without the slightest trace of irony by an editor of the magazine for which I worked), so little money that some weeks I had to charge food at Bloomingdale’s gourmet shop in order to eat, a fact which went unmentioned in the letters I wrote to California. I never told my father that I needed money because then he would have sent it, and I would never know if I could do it by myself. At that time making a living seemed a game to me, with arbitrary but quite inflexible rules. And except on a certain kind of winter evening—six-thirty in the Seventies, say, already dark and bitter with a wind off the river, when I would be walking very fast toward a bus and would look in the bright windows of brownstones and see cooks working in clean kitchens and and imagine women lighting candles on the floor above and beautiful children being bathed on the floor above that—except on nights like those, I never felt poor; I had the feeling that if I needed money I could always get it. I could write a syndicated column for teenagers under the name “Debbi Lynn” or I could smuggle gold into India or I could become a $100 call girl, and none of would matter.</p><p>Nothing was irrevocable; everything was within reach. Just around every corner lay something curious and interesting, something I had never before seen or done or known about. I could go to a party and meet someone who called himself Mr. Emotional Appeal and ran The Emotional Appeal Institute or Tina Onassis Blandford or a Florida cracker who was then a regular on what the called “the Big C,” the Southampton-El Morocco circuit (“I’m well connected on the Big C, honey,” he would tell me over collard greens on his vast borrowed terrace), or the widow of the celery king of the Harlem market or a piano salesman from Bonne Terre, Missouri, or someone who had already made and list two fortunes in Midland, Texas. I could make promises to myself and to other people and there would be all the time in the world to keep them. I could stay up all night and make mistakes, and none of them would count.</p><p>You see I was in a curious position in New York: it never occurred to me that I was living a real life there. In my imagination I was always there for just another few months, just until Christmas or Easter or the first warm day in May. For that reason I was most comfortable with the company of Southerners. They seemed to be in New York as I was, on some indefinitely extended leave from wherever they belonged, disciplined to consider the future, temporary exiles who always knew when the flights left for New Orleans or Memphis or Richmond or, in my case, California. Someone who lives with a plane schedule in the drawer lives on a slightly different calendar. Christmas, for example, was a difficult season. Other people could take it in stride, going to Stowe or going abroad or going for the day to their mothers’ places in Connecticut; those of us who believed that we lived somewhere else would spend it making and canceling airline reservations, waiting for weatherbound flights as if for the last plane out of Lisbon in 1940, and finally comforting one another, those of us who were left, with oranges and mementos and smoked-oyster stuffings of childhood, gathering close, colonials in a far country.</p><p>Which is precisely what we were. I am not sure that it is possible for anyone brought up in the East to appreciate entirely what New York, the idea of New York, means to those of us who came out of the West and the South. To an Eastern child, particularly a child who has always has an uncle on Wall Street and who has spent several hundred Saturdays first at F.A.O. Schwarz and being fitted for shoes at Best’s and then waiting under the Biltmore clock and dancing to Lester Lanin, New York is just a city, albeit the city, a plausible place for people to live, But to those of us who came from places where no one had heard of Lester Lanin and Grand Central Station was a Saturday radio program, where Wall Street and Fifth Avenue and Madison Avenue were not places at all but abstractions (“Money,” and “High Fashion,” and “The Hucksters”), New York was no mere city. It was instead an infinitely romantic notion, the mysterious nexus of all love and money and power, the shining and perishable dream itself. To think of “living” there was to reduce the miraculous to the mundane; one does not “live” at Xanadu.</p><p>In fact it was difficult in the extreme for me to understand those young women for whom New York was not simply an ephemeral Estoril but a real place, girls who bought toasters and installed new cabinets in their apartments and committed themselves to some reasonable furniture. I never bought any furniture in New York. For a year or so I lived in other people’s apartments; after that I lived in the Nineties in an apartment furnished entirely with things taken from storage by a friend whose wife had moved away. And when I left the apartment in the Nineties (that was when I was leaving everything, when it was all breaking up) I left everything in it, even my winter clothes and the map of Sacramento County I had hung on the bedroom wall to remind me who I was, and I moved into a monastic four-room floor-through on Seventy-fifth Street. “Monastic” is perhaps misleading here, implying some chic severity; until after I was married and my husband moved some furniture in, there was nothing at all in those four rooms except a cheap double mattress and box springs, ordered by telephone the day I decided to move, and two French garden chairs lent me by a friend who imported them. (It strikes me now that the people I knew in New York all had curious and self-defeating sidelines. They imported garden chairs which did not sell very well at Hammacher Schlemmer or they tried to market hair staighteners in Harlem or they ghosted exposés of Murder Incorporated for Sunday supplements. I think that perhaps none of us was very serious, engagé only about our most private lives.)</p><p>All I ever did to that apartment was hang fifty yards of yellow theatrical silk across the bedroom windows, because I had some idea that the gold light would make me feel better, but I did not bother to weight the curtains correctly and all that summer the long panels of transparent golden silk would blow out  the windows and get tangled and drenched in afternoon thunderstorms. That was the year, my twenty-eight, when I was discovering that not all of the promises would be kept, that some things are in fact irrevocable and that it had counted after all, every evasion and ever procrastination, every word, all of it.</p><hr><p>That is what it was all about, wasn’t it? Promises? Now when New York comes back to me it comes in hallucinatory flashes, so clinically detailed that I sometimes wish that memory would effect the distortion with which it is commonly credited. For a lot of the time I was in New York I used a perfume called Fleurs de Rocaille, and then L’Air du Temps, and now the slightest trace of either can short-circuit my connections for the rest of the day. Nor can I smell Henri Bendel jasmine soap without falling back into the past, or the particular mixture of spices used for boiling crabs. There were barrels of crab boil in a Czech place in the Eighties where I once shopped. Smells, of course, are notorious memory stimuli, but there are other things which affect me the same way. Blue-and-white striped sheets. Vermouth cassis. Some faded nightgowns which were new in 1959 or 1960, and some chiffon scarves I bought about the same time.</p><p>I suppose that a lot of us who have been very young in New York have the same scenes in our home screens. I remember sitting in a lot of apartments with a slight headache about five o’clock in the morning. I had a friend who could not sleep, and he knew a few other people who had the same trouble, and we would watch the sky lighten and have a last drink with no ice and then go home in the early morning, when the streets were clean and wet (had it rained in the night? we never knew) and the few cruising taxis still had their headlights on and the only color was the red and green of traffic signals. The White Rose bars opened very early in the morning; I recall waiting in one of them to watch an astronaut go into space, waiting so long that at the moment it actually happened I had my eyes not on the television screen but on a cockroach on the tile floor. I liked the bleak branches above Washington Square at dawn, and the monochromatic flatness of Second Avenue, the fire escapes and the grilled storefronts peculiar and empty in their perspective.</p><p>It is relatively hard to fight at six-thirty or seven in the morning, without any sleep, which was perhaps one reason why we stayed up all night, and it seemed to me a pleasant time of day. The windows were shuttered in that apartment in the Nineties and I could sleep for a few hours and then go to work. I could work the on two or three hours’ sleep and a container of coffee from Chock Full O’ Nuts. I liked going to work, liked the soothing and satisfactory rhythm of getting out a magazine, liked the orderly progression of four-color closings and two-color closings and black-and-white closings and then The Product, no abstraction but something which looked effortlessly glossy and could be picked up on a newsstand and weighed in the hand. I liked all the minutiae of proofs and layouts, liked working late on the nights the magazines went to press, sitting and reading Variety and waiting for the copy desk to call. From my office, I could look across town to the weather signal on the Mutual of New York Building and the lights that alternately spelled TIME and LIFE above Rockeffeler Plaza; that pleased me obscurely, and so did walking uptown in the mauve eight o’clocks of early summer evenings and looking at things, Lowestoft tureens in Fifty-seventh Street windows, people in evening clothes trying to get taxis, the trees just coming into full leaf, the lambent air, all the sweet promises of money and summer.</p><p>Some years passed, but I still did not lose that sense of wonder about New York. I began to cherish the loneliness of it, the sense that at any given time no one need know where I was or what I was doing. I liked walking, from the East River over to the Hudson and back on brisk days, down around the Village on warm days. A friend would leave me the key to her apartment in the West Village when she was out of town, and sometimes I would just move down there, because by that time the telephone was beginning to bother me (the canker, you see, was already in the rose) and not many people had that number. I remember one day when someone who did have the West Village number came to pick me up for lunch there, and we both had hangovers, and I cut my finger opening him a beer and burst into tears, and we walked to a Spanish restaurant and drank bloody Marys and gazpacho until we felt better. I was not then guilt-ridden about spending afternoons that way, because I still had all the afternoons in the world.</p><p>And even that late in the game I still liked going to parties, all parties, bad parties, Saturday-afternoon parties given by recently married couples who lived in Stuyvesant Town, West Side parties given by unpublished or failed writers who served cheap red wine and talked about going to Guatalajara, Village parties where all the guests worked for advertising agencies and voted for Reform Democrats, press parties at Sardi’s, the worst kind of parties. You will have perceived by now that I was not one to profit by the experience of others, that it was a very long time indeed before I stopped believing in new faces and began to understand the lesson in that story, which was that it is distinctly possible to stay too long at the Fair.</p><hr><p>I could not tell you when I began to understand that. All I know is that it was very bad when I was twenty-eight. Everything that was said to me I seemed to have heard before, and I could no longer listen. I could no longer sit in little bars near Grand Central and listen to someone complaining of his wife’s inability to cope with the help while he missed another train to Connecticut. I no longer had any interest in hearing about the advances other people had received from their publishers, about plays which were having second-act trouble in Philadelphia, or about people I would like very much if only I would come out and meet them. I had already met them, always. There were certain parts of the city which I had to avoid. I could not bear upper Madison Avenue on weekday mornings (this was a particularly inconvenient aversion, since I then lived just fifty or sixty feet east of Madison), because I would see women walking Yorkshire terriers and shopping at Gristede’s, and some Veblenesque gorge would rise in my throat. I could not go to Times Square in the afternoon, or to the New York Public Library for any reason whatsoever. One day I could not go into a Schrafft’s; the next it would be the Bonwit Teller.</p><p>I hurt the people I cared about, and insulted those I did not. I cut myself off from the one person who was closer to me than any other. I cried until I was not even aware when I was crying and when I was not, I cried in elevators and in taxis and in Chinese laundries, and when I went to the doctor, he said only that I seemed to be depressed, and that I should see a “specialist.” He wrote down a psychiatrist’s name and address for me, but I did not go.</p><p>Instead I got married, which as it turned out was a very good thing to do but badly timed, since I still could not walk on upper Madison Avenue in the mornings and still could not talk to people and still cried in Chinese laundries. I had never before understood what “despair” meant, and I am not sure that I understand now, but I understood that year. Of course I could not work. I could not even get dinner with any degree of certainty, and I would sit in the apartment on Seventy-fifth Street paralyzed until my husband would call from his office and say gently that I did not have to get dinner, that I could meet him at Michael’s Pub or at Toots Shor’s or at Sardi’s East. And then one morning in April (we had been married in January) he called and told me that he wanted to get out of New York for a while, that he would take a six-month leave of absence, that we would go somewhere.</p><p>It was three years ago he told me that, and we have lived in Los Angeles since. Many of the people we knew in New York think this a curious aberration, and in fact tell us so. There is no possible, no adequate answer to that, and so we give certain stock answers, the answers everyone gives. I talk about how difficult it would be for us to “afford” to live in New York right now, about how much “space” we need, All I mean is that I was very young in New York, and that at some point the golden rhythm was broken, and I am not that young anymore. The last time I was in New York was in a cold January, and everyone was ill and tired. Many of the people I used to know there had moved to Dallas or had gone on Antabuse or had bought a farm in New Hampshire. We stayed ten days, and then we took an afternoon flight back to Los Angeles, and on the way home from the airport that night I could see the moon on the Pacific and smell jasmine all around and we both knew that there was no longer any point in keeping the apartment we still kept in New York. There were years when I called Los Angeles “the Coast,” but they seem a long time ago.</p><hr><div class="figure " style="width:;"><img class="fig-img" src="https://res.cloudinary.com/dx7aiyb0q/image/upload/v1582837322/posts/didion3_jitqqc.jpg" alt=""></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;  “All I mean is that I was very young in New York, and that at some point the golden rhythm was broken, and I am not that young anymore.”&lt;br&gt;  — Joan Didion&lt;/p&gt;
&lt;hr&gt;
    
    </summary>
    
      <category term="Literature" scheme="https://www.zzzdavid.tech/categories/Literature/"/>
    
    
      <category term="PersonalEssay" scheme="https://www.zzzdavid.tech/tags/PersonalEssay/"/>
    
      <category term="JoanDidion" scheme="https://www.zzzdavid.tech/tags/JoanDidion/"/>
    
  </entry>
  
</feed>
